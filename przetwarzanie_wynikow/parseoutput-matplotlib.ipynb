{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff78de34",
   "metadata": {},
   "source": [
    "Czytanie plików fio i dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecadccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_rgb\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0aa936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict_tree(d, indent=0, max_depth=None):\n",
    "    \"\"\"Rekurencyjnie wypisuje strukturę kluczy zagnieżdżonego słownika do zadanej głębokości.\"\"\"\n",
    "    if not isinstance(d, dict) or (max_depth is not None and indent >= max_depth):\n",
    "        return\n",
    "    for key in d:\n",
    "        print('  ' * indent + str(key))\n",
    "        print_dict_tree(d[key], indent + 1, max_depth)\n",
    "\n",
    "# Przykład użycia:\n",
    "# print_dict_tree(fio_resultsdict, max_depth=3)\n",
    "\n",
    "\n",
    "def print_latex_image(image_path, image_name, caption=None, label=None, width=None):\n",
    "    if image_name.endswith('.png'):\n",
    "        image_name = image_name[:-4]\n",
    "    width_str = f\"[width={width}]\" if width else \"[width=\\\\textwidth]\"\n",
    "    caption_str = f\"\\\\caption{{{caption}}}\" if caption else \"\\\\caption{TODO caption}\" \n",
    "    label_str = f\"\\\\label{{{label}}}\" if label else f\"\\\\label{{fig:{image_name}}}\"\n",
    "    latex_code = (\n",
    "        \"\\\\begin{figure}[H]\\n\"\n",
    "        \"    \\\\centering\\n\"\n",
    "        f\"    \\\\includegraphics{width_str}{{images/{image_path}/{image_name}}}\\n\"\n",
    "        f\"    {caption_str}\\n\"\n",
    "        f\"    {label_str}\\n\"\n",
    "        \"\\\\end{figure}\\n\"\n",
    "    )\n",
    "    print(latex_code)\n",
    "    \n",
    "def generate_caption(\n",
    "    workload=None,\n",
    "    storage=None,\n",
    "    block_size=None,\n",
    "    compression=None,\n",
    "    snapshot=None,\n",
    "    combine_storage_types=False,\n",
    "    combine_compression_types=False,\n",
    "    combine_snapshots=False,\n",
    "    storage_list=None,        # <── NEW\n",
    "    compression_list=None,    # <── NEW\n",
    "    snapshot_list=None,       # <── NEW\n",
    "    workload_translations=None,\n",
    "    storage_type_translations=None,\n",
    "    compression_translations=None\n",
    "):\n",
    "    \"\"\"Zwraca naturalnie brzmiący caption – z pełnymi listami, gdy użyto *combine_*.\"\"\"\n",
    "\n",
    "    # Pomocnicza funkcja do ładnego łączenia list: „A, B i C”\n",
    "    def natural_join(items):\n",
    "        if not items:\n",
    "            return \"\"\n",
    "        if len(items) == 1:\n",
    "            return items[0]\n",
    "        return \", \".join(items[:-1]) + \" i \" + items[-1]\n",
    "\n",
    "    sentence = \"Porównanie wydajności\"\n",
    "\n",
    "    # 1. OBCIĄŻENIE\n",
    "    if workload:\n",
    "        sentence += f\" dla obciążenia typu {workload_translations.get(workload, workload)}\"\n",
    "\n",
    "    # 2. TYP DYSKU\n",
    "    if combine_storage_types:\n",
    "        # konwertuj listę dysków na słowa (przetłumaczone)\n",
    "        translated = [storage_type_translations.get(s.upper(), f\"dysku {s}\") for s in (storage_list or [])]\n",
    "        sentence += f\" na {natural_join(translated)}\"\n",
    "    elif storage:\n",
    "        sentence += f\" z użyciem {storage_type_translations.get(storage.upper(), f'dysku {storage}')}\"\n",
    "\n",
    "    # 3. ROZMIAR BLOKU\n",
    "    if block_size:\n",
    "        sentence += (\n",
    "            \" przy domyślnym rozmiarze bloku\"\n",
    "            if block_size == \"default\"\n",
    "            else f\" przy rozmiarze bloku {block_size}\"\n",
    "        )\n",
    "\n",
    "    # 4. KOMPRESJA\n",
    "    if combine_compression_types:\n",
    "        comp_values = compression_list or []\n",
    "        has_none = \"none\" in comp_values\n",
    "        named = [compression_translations.get(c, c) for c in comp_values if c != \"none\"]\n",
    "\n",
    "        if len(comp_values) == 1 and comp_values[0] == \"none\":\n",
    "            pass  # nie pisz nic\n",
    "        elif has_none and len(named) == 1:\n",
    "            # bez kompresji oraz z kompresją lz4\n",
    "            sentence += f\", bez kompresji oraz z kompresją {named[0]}\"\n",
    "        elif has_none and named:\n",
    "            sentence += f\", bez kompresji oraz z kompresją: {natural_join(named)}\"\n",
    "        elif named:\n",
    "            sentence += f\", z kompresją: {natural_join(named)}\"\n",
    "    elif compression and compression != \"none\":\n",
    "        sentence += f\", z kompresją: {compression_translations.get(compression, compression)}\"\n",
    "\n",
    "\n",
    "\n",
    "    # 5. SNAPSHOT\n",
    "    if combine_snapshots:\n",
    "        snap_values = [str(s) for s in snapshot_list or []]\n",
    "        has_zero = \"0\" in snap_values\n",
    "        non_zero = [s for s in snap_values if s != \"0\"]\n",
    "\n",
    "        if len(snap_values) == 1 and snap_values[0] == \"0\":\n",
    "            pass  # nie pisz nic\n",
    "        elif has_zero and len(non_zero) == 1:\n",
    "            sentence += f\", bez snapshotu oraz z snapshotem {non_zero[0]}\"\n",
    "        elif has_zero and non_zero:\n",
    "            sentence += f\", bez snapshotu oraz z snapshotami: {natural_join(non_zero)}\"\n",
    "        elif non_zero:\n",
    "            sentence += f\", z snapshotami: {natural_join(non_zero)}\"\n",
    "\n",
    "    sentence += \".\"\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a404a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_translations = {\n",
    "        \"btrfs\": \"Btrfs\",\n",
    "        \"ext4\": \"Ext4\",\n",
    "        \"xfs\": \"XFS\",\n",
    "        \"f2fs\": \"F2FS\",\n",
    "        \"exfat\": \"exFAT\",\n",
    "        \"zfs\": \"ZFS\",\n",
    "        \"zfs_nocache\": \"ZFS bez cache\",\n",
    "        \"zfs_limit\": \"ZFS Limit\",\n",
    "        \"zfs_l2arc\": \"ZFS L2ARC\",\n",
    "        \"zfs_l2arc_limit\": \"ZFS L2ARC Limit\",\n",
    "    }\n",
    "\n",
    "metric_translations = {\n",
    "    \"Bandwidth (MiB/s)\": \"Przepustowość (MiB/s)\",\n",
    "    \"Bandwidth READ (MiB/s)\": \"Odczyt (MiB/s)\",\n",
    "    \"Bandwidth WRITE (MiB/s)\": \"Zapis (MiB/s)\",\n",
    "    \"IOPS\": \"IOPS\",\n",
    "    \"IOPS READ\": \"IOPS - odczyt\",\n",
    "    \"IOPS WRITE\": \"IOPS - zapis\",\n",
    "    \"Latency (ms)\": \"Opóźnienie (ms)\",\n",
    "    \"Latency READ (ms)\": \"Opóźnienie - odczyt (ms)\",\n",
    "    \"Latency WRITE (ms)\": \"Opóźnienie - zapis (ms)\"\n",
    "}\n",
    "\n",
    "workload_translations = {\n",
    "    \"database\": \"baza danych\",\n",
    "    \"multimedia\": \"multimedia\",\n",
    "    \"webserver\": \"serwer WWW\",\n",
    "    \"archive\": \"archiwum\"\n",
    "}\n",
    "\n",
    "storage_type_translations = {\n",
    "    \"NVME\": \"NVMe\",\n",
    "    \"RAID02HDD\": \"RAID0 2xHDD\",\n",
    "    \"RAID04HDD\": \"RAID0 4xHDD\",\n",
    "    \"STRIPE4HDD\": \"Stripe 4xHDD\",\n",
    "    \"STRIPE2HDD\": \"Stripe 2xHDD\",\n",
    "    \"MIRROR\": \"Mirror\",\n",
    "}\n",
    "\n",
    "compression_translations = {\n",
    "    \"none\": \"brak\",\n",
    "    \"lz4\": \"LZ4\",\n",
    "    \"zlib_1\": \"zlib-1\",\n",
    "    \"zlib_3\": \"zlib-3\",\n",
    "    \"zlib-9\": \"zlib-9\",\n",
    "    \"zstd_1\": \"zstd-1\",\n",
    "    \"zstd_3\": \"zstd-3\",\n",
    "    \"zstd_9\": \"zstd-9\",\n",
    "    \"zstd_15\": \"zstd-15\",\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac3bfb1-c53b-41c1-9991-c573de3694b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fio_results(file_path):\n",
    "    def parse_text_fio(file):\n",
    "        # Regular expressions\n",
    "        bandwidth_regex = re.compile(r'WRITE: bw=(\\d+(?:\\.\\d+)?)([MK]iB/s)')\n",
    "        bandwidth_read_regex = re.compile(r'READ: bw=(\\d+(?:\\.\\d+)?)([MK]iB/s)')\n",
    "        iops_regex = re.compile(r'write: IOPS=(\\d+)')\n",
    "        iops_read_regex = re.compile(r'read: IOPS=(\\d+)')\n",
    "        latency_regex = re.compile(r'lat (\\([mu]sec\\)): min=\\d+\\.?\\d*[km]?, max=\\d+\\.?\\d*[km]?, avg=(\\d+\\.\\d+[km]?), stdev=\\d+\\.?\\d*')\n",
    "\n",
    "        def convert_bandwidth(value, unit):\n",
    "            value = float(value)\n",
    "            if unit == \"KiB/s\":\n",
    "                return value / 1024\n",
    "            return value\n",
    "\n",
    "        results = {}\n",
    "        last = 'read'\n",
    "        for line in file:\n",
    "            if 'write' in line:\n",
    "                last = 'write'\n",
    "            elif 'read' in line:\n",
    "                last = 'read'\n",
    "\n",
    "            if (bw := bandwidth_regex.search(line)):\n",
    "                value, unit = bw.groups()\n",
    "                results['Bandwidth WRITE (MiB/s)'] = convert_bandwidth(value, unit)\n",
    "            if (bw := bandwidth_read_regex.search(line)):\n",
    "                value, unit = bw.groups()\n",
    "                results['Bandwidth READ (MiB/s)'] = convert_bandwidth(value, unit)\n",
    "            if (iops := iops_regex.search(line)):\n",
    "                results['IOPS WRITE'] = float(iops.group(1))\n",
    "            if (iops := iops_read_regex.search(line)):\n",
    "                results['IOPS READ'] = float(iops.group(1))\n",
    "            if (lat := latency_regex.search(line)):\n",
    "                lat_val = float(lat.group(2))\n",
    "                if lat.group(1) == '(usec)':\n",
    "                    lat_val /= 1000\n",
    "                results[f'Latency {last.upper()} (ms)'] = lat_val\n",
    "\n",
    "        return results\n",
    "\n",
    "    def parse_json_fio(data):\n",
    "        results = {}\n",
    "        for job in data.get('jobs', []):\n",
    "            for rw_type in ['read', 'write']:\n",
    "                if job.get(rw_type, {}).get('iops', 0) > 0:\n",
    "                    iops = job[rw_type]['iops']\n",
    "                    bw_kib = job[rw_type]['bw']\n",
    "                    latency_ns = job[rw_type].get('lat_ns', {}).get('mean', 0)\n",
    "\n",
    "                    results[f'IOPS {rw_type.upper()}'] = round(iops, 2)\n",
    "                    results[f'Bandwidth {rw_type.upper()} (MiB/s)'] = round(bw_kib / 1024, 2)\n",
    "                    results[f'Latency {rw_type.upper()} (ms)'] = round(latency_ns / 1_000_000, 3)\n",
    "\n",
    "        return results\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        first_char = f.read(1)\n",
    "        f.seek(0)\n",
    "        if first_char == '{':\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                return parse_json_fio(data)\n",
    "            except json.JSONDecodeError:\n",
    "                f.seek(0)\n",
    "                return parse_text_fio(f)\n",
    "        else:\n",
    "            return parse_text_fio(f)\n",
    "\n",
    "\n",
    "def parse_dd_results(file_path):\n",
    "    # Regular expressions\n",
    "    bandwidth_regex = re.compile(r'(\\d+(?:\\.\\d+)?) ([GMK]B/s)')\n",
    "    time_regex = re.compile(r'(\\d+(?:\\.\\d+)?) s')\n",
    "\n",
    "    # Function to convert bandwidth to MiB/s\n",
    "    def convert_bandwidth(value, unit):\n",
    "        value = float(value)\n",
    "        if unit == \"KB/s\":\n",
    "            return value / 1024  # Convert KB/s to MiB/s\n",
    "        elif unit == \"MB/s\":\n",
    "            return value  # Already in MiB/s\n",
    "        elif unit == \"GB/s\":\n",
    "            return value * 1024  # Convert GB/s to MiB/s\n",
    "        return value\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Match bandwidth\n",
    "            bw_match = bandwidth_regex.search(line)\n",
    "            if bw_match:\n",
    "                value, unit = bw_match.groups()\n",
    "                if 'write' in file_path:\n",
    "                    results['Bandwidth WRITE (MiB/s)'] = convert_bandwidth(value, unit)\n",
    "                else:\n",
    "                    results['Bandwidth READ (MiB/s)'] = convert_bandwidth(value, unit)\n",
    "\n",
    "            # Match time\n",
    "            time_match = time_regex.search(line)\n",
    "            if time_match:\n",
    "                results['Time (s)'] = float(time_match.group(1))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "def extract_values(resultsfolder, file_names, parser, program_type):\n",
    "    resultsdict = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(dict))))))\n",
    "\n",
    "    def extract_key(file_name):\n",
    "        match = re.match(r\"^[^_]+_(.*?)_test\", file_name)\n",
    "        return match.group(1) if match else os.path.splitext(file_name)[0]\n",
    "\n",
    "    prepaths = [folder for folder in glob.glob(os.path.join(resultsfolder, '*/')) if program_type in folder]\n",
    "\n",
    "    for prepath in prepaths:\n",
    "        folder_parts = os.path.basename(os.path.normpath(prepath)).split('_')\n",
    "        filesystem = '_'.join(folder_parts[2:-1])\n",
    "        storage = folder_parts[-1]\n",
    "\n",
    "        if program_type in ['fio_results', 'dd_results']:\n",
    "            block_size_folders = glob.glob(os.path.join(prepath, '*/'))\n",
    "            for block_size_folder in block_size_folders:\n",
    "                folder_name_parts = os.path.basename(os.path.normpath(block_size_folder)).split('_')\n",
    "\n",
    "                block_size = 'unknown'\n",
    "                direct = '1'  # domyślna wartość\n",
    "                compression = 'none'\n",
    "\n",
    "                if len(folder_name_parts) >= 5 and folder_name_parts[0] == \"block\" and folder_name_parts[1] == \"size\":\n",
    "                    block_size = folder_name_parts[2]\n",
    "                    try:\n",
    "                        direct_idx = folder_name_parts.index(\"direct\")\n",
    "                        direct = folder_name_parts[direct_idx + 1]\n",
    "                    except ValueError:\n",
    "                        direct = '1'  # fallback\n",
    "\n",
    "                    try:\n",
    "                        compression_idx = folder_name_parts.index(\"compression\")\n",
    "                        compression = '_'.join(folder_name_parts[compression_idx + 1:]) or 'none'\n",
    "                    except ValueError:\n",
    "                        compression = 'none'\n",
    "                else:\n",
    "                    print(f\"Pomijanie folderu o nieoczekiwanej strukturze: {block_size_folder}\")\n",
    "                    continue\n",
    "\n",
    "                folders = glob.glob(os.path.join(block_size_folder, '*/'))\n",
    "                cumulative_data = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "\n",
    "                for folder in folders:\n",
    "                    # Automatyczne wykrycie plików wynikowych (fio_*_test_output*.json lub .txt)\n",
    "                    file_paths = glob.glob(os.path.join(folder, 'fio_*_test_output*.*'))\n",
    "\n",
    "                    for file_path in file_paths:\n",
    "                        if os.path.exists(file_path):\n",
    "                            try:\n",
    "                                file_name = os.path.basename(file_path)\n",
    "                                results = parser(file_path)\n",
    "                                if results:\n",
    "                                    test_key = extract_key(file_name)\n",
    "\n",
    "                                    # Wyciągnij snapshot z nazwy pliku, np. _snapshots_120\n",
    "                                    snapshot_match = re.search(r'_snapshots_(\\d+)', file_name)\n",
    "                                    snapshot = snapshot_match.group(1) if snapshot_match else '0'\n",
    "\n",
    "                                    for key, value in results.items():\n",
    "                                        cumulative_data[test_key][snapshot][key].append(value)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Błąd podczas parsowania {file_path}: {e}\")\n",
    "                        else:\n",
    "                            print(f\"Plik nie znaleziony: {file_path}\")\n",
    "\n",
    "                for test_key, snapshots_data in cumulative_data.items():\n",
    "                    for snapshot, metrics in snapshots_data.items():\n",
    "                        if metrics:\n",
    "                            ranges_snapshot = {\n",
    "                                key: {\n",
    "                                    'median': round(np.median(values), 3),\n",
    "                                    'std': round(np.std(values), 3),\n",
    "                                    'min': round(min(values), 3),\n",
    "                                    'max': round(max(values), 3),\n",
    "                                    'avg': round(sum(values) / len(values), 2)\n",
    "                                } if values else '-' for key, values in metrics.items()\n",
    "                            }\n",
    "                            resultsdict[filesystem][direct][block_size][compression][storage][snapshot][test_key] = ranges_snapshot\n",
    "\n",
    "        else:\n",
    "            # Programy bez block size i snapshotów (np. hdparm)\n",
    "            folders = glob.glob(os.path.join(prepath, '*/'))\n",
    "            cumulative_data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "            for folder in folders:\n",
    "                file_paths = glob.glob(os.path.join(folder, '*'))\n",
    "                for file_path in file_paths:\n",
    "                    if os.path.exists(file_path):\n",
    "                        try:\n",
    "                            file_name = os.path.basename(file_path)\n",
    "                            results = parser(file_path)\n",
    "                            if results:\n",
    "                                test_key = extract_key(file_name)\n",
    "                                for key, value in results.items():\n",
    "                                    cumulative_data[test_key][key].append(value)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Błąd podczas parsowania {file_path}: {e}\")\n",
    "                    else:\n",
    "                        print(f\"Plik nie znaleziony: {file_path}\")\n",
    "\n",
    "            for test_key, metrics in cumulative_data.items():\n",
    "                if metrics:\n",
    "                    ranges = {\n",
    "                        key: {\n",
    "                            'median': round(np.median(values), 3),\n",
    "                            'std': round(np.std(values), 3),\n",
    "                            'min': round(min(values), 3),\n",
    "                            'max': round(max(values), 3),\n",
    "                            'avg': round(sum(values) / len(values), 2)\n",
    "                        } if values else '-' for key, values in metrics.items()\n",
    "                    }\n",
    "                    resultsdict[filesystem]['1']['no_block_size']['none'][storage]['0'][test_key] = ranges\n",
    "\n",
    "    return resultsdict\n",
    "\n",
    "\n",
    "def parse_hdparm_results(file_path):\n",
    "    # Regular expression to match the bandwidth\n",
    "    bandwidth_regex = re.compile(r'Timing O_DIRECT disk reads: (\\d+(?:\\.\\d+)?) MB in .* seconds = (\\d+(?:\\.\\d+)) MB/sec')\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Match bandwidth\n",
    "            bw_match = bandwidth_regex.search(line)\n",
    "            if bw_match:\n",
    "                total_mb, bandwidth = bw_match.groups()\n",
    "                results['Total Data Read (MB)'] = float(total_mb)\n",
    "                results['Bandwidth (MiB/s)'] = float(bandwidth)\n",
    "\n",
    "    return results\n",
    "    \n",
    "def extract_hdparm_values_by_device(resultsfolder, file_names, parser, group_by_computer=False):\n",
    "    # Adjust the default dictionary structure based on whether grouping by computer\n",
    "    resultsdict = (\n",
    "        defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "        if group_by_computer\n",
    "        else defaultdict(lambda: defaultdict(list))\n",
    "    )\n",
    "\n",
    "    # Iterate through all device type folders (e.g., hdparm_results_btrfs_hdd)\n",
    "    prepaths = glob.glob(os.path.join(resultsfolder, '*/'))\n",
    "    for prepath in prepaths:\n",
    "        # Check if folder starts with 'hdparm'\n",
    "        folder_name = os.path.basename(os.path.normpath(prepath))\n",
    "        if not folder_name.startswith('hdparm'):\n",
    "            continue  # skip non-hdparm folders\n",
    "\n",
    "        device_type = prepath.split('_')[-1].lower().strip('\\\\').strip('/')\n",
    "        \n",
    "        # Iterate through configuration folders\n",
    "        config_folders = glob.glob(os.path.join(prepath, '*/'))  # e.g., block_size_4096_compression_none/\n",
    "        for config_folder in config_folders:\n",
    "            # Iterate through computer folders (lab-sec-*)\n",
    "            computer_folders = glob.glob(os.path.join(config_folder, '*/'))\n",
    "            for computer_folder in computer_folders:\n",
    "                computer_name = os.path.basename(os.path.normpath(computer_folder))\n",
    "                \n",
    "                for file_name in file_names:\n",
    "                    file_path = os.path.join(computer_folder, file_name)\n",
    "                    if os.path.exists(file_path):\n",
    "                        try:\n",
    "                            # Parse the file and collect results\n",
    "                            results = parser(file_path)\n",
    "                            if group_by_computer:\n",
    "                                for key, value in results.items():\n",
    "                                    resultsdict[device_type][computer_name][key].append(value)\n",
    "                            else:\n",
    "                                for key, value in results.items():\n",
    "                                    resultsdict[device_type][key].append(value)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error parsing {file_path}: {e}\")\n",
    "                    else:\n",
    "                        print(f\"File not found: {file_path}\")\n",
    "\n",
    "    # Aggregate results by calculating min, max, and avg for each metric\n",
    "    aggregated_results = {}\n",
    "    for device_type, computers_or_metrics in resultsdict.items():\n",
    "        if group_by_computer:\n",
    "            aggregated_results[device_type] = {}\n",
    "            for computer, metrics in computers_or_metrics.items():\n",
    "                aggregated_results[device_type][computer] = {\n",
    "                    key: {\n",
    "                        'median': round(np.median(values), 3),\n",
    "                        'std': round(np.std(values), 3),\n",
    "                        'min': round(min(values), 3),\n",
    "                        'max': round(max(values), 3),\n",
    "                        'avg': round(sum(values) / len(values), 2)\n",
    "                    } if values else '-' for key, values in metrics.items()\n",
    "                }\n",
    "        else:\n",
    "            aggregated_results[device_type] = {\n",
    "                key: {\n",
    "                    'median': round(np.median(values), 3),\n",
    "                    'std': round(np.std(values), 3),\n",
    "                    'min': round(min(values), 3),\n",
    "                    'max': round(max(values), 3),\n",
    "                    'avg': round(sum(values) / len(values), 2)\n",
    "                } if values else '-' for key, values in computers_or_metrics.items()\n",
    "            }\n",
    "\n",
    "    return aggregated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9363a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = \"zwykle\"\n",
    "resultsfolder = ''\n",
    "fio_file_names = []\n",
    "\n",
    "if tests == \"article\":\n",
    "    resultsfolder = '../wyniki_50G_article/'\n",
    "    fio_file_names = [\n",
    "    'fio_database_article_test_output.txt',\n",
    "    'fio_seq_read_article_test_output.txt',\n",
    "    'fio_seq_write_article_test_output.txt',\n",
    "    ]\n",
    "elif tests == \"zwykle\":\n",
    "    resultsfolder = '../wyniki_zwykle/'\n",
    "    fio_file_names = [\n",
    "        'fio_database_test_output.txt',\n",
    "        'fio_multimedia_test_output.txt',\n",
    "        'fio_webserver_test_output.txt',\n",
    "        'fio_archive_test_output.txt',\n",
    "    ]\n",
    "\n",
    "\n",
    "fio_resultsdict = extract_values(resultsfolder, fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "print_dict_tree(fio_resultsdict, max_depth=6)\n",
    "\n",
    "# Example usage for dd\n",
    "dd_file_names = [\n",
    "    'dd_read_test_output.txt',\n",
    "    'dd_write_test_output.txt',\n",
    "]\n",
    "\n",
    "dd_resultsdict = extract_values(resultsfolder, dd_file_names, parse_dd_results, program_type='dd_results')\n",
    "print(dd_resultsdict)\n",
    "\n",
    "hdparm_file_names = [\n",
    "    'hdparm_test_output.txt',\n",
    "]\n",
    "\n",
    "hdparm_resultsdict = extract_hdparm_values_by_device(resultsfolder, hdparm_file_names, parse_hdparm_results, group_by_computer=False)\n",
    "\n",
    "print(hdparm_resultsdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef3785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_merge_results(dict1, dict2):\n",
    "    \"\"\"Rekurencyjnie scala dict2 do dict1 na głębokość 3 poziomów.\"\"\"\n",
    "    for fs, block_sizes in dict2.items():\n",
    "        if fs not in dict1:\n",
    "            dict1[fs] = block_sizes\n",
    "            continue\n",
    "        for block_size, storages in block_sizes.items():\n",
    "            if block_size not in dict1[fs]:\n",
    "                dict1[fs][block_size] = storages\n",
    "                continue\n",
    "            for storage, workloads in storages.items():\n",
    "                if storage not in dict1[fs][block_size]:\n",
    "                    dict1[fs][block_size][storage] = workloads\n",
    "                else:\n",
    "                    # Jeśli istnieje, scal workloady (np. database, multimedia, itd.)\n",
    "                    for workload, metrics in workloads.items():\n",
    "                        if workload not in dict1[fs][block_size][storage]:\n",
    "                            dict1[fs][block_size][storage][workload] = metrics\n",
    "                        else:\n",
    "                            # Jeśli istnieje, scal metryki (np. Bandwidth READ, IOPS, itd.)\n",
    "                            dict1[fs][block_size][storage][workload].update(metrics)\n",
    "\n",
    "\n",
    "if tests == \"article\":\n",
    "    fio_file_names = [\n",
    "    'fio_database_article_test_output.json',\n",
    "    'fio_seq_read_article_test_output.json',\n",
    "    'fio_seq_write_article_test_output.json',\n",
    "    ]\n",
    "\n",
    "    fio_resultsdict_raid= extract_values('../wyniki_raid_article_50G/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "    fio_resultsdict_raid_compression= extract_values('../wyniki_raid_article_50G_kompresja/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "    deep_merge_results(fio_resultsdict, fio_resultsdict_raid)\n",
    "    deep_merge_results(fio_resultsdict, fio_resultsdict_raid_compression)\n",
    "    print_dict_tree(fio_resultsdict, max_depth=3)\n",
    "\n",
    "elif tests == \"zwykle\":\n",
    "    fio_file_names = [\n",
    "        'fio_database_test_output.json',\n",
    "        'fio_multimedia_test_output.json',\n",
    "        'fio_webserver_test_output.json',\n",
    "        'fio_archive_test_output.json',\n",
    "    ]\n",
    "    #Scalanie różnych folderów wyników\n",
    "\n",
    "    # Usuń dane dla zfs i zfs_nocache\n",
    "    fio_resultsdict.pop('zfs', None)\n",
    "    fio_resultsdict.pop('zfs_nocache', None)\n",
    "    fio_resultsdict.pop('zfs_primary', None)\n",
    "\n",
    "    # Wczytaj dane z drugiego folderu\n",
    "    fio_resultsdict_nowe_zfs = extract_values('../wyniki_zwykle_nowe_zfs/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "    fio_resultsdict_nowe_zfs.pop('zfs', None)\n",
    "    fio_resultsdict_nowe_zfs.pop('zfs_primary', None)\n",
    "    fio_resultsdict_zfs_stats = extract_values('../wyniki_zwykle_zfs_stats/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "    fio_resultsdict_zfs_stats_limit = extract_values('../wyniki_zwykle_zfs_stats_limit/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "    fio_resultsdict_kompresja = extract_values('../wyniki_zwykle_kompresja/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "    fio_resultsdict_kompresja.pop('zfs', None)\n",
    "    fio_resultsdict_kompresja.pop('zfs_l2arc', None)\n",
    "    fio_resultsdict_kompresja_arcstats = extract_values('../wyniki_zwykle_kompresja_arcstats/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "    fio_resultsdict_raid = extract_values('../wyniki_raid/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "    fio_resultsdict_snapshot = extract_values('../wyniki_zwykle_snapshots_nowe/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "    fio_resultsdict_uring = extract_values('../wyniki_zwykle_nowy_silnik/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "\n",
    "    fio_resultsdict_nowa_wersja = extract_values('../wyniki_zwykle_directtest_nowe2.3/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "    if 'zfs' in fio_resultsdict_nowa_wersja:\n",
    "        fio_resultsdict_nowa_wersja['zfs_nowa_wersja'] = fio_resultsdict_nowa_wersja.pop('zfs')\n",
    "    fio_resultsdict_snapshots_nowa_wersja = extract_values('../wyniki_zwykle_snapshots_nowe2.3/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "    if 'zfs_nocache' in fio_resultsdict_snapshots_nowa_wersja:\n",
    "        fio_resultsdict_snapshots_nowa_wersja['zfs_nocache_nowa_wersja'] = fio_resultsdict_snapshots_nowa_wersja.pop('zfs_nocache')\n",
    "\n",
    "\n",
    "\n",
    "    # Dodaj \"_uring\" do wszystkich kluczy na najwyższym poziomie w fio_resultsdict_uring\n",
    "    fio_resultsdict_uring_renamed = {}\n",
    "    for fs, fs_data in fio_resultsdict_uring.items():\n",
    "        fio_resultsdict_uring_renamed[f\"{fs}_uring\"] = fs_data\n",
    "    deep_merge_results(fio_resultsdict, fio_resultsdict_uring_renamed)\n",
    "\n",
    "    # Dodaj dane z drugiego folderu do głównego zbioru\n",
    "    #fio_resultsdict.update(fio_resultsdict_new)\n",
    "    # Zmień nazwę kluczy 'zfs' i 'zfs_l2arc' na 'zfs_limit' i 'zfs_l2arc_limit' w fio_resultsdict_zfs_stats_limit\n",
    "    if 'zfs' in fio_resultsdict_zfs_stats_limit:\n",
    "        fio_resultsdict_zfs_stats_limit['zfs_limit'] = fio_resultsdict_zfs_stats_limit.pop('zfs')\n",
    "    if 'zfs_l2arc' in fio_resultsdict_zfs_stats_limit:\n",
    "        fio_resultsdict_zfs_stats_limit['zfs_l2arc_limit'] = fio_resultsdict_zfs_stats_limit.pop('zfs_l2arc')\n",
    "    deep_merge_results(fio_resultsdict, fio_resultsdict_nowa_wersja)\n",
    "    #deep_merge_results(fio_resultsdict, fio_resultsdict_snapshots_nowa_wersja)\n",
    "    deep_merge_results(fio_resultsdict, fio_resultsdict_nowe_zfs)\n",
    "    #deep_merge_results(fio_resultsdict, fio_resultsdict_snapshot)\n",
    "    deep_merge_results(fio_resultsdict, fio_resultsdict_zfs_stats_limit)\n",
    "    deep_merge_results(fio_resultsdict, fio_resultsdict_zfs_stats)\n",
    "    deep_merge_results(fio_resultsdict, fio_resultsdict_kompresja)\n",
    "    deep_merge_results(fio_resultsdict, fio_resultsdict_kompresja_arcstats)\n",
    "    deep_merge_results(fio_resultsdict, fio_resultsdict_raid)\n",
    "    print_dict_tree(fio_resultsdict, max_depth=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0a3d80",
   "metadata": {},
   "source": [
    "Funkcja do generowania wykresów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb5d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def darken_color(color, amount=0.7):\n",
    "    \"\"\"Zmniejsz jasność koloru RGB.\"\"\"\n",
    "    c = np.array(to_rgb(color))\n",
    "    return tuple(np.clip(c * amount, 0, 1))\n",
    "\n",
    "def plot_performance_metrics(data, metrics, storage_types, block_sizes, direct_values,\n",
    "                              include_min_max=False, include_std=False,\n",
    "                              workloads=None,\n",
    "                              file_systems=None, colors=None,\n",
    "                              hdparm_data=None, hdparm_stat='avg',\n",
    "                              preserve_ylim=False, debug=False,\n",
    "                              combine_storage_types=False, \n",
    "                              combine_compression_types=False, compression_types=None,\n",
    "                              save_dir=None, save=False,\n",
    "                              caption_autogen=False,\n",
    "                              snapshots=None, combine_snapshots=False, combine_block_sizes=False,\n",
    "                              combine_direct_values=False):\n",
    "\n",
    "    if colors is None:\n",
    "        colors = [\n",
    "            'b', 'g', 'r', 'c', 'm', 'y', 'orange', '#FFD700', '#A0522D',\n",
    "            '#8A2BE2', '#00CED1', '#DC143C', '#228B22', '#FF69B4', '#1E90FF',\n",
    "            '#FFDAB9', '#7FFF00', '#D2691E', '#483D8B', '#00FA9A', '#FF6347'\n",
    "        ]\n",
    "    if file_systems is None:\n",
    "        file_systems = list(data.keys())\n",
    "\n",
    "    direct_values_list = direct_values if not combine_direct_values else [\"none\"]\n",
    "    for direct in direct_values_list:\n",
    "        block_sizes_list = block_sizes if not combine_block_sizes else [\"none\"]\n",
    "        for block_size in block_sizes_list:\n",
    "            compression_list = compression_types if compression_types else [\"none\"]\n",
    "            if combine_compression_types:\n",
    "                compression_list = [None]\n",
    "            for compression in compression_list:\n",
    "                storage_list = storage_types if not combine_storage_types else [None]\n",
    "\n",
    "                for storage in storage_list:\n",
    "                    snapshot_list = snapshots if snapshots else [\"0\"]\n",
    "                    snapshot_list = snapshot_list if not combine_snapshots else [None]\n",
    "                    for snapshot in snapshot_list: \n",
    "                        for workload in workloads:\n",
    "                            prepared_data = {}\n",
    "\n",
    "                            for fs in file_systems:   \n",
    "                                for drct in (direct_values if combine_direct_values else [direct]):   \n",
    "                                    for bs in (block_sizes if combine_block_sizes else [block_size]):\n",
    "                                        for st in (storage_types if combine_storage_types else [storage]):\n",
    "                                            for comp in (compression_types if combine_compression_types else [compression]):\n",
    "                                                for snap in (snapshots if combine_snapshots else [snapshot]): \n",
    "                                                    fs_data = data.get(fs, {})\n",
    "\n",
    "                                                    if not fs_data:\n",
    "                                                        if debug:\n",
    "                                                            print(f\"⚠️ Pomijanie {fs} - brak danych\")\n",
    "                                                        continue\n",
    "\n",
    "                                                    drct_data = fs_data.get(drct, {})\n",
    "                                                    if not drct_data:\n",
    "                                                        if debug:\n",
    "                                                            print(f\"⚠️ Pomijanie {fs} - brak danych dla direct='{drct}'\")\n",
    "                                                        continue\n",
    "\n",
    "                                                    if combine_block_sizes: #Jak jest combine_block_sizes, to nie używaj default block size\n",
    "                                                        block_data = drct_data.get(bs, {}) \n",
    "                                                    else:\n",
    "                                                        block_data = drct_data.get(bs, {}) or drct_data.get('default', {})\n",
    "\n",
    "                                                    if not block_data:\n",
    "                                                        if debug:\n",
    "                                                            if combine_block_sizes:\n",
    "                                                                print(f\"⚠️ Pomijanie {fs} - brak danych dla block_size '{bs}'\")\n",
    "                                                            else:\n",
    "                                                                print(f\"⚠️ Pomijanie {fs} - brak danych dla block_size '{bs}' lub 'default'\")\n",
    "                                                        continue\n",
    "\n",
    "                                                    storage_data = block_data.get(comp, {}).get(st.lower())\n",
    "                                                    if not storage_data:\n",
    "                                                        if debug:\n",
    "                                                            print(f\"⚠️ Pomijanie {fs} - brak danych dla storage '{st}' w block_size '{block_size}' lub 'default'\")\n",
    "                                                        continue\n",
    "\n",
    "                                                    snapshot_data = storage_data.get(snap) \n",
    "\n",
    "                                                    if not isinstance(snapshot_data, dict):\n",
    "                                                        if debug:\n",
    "                                                            print(f\"⚠️ Pomijanie {fs} - storage '{st}': brak danych dla snapshot '{snapshot}'\")\n",
    "                                                        continue\n",
    "\n",
    "                                                    workload_data = snapshot_data.get(workload)\n",
    "                                                    if not isinstance(workload_data, dict):\n",
    "                                                        if debug:\n",
    "                                                            print(f\"⚠️ Pomijanie {fs} - storage '{st}': brak danych dla workload '{workload}'\")\n",
    "                                                        continue\n",
    "\n",
    "                                                    label_parts = [fs_translations.get(fs, fs)]\n",
    "                                                    if combine_direct_values:\n",
    "                                                        label_parts.append(\"bez direct\" if drct == \"0\" else \"z direct\")\n",
    "                                                    if combine_block_sizes:\n",
    "                                                        label_parts.append(\"domyślny rozmiar bloku\" if bs == \"default\" else f\"rozmiar bloku: {bs}\")\n",
    "                                                    if combine_storage_types:\n",
    "                                                        label_parts.append(storage_type_translations.get(st.upper(), st.upper()))\n",
    "                                                    if combine_compression_types:\n",
    "                                                        label_parts.append(f\"({compression_translations.get(comp, comp)})\")\n",
    "                                                    if combine_snapshots:\n",
    "                                                        if snap == 0:\n",
    "                                                            label_parts.append(\"Snapshot: brak\")\n",
    "                                                        else:\n",
    "                                                            label_parts.append(f\"Snapshot: {snap}\")\n",
    "                                                    label = \" \".join(label_parts)\n",
    "\n",
    "                                                    prepared_data[label] = workload_data\n",
    "\n",
    "                            if not prepared_data:\n",
    "                                if debug:\n",
    "                                    print(f\"⚠️ Brak danych po przetworzeniu dla compression={compression}, block_size={block_size}, workload={workload}\")\n",
    "                                continue\n",
    "\n",
    "                            plots = []\n",
    "                            for metric in metrics:\n",
    "                                if isinstance(metric, dict):\n",
    "                                    read_metric = metric.get(\"read\")\n",
    "                                    write_metric = metric.get(\"write\")\n",
    "                                    first_key = next(iter(prepared_data), None)\n",
    "                                    if not first_key:\n",
    "                                        continue\n",
    "                                    sample = prepared_data[first_key]\n",
    "                                    if read_metric and write_metric and read_metric in sample and write_metric in sample:\n",
    "                                        plots.append((\"grouped\", metric[\"name\"], read_metric, write_metric))\n",
    "                                    elif read_metric and read_metric in sample:\n",
    "                                        plots.append((\"single\", read_metric))\n",
    "                                    elif write_metric and write_metric in sample:\n",
    "                                        plots.append((\"single\", write_metric))\n",
    "                                    else:\n",
    "                                        plots.append((\"single\", metric[\"name\"]))\n",
    "                                else:\n",
    "                                    first_key = next(iter(prepared_data), None)\n",
    "                                    if first_key and metric in prepared_data[first_key]:\n",
    "                                        plots.append((\"single\", metric))\n",
    "\n",
    "                            fig, axs = plt.subplots(len(plots), 1, figsize=(10, 3 * len(plots)))\n",
    "                            if len(plots) == 1:\n",
    "                                axs = [axs]\n",
    "\n",
    "                            # Zwiększ przestrzeń na tytuł, ale zachowaj kontrolę nad \"plot area\"\n",
    "                            fig.subplots_adjust(top=0.9, bottom=0.15, left=0.15, right=0.95)\n",
    "\n",
    "                            # Ustaw stały rozmiar plot area (w jednostkach figure-coordinates)\n",
    "                            fixed_ax_height = 0.6 / len(plots)  # rozdziel proporcjonalnie\n",
    "                            for i, ax in enumerate(axs):\n",
    "                                ax.set_position([0.15, 0.9 - (i+1)*fixed_ax_height, 0.8, fixed_ax_height * 0.9])\n",
    "\n",
    "                            \n",
    "                            translated_workload = workload_translations.get(workload, workload)\n",
    "                            translated_block_size = \"domyślny\" if block_size == \"default\" else block_size\n",
    "\n",
    "                            combined_label = \", \".join(\n",
    "                                [storage_type_translations.get(s.upper(), s.upper()) for s in storage_types]\n",
    "                            ) if combine_storage_types else storage_type_translations.get(storage.upper(), storage.upper())\n",
    "\n",
    "                            if combine_compression_types:\n",
    "                                compression_label = \", \".join([compression_translations.get(c, c) for c in compression_types])\n",
    "                            else:\n",
    "                                compression_label = compression_translations.get(compression, compression)\n",
    "\n",
    "                            if combine_snapshots:\n",
    "                                snapshot_label = \", \".join([\"brak\" if str(s) == \"0\" else f\"{s}\" for s in snapshots])\n",
    "                            else:\n",
    "                                snapshot_label = \"brak\" if str(snapshot) == \"0\" else f\"{snapshot}\"\n",
    "\n",
    "                            if combine_direct_values:\n",
    "                                direct_label = \", \".join([f\"{d}\" for d in direct_values])\n",
    "                            else:\n",
    "                                direct_label = direct\n",
    "\n",
    "                            title_groups = [\n",
    "                                translated_workload.capitalize() if workload else \"\",\n",
    "                                f'Direct: {direct_label}',\n",
    "                                f\"Typ dysku: {combined_label}\",\n",
    "                                f\"Rozmiar bloku: {translated_block_size}\",\n",
    "                                f\"Snapshot: {snapshot_label}\",\n",
    "                                f\"Kompresja: {compression_label}\",\n",
    "                            ]\n",
    "\n",
    "                            # Parametry łamania\n",
    "                            max_line_length = 80  # można dostosować do szerokości wykresu\n",
    "                            lines = []\n",
    "                            current_line = \"\"\n",
    "\n",
    "                            for part in title_groups:\n",
    "                                if not part:\n",
    "                                    continue\n",
    "                                # +3 uwzględnia \" | \" między elementami\n",
    "                                if len(current_line) + len(part) + (3 if current_line else 0) <= max_line_length:\n",
    "                                    if current_line:\n",
    "                                        current_line += \" | \" + part\n",
    "                                    else:\n",
    "                                        current_line = part\n",
    "                                else:\n",
    "                                    lines.append(current_line)\n",
    "                                    current_line = part\n",
    "\n",
    "                            if current_line:\n",
    "                                lines.append(current_line)\n",
    "\n",
    "                            fig.suptitle(\"\\n\".join(lines))\n",
    "\n",
    "\n",
    "\n",
    "                            for i, plot in enumerate(plots):\n",
    "                                ax = axs[i]\n",
    "\n",
    "                                if plot[0] == \"grouped\":\n",
    "                                    _, base_name, read_metric, write_metric = plot\n",
    "                                    fs_labels = []\n",
    "                                    read_vals, write_vals = [], []\n",
    "                                    read_err, write_err = ([], []), ([], [])\n",
    "                                    color_map = {}\n",
    "\n",
    "                                    for idx, label in enumerate(prepared_data):\n",
    "                                        workload_data = prepared_data[label]\n",
    "                                        read_data = workload_data.get(read_metric, {})\n",
    "                                        write_data = workload_data.get(write_metric, {})\n",
    "\n",
    "                                        read_avg = read_data.get('avg')\n",
    "                                        write_avg = write_data.get('avg')\n",
    "                                        if read_avg is not None and write_avg is not None:\n",
    "                                            fs_labels.append(label)\n",
    "                                            read_vals.append(read_avg)\n",
    "                                            write_vals.append(write_avg)\n",
    "\n",
    "                                            if include_min_max:\n",
    "                                                read_min = read_data.get('min', read_avg)\n",
    "                                                read_max = read_data.get('max', read_avg)\n",
    "                                                read_err[0].append(read_avg - read_min)\n",
    "                                                read_err[1].append(read_max - read_avg)\n",
    "\n",
    "                                                write_min = write_data.get('min', write_avg)\n",
    "                                                write_max = write_data.get('max', write_avg)\n",
    "                                                write_err[0].append(write_avg - write_min)\n",
    "                                                write_err[1].append(write_max - write_avg)\n",
    "\n",
    "                                            elif include_std:\n",
    "                                                read_std = read_data.get('std', 0)\n",
    "                                                write_std = write_data.get('std', 0)\n",
    "                                                read_err[0].append(read_std)\n",
    "                                                read_err[1].append(read_std)\n",
    "                                                write_err[0].append(write_std)\n",
    "                                                write_err[1].append(write_std)\n",
    "\n",
    "\n",
    "                                            color_map[label] = colors[idx % len(colors)]\n",
    "\n",
    "                                    x = np.arange(len(fs_labels))\n",
    "                                    bar_width = 0.35\n",
    "                                    read_colors = [color_map[fs] for fs in fs_labels]\n",
    "                                    write_colors = [darken_color(color_map[fs]) for fs in fs_labels]\n",
    "\n",
    "                                    read_bars = ax.bar(x - bar_width/2, read_vals, bar_width,\n",
    "                                                    label='_Odczyt', color=read_colors,\n",
    "                                                    yerr=read_err if include_min_max or include_std else None, capsize=5)\n",
    "                                    write_bars = ax.bar(x + bar_width/2, write_vals, bar_width,\n",
    "                                                        label='_Zapis', color=write_colors,\n",
    "                                                        yerr=write_err if include_min_max or include_std else None, capsize=5)\n",
    "\n",
    "                                    for j, bar in enumerate(read_bars):\n",
    "                                        y = bar.get_height()\n",
    "                                        err = read_err[1][j] if include_min_max or include_std else 0\n",
    "                                        ax.text(bar.get_x() + bar.get_width() / 2, y + err + 0.02 * y,\n",
    "                                                f'{y:.2f}\\nOdczyt', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "                                    for j, bar in enumerate(write_bars):\n",
    "                                        y = bar.get_height()\n",
    "                                        err = write_err[1][j] if include_min_max or include_std else 0\n",
    "                                        ax.text(bar.get_x() + bar.get_width() / 2, y + err + 0.02 * y,\n",
    "                                                f'{y:.2f}\\nZapis', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "                                    max_height = max(\n",
    "                                        [v + e for v, e in zip(read_vals, read_err[1])] +\n",
    "                                        [v + e for v, e in zip(write_vals, write_err[1])]\n",
    "                                    ) if include_min_max or include_std else max(max(read_vals, default=0), max(write_vals, default=0))\n",
    "                                    ax.set_ylim(0, max_height * 1.4)\n",
    "\n",
    "                                    translated_label = metric_translations.get(base_name, base_name)\n",
    "                                    ax.set_xticks(x)\n",
    "                                    if len(fs_labels) >= 6:\n",
    "                                        ax.set_xticklabels(fs_labels, rotation=45, ha='center', fontsize=8)\n",
    "                                    else:\n",
    "                                        ax.set_xticks(x)\n",
    "                                        ax.set_xticklabels(fs_labels, fontsize=10)  \n",
    "\n",
    "                                    ax.set_ylabel(translated_label)\n",
    "                                    ax.set_title(translated_label)\n",
    "                                    ax.grid(axis='y', linestyle='--', alpha=0.5, zorder=0)\n",
    "                                    for spine in ax.spines.values():\n",
    "                                        spine.set_edgecolor('black')\n",
    "                                        spine.set_linewidth(0.5)\n",
    "                                    # ax.legend()\n",
    "                                    if hdparm_data and hdparm_stat and metric[\"name\"] == \"Bandwidth (MiB/s)\" and not combine_storage_types:\n",
    "                                        if storage and storage.lower() in hdparm_data:\n",
    "                                            device_data = hdparm_data[storage.lower()]\n",
    "                                            hdparm_val = device_data.get(\"Bandwidth (MiB/s)\", {}).get(hdparm_stat)\n",
    "                                            if hdparm_val:\n",
    "                                                ylim = ax.get_ylim()\n",
    "                                                label_text = f'hdparm {storage} ({hdparm_val:.1f} MiB/s)'\n",
    "                                                if hdparm_val <= ylim[1]:\n",
    "                                                    ax.axhline(hdparm_val, color='#8B0000', linestyle='--', linewidth=1.5, alpha=0.5 , label=label_text)\n",
    "                                                    ax.legend(fontsize=8, loc='upper left', bbox_to_anchor=(0.78, 1.25))\n",
    "                                                else:\n",
    "                                                    # Dodaj \"fake\" obiekt do legendy\n",
    "                                                    legend_line = Line2D([0], [0], color='#8B0000', linestyle='--', linewidth=1.5, alpha=0.5)\n",
    "                                                    ax.legend([legend_line], [label_text], fontsize=8, loc='upper left', bbox_to_anchor=(0.78, 1.25))\n",
    "                                        elif debug:\n",
    "                                            print(f\"⚠️ Brak danych hdparm dla {storage} w {metric['name']}\")\n",
    "\n",
    "                                elif plot[0] == \"single\":\n",
    "                                    _, metric = plot\n",
    "                                    fs_labels = []\n",
    "                                    avg_values = []\n",
    "                                    y_errs = ([], [])\n",
    "                                    color_map = {}\n",
    "\n",
    "                                    for idx, label in enumerate(prepared_data):\n",
    "                                        workload_data = prepared_data[label]\n",
    "                                        metric_data = workload_data.get(metric, {})\n",
    "                                        avg = metric_data.get(\"avg\")\n",
    "                                        if avg is not None:\n",
    "                                            fs_labels.append(label)\n",
    "                                            avg_values.append(avg)\n",
    "\n",
    "                                            if include_min_max:\n",
    "                                                min_val = metric_data.get(\"min\", avg)\n",
    "                                                max_val = metric_data.get(\"max\", avg)\n",
    "                                                y_errs[0].append(avg - min_val)\n",
    "                                                y_errs[1].append(max_val - avg)\n",
    "                                            elif include_std:\n",
    "                                                std_val = metric_data.get(\"std\", 0)\n",
    "                                                y_errs[0].append(std_val)\n",
    "                                                y_errs[1].append(std_val)\n",
    "\n",
    "\n",
    "                                            color_map[label] = colors[idx % len(colors)]\n",
    "\n",
    "                                    bars = ax.bar(fs_labels, avg_values, color=[color_map[fs] for fs in fs_labels],\n",
    "                                                yerr=y_errs if include_min_max or include_std else None, capsize=5)\n",
    "\n",
    "                                    if len(fs_labels) >= 6:\n",
    "                                        ax.set_xticks(range(len(fs_labels)))\n",
    "                                        ax.set_xticklabels(fs_labels, rotation=45, ha='center', fontsize=8)\n",
    "                                    else:\n",
    "                                        ax.set_xticks(range(len(fs_labels)))\n",
    "                                        ax.set_xticklabels(fs_labels, fontsize=10)\n",
    "\n",
    "                                    for j, bar in enumerate(bars):\n",
    "                                        y = bar.get_height()\n",
    "                                        err = y_errs[1][j] if include_min_max or include_std else 0\n",
    "                                        ax.text(bar.get_x() + bar.get_width() / 2, y + err + 0.02 * y,\n",
    "                                                f'{y:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "                                    if avg_values:\n",
    "                                        max_height = max([v + e for v, e in zip(avg_values, y_errs[1])] if include_min_max or include_std else avg_values)\n",
    "                                        ax.set_ylim(0, max_height * 1.4)\n",
    "\n",
    "                                    translated_label = metric_translations.get(metric, metric)\n",
    "                                    ax.set_ylabel(translated_label)\n",
    "                                    ax.set_title(translated_label)\n",
    "                                    ax.grid(axis='y', linestyle='--', alpha=0.5, zorder=0)\n",
    "                                    for spine in ax.spines.values():\n",
    "                                        spine.set_edgecolor('black')\n",
    "                                        spine.set_linewidth(0.5)\n",
    "                                    if hdparm_data and hdparm_stat and \"bandwidth\" in metric.lower() and not combine_storage_types:\n",
    "                                        if storage and storage.lower() in hdparm_data:\n",
    "                                            device_data = hdparm_data[storage.lower()]\n",
    "                                            hdparm_val = device_data.get(\"Bandwidth (MiB/s)\", {}).get(hdparm_stat)\n",
    "                                            if hdparm_val:\n",
    "                                                ylim = ax.get_ylim()\n",
    "                                                label_text = f'hdparm {storage} ({hdparm_val:.1f} MiB/s)'\n",
    "                                                if hdparm_val <= ylim[1]:\n",
    "                                                    ax.axhline(hdparm_val, color='#8B0000', linestyle='--', linewidth=1.5, alpha=0.5, label=label_text)\n",
    "                                                    ax.legend(fontsize=8, loc='upper left', bbox_to_anchor=(0.78, 1.25))\n",
    "\n",
    "                                                else:\n",
    "                                                    # Dodaj \"fake\" obiekt do legendy\n",
    "                                                    legend_line = Line2D([0], [0], color='#8B0000', linestyle='--', alpha=0.5, linewidth=1.5)\n",
    "                                                    ax.legend([legend_line], [label_text], fontsize=8, loc='upper left', bbox_to_anchor=(0.78, 1.25))\n",
    "                                        elif debug:\n",
    "                                            print(f\"⚠️ Brak danych hdparm dla {storage} w {metric}\")\n",
    "\n",
    "                            plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "                            if save and save_dir:\n",
    "                                os.makedirs(\"wykresy/\"+save_dir, exist_ok=True)\n",
    "                                safe_combined_label = combined_label.replace(\" \", \"\").replace(\"/\", \"\")\n",
    "                                safe_compression_label = \"combined\" if combine_compression_types else compression.lower()\n",
    "                                safe_file_systems = \"_\".join([fs.replace(\"_\", \"\") for fs in file_systems])\n",
    "                                safe_snapshots = \"_\".join([sp.replace(\"_\", \"\") for sp in snapshots]) if combine_snapshots else \"default\"\n",
    "                                filename = f\"syntetyk_{safe_file_systems}_{safe_combined_label.lower()}_{block_size.lower()}_{safe_compression_label}_{safe_snapshots}_{workload.lower()}.png\"\n",
    "                                filepath = os.path.join(\"wykresy/\"+save_dir, filename)\n",
    "                                fig.savefig(filepath, bbox_inches='tight')\n",
    "                                if caption_autogen:\n",
    "                                    caption = generate_caption(\n",
    "                                        workload=workload,\n",
    "                                        storage=storage,\n",
    "                                        block_size=block_size,\n",
    "                                        compression=compression,\n",
    "                                        snapshot=snapshot,\n",
    "                                        combine_storage_types=combine_storage_types,\n",
    "                                        combine_compression_types=combine_compression_types,\n",
    "                                        combine_snapshots=combine_snapshots,\n",
    "                                        storage_list=storage_types,\n",
    "                                        compression_list=compression_types,\n",
    "                                        snapshot_list=snapshots,\n",
    "                                        workload_translations=workload_translations,\n",
    "                                        storage_type_translations=storage_type_translations,\n",
    "                                        compression_translations=compression_translations,\n",
    "                                    )\n",
    "                                else:\n",
    "                                    caption = None\n",
    "                                print_latex_image(save_dir, filename, caption=caption)\n",
    "                                if debug:\n",
    "                                    print(f\"📁 Zapisano wykres do pliku: {filepath}\")\n",
    "                            else:\n",
    "                                plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1b1198",
   "metadata": {},
   "source": [
    "Funkcja do generowania tabelki fio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ad6153-be83-4703-bb31-52f4d1bc2602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_row_data(data, workload, columns, file_systems=None, storage_types=None, block_sizes=None,\n",
    "                     compressions=None, snapshots=None):\n",
    "    rows = []\n",
    "    for fs, block_data in data.items():\n",
    "        if file_systems and fs not in file_systems:\n",
    "            continue\n",
    "        for block_size in block_sizes if block_sizes else block_data.keys():\n",
    "            block_entry = block_data.get(block_size) or block_data.get(\"default\")\n",
    "            if not block_entry:\n",
    "                continue\n",
    "            for compression, storage_data in block_entry.items():\n",
    "                if compressions and compression not in compressions:\n",
    "                    continue\n",
    "                for storage, snapshot_data in storage_data.items():\n",
    "                    if storage_types and storage.upper() not in [s.upper() for s in storage_types]:\n",
    "                        continue\n",
    "                    for snapshot, workloads in snapshot_data.items():\n",
    "                        if snapshots and snapshot not in snapshots:\n",
    "                            continue\n",
    "                        if workload not in workloads:\n",
    "                            continue\n",
    "\n",
    "                        translated_block_size = \"domyślny\" if block_size == \"default\" else block_size\n",
    "                        translated_compression = compression_translations.get(compression, compression)\n",
    "                        translated_storage = storage_type_translations.get(storage.upper(), storage)\n",
    "                        translated_snapshot = snapshot\n",
    "                        translated_fs = fs\n",
    "\n",
    "                        row = [\n",
    "                            translated_fs,\n",
    "                            translated_block_size,\n",
    "                            translated_compression,\n",
    "                            translated_storage,\n",
    "                            translated_snapshot\n",
    "                        ]\n",
    "\n",
    "                        for col in columns[5:]:  # Skip translated base columns\n",
    "                            try:\n",
    "                                storage_col, metric_stat = col.split(\" \", 1)\n",
    "                                metric, stat = metric_stat.rsplit(\" \", 1)\n",
    "                            except ValueError:\n",
    "                                print(f\"Nie udało się sparsować kolumny: {col}\")\n",
    "                                continue\n",
    "                            \n",
    "                            metric_key = metric\n",
    "                            value = \"N/A\"\n",
    "                            if storage.lower() == storage_col.lower():\n",
    "                                value = workloads[workload].get(metric_key, {}).get(stat.lower(), \"N/A\")\n",
    "                            row.append(value)\n",
    "                        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "\n",
    "def generate_columns(metrics, stats=[\"MIN\", \"AVG\", \"MAX\"], storage_types=[\"HDD\", \"SSD\", \"NVME\"]):\n",
    "    columns = [\"System plików\", \"Rozmiar bloku\", \"Kompresja\", \"Typ nośnika\", \"Migawka\"]\n",
    "    for storage in storage_types:\n",
    "        for metric in metrics:\n",
    "            if \"Bandwidth\" in metric:\n",
    "                base_metric = f\"{metric} (MiB/s)\"\n",
    "            elif \"Latency\" in metric:\n",
    "                base_metric = f\"{metric} (ms)\"\n",
    "            else:\n",
    "                base_metric = metric\n",
    "            for stat in stats:\n",
    "                columns.append(f\"{storage} {base_metric} {stat}\")\n",
    "    return columns\n",
    "\n",
    "\n",
    "\n",
    "def display_performance_metrics(data, workloads, metrics, stats=[\"MIN\", \"AVG\", \"MAX\"],\n",
    "                                storage_types=[\"HDD\", \"SSD\", \"NVME\"], file_systems=None,\n",
    "                                block_sizes=None, compressions=None, snapshots=None):\n",
    "    for workload in workloads:\n",
    "        columns = generate_columns(metrics, stats, storage_types)\n",
    "        rows = extract_row_data(data, workload, columns, file_systems, storage_types, block_sizes, compressions, snapshots)\n",
    "        df = pd.DataFrame(rows, columns=columns)\n",
    "        display(df.style.set_caption(f\"Performance Metrics: {workload.capitalize()}\").format(precision=3))\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if tests == \"article\":\n",
    "    workloads = [\"database_article\", \"seq_read_article\", \"seq_write_article\"] \n",
    "elif tests == \"zwykle\":\n",
    "    workloads = [\"database\", \"multimedia\", \"webserver\", \"archive\"]\n",
    "\n",
    "metrics = {\n",
    "    'database_article': [\"Bandwidth READ\", \"Bandwidth WRITE\", \"IOPS READ\", \"IOPS WRITE\", \"Latency READ\", \"Latency WRITE\"],\n",
    "    'seq_write_article': [\"Bandwidth WRITE\", \"IOPS WRITE\", \"Latency WRITE\"],\n",
    "    'seq_read_article': [\"Bandwidth READ\", \"IOPS READ\", \"Latency READ\"],\n",
    "    'database': [\"Bandwidth READ\", \"Bandwidth WRITE\", \"IOPS READ\", \"IOPS WRITE\", \"Latency READ\", \"Latency WRITE\"],\n",
    "    'archive': [\"Bandwidth WRITE\", \"IOPS WRITE\", \"Latency WRITE\"],\n",
    "    'multimedia': [\"Bandwidth READ\", \"IOPS READ\", \"Latency READ\"],\n",
    "    'webserver': [\"Bandwidth READ\", \"IOPS READ\", \"Latency READ\"],\n",
    "    \"default\": [\"Bandwidth READ\", \"Bandwidth WRITE\", \"IOPS READ\", \"IOPS WRITE\", \"Latency READ\", \"Latency WRITE\"],\n",
    "}\n",
    "block_sizes = [\"default\"]  # Specify block sizes to display\n",
    "#storage_types=[\"hdd\", \"stripe2hdd\", \"stripe4hdd\", \"mirror\", \"raidz1\", \"raidz2\", \"raid10\"]\n",
    "storage_types=[\"hdd\"]\n",
    "# Generate and display tables for each workload\n",
    "file_systems = [\"ext4\", \"xfs\", \"btrfs\", \"zfs\"]\n",
    "compression_types = [\"none\",\"lz4\"]\n",
    "snapshots = [\"0\"]  # Specify snapshots to display\n",
    "\n",
    "direct_values = [\"0\", \"1\"]  # Direct values to display\n",
    "for workload in workloads:\n",
    "  \n",
    "    workload_metrics = metrics.get(workload, metrics[\"default\"])\n",
    "    display_performance_metrics(fio_resultsdict, [workload], workload_metrics, block_sizes=block_sizes, storage_types=storage_types, direct_values=direct_values, stats = [\"AVG\"], file_systems = file_systems,compressions=compression_types, snapshots=snapshots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b62525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "metrics = [\n",
    "    {\"name\": \"Bandwidth (MiB/s)\", \"read\": \"Bandwidth READ (MiB/s)\", \"write\": \"Bandwidth WRITE (MiB/s)\"},\n",
    "    #{\"name\": \"IOPS\", \"read\": \"IOPS READ\", \"write\": \"IOPS WRITE\"}, \n",
    "    #{\"name\": \"Latency (ms)\", \"read\": \"Latency READ (ms)\", \"write\": \"Latency WRITE (ms)\"},\n",
    "]\n",
    "\n",
    "if tests == \"article\":\n",
    "    workloads = [\"database_article\", \"seq_read_article\", \"seq_write_article\"]\n",
    "elif tests == \"zwykle\":\n",
    "    workloads = [\"database\", \"multimedia\", \"webserver\", \"archive\"]\n",
    "\n",
    "#storage_types = ['HDD','RAID02HDD', 'RAID04HDD', 'RAID1', 'RAID5', 'RAID6', 'RAID10']\n",
    "#storage_types = [\"hdd\", \"stripe2hdd\", \"stripe4hdd\", \"mirror\", \"raidz1\", \"raidz2\", \"raid10\"]\n",
    "storage_types = [\"HDD\"]  # Lista typów nośników do analizy\n",
    "\n",
    "block_sizes = ['default']  \n",
    "\n",
    "file_systems = [\"zfs\",\"zfs_nowa_wersja\",\"zfs_nocache\"]  #['exfat','ext4', 'xfs', 'btrfs', \"f2fs\", \"zfs\", \"zfs_nocache\"]\n",
    "\n",
    "#workloads = [\"database\"]\n",
    "\n",
    "#compression_types = [\"none\",\"zlib_1\", \"zlib_3\", \"zlib-9\", \"zstd_1\", \"zstd_1\", \"zstd_3\", \"zstd_9\", \"zstd_15\"]  # Kompresja brfs\n",
    "#compression_types = [\"none\", \"gzip-1\", \"gzip-3\", \"gzip-9\", \"lz4\"]  # Kompresja zfs\n",
    "compression_types = [\"none\"]\n",
    "#snapshots= [\"0\",\"120\", \"240\",\"360\",\"480\",\"600\",\"720\",\"840\",\"960\",\"1080\",\"1200\"]  # Lista migawków do analizy, jeśli jest dostępna\n",
    "snapshots = [\"0\"] \n",
    "direct_values = [\"0\",\"1\"]\n",
    "#plot_performance_metrics(fio_resultsdict, metrics, storage_types, block_sizes, include_min_max=True, workload=workload, hdparm_data=hdparm_resultsdict, hdparm_stat='avg' )\n",
    "plot_performance_metrics(fio_resultsdict, metrics, storage_types, block_sizes, direct_values=direct_values, compression_types=compression_types, \n",
    "                            include_min_max=False, include_std=True,\n",
    "                            workloads=workloads, combine_storage_types=False,\n",
    "                            combine_compression_types=False, file_systems=file_systems,\n",
    "                            hdparm_data=hdparm_resultsdict, hdparm_stat='max', \n",
    "                            snapshots=snapshots, combine_snapshots=False,\n",
    "                            preserve_ylim=True, debug=True,\n",
    "                            save_dir=\"wykresy_kompresja\", save=False, caption_autogen=False,\n",
    "                            combine_direct_values=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb57889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate all possible columns\n",
    "def generate_columns(metrics, stats=[\"MIN\", \"MAX\", \"AVG\"], storage_types=[\"HDD\", \"SSD\", \"NVME\"], file_systems=None):\n",
    "    columns = [\"File System\", \"Block Size\"]  # Include Block Size in columns\n",
    "    for storage in storage_types:\n",
    "        for metric in metrics:\n",
    "            for stat in stats:\n",
    "                columns.append(f\"{storage} {metric} {stat}\")\n",
    "    return columns\n",
    "\n",
    "def extract_row_data(data, columns, file_systems=None, block_sizes=None):\n",
    "    rows = []\n",
    "    for fs, block_data in data.items():\n",
    "        if file_systems and fs not in file_systems:\n",
    "            continue\n",
    "        for block_size, devices in block_data.items():  # Iterate over block sizes\n",
    "            if block_sizes and block_size not in block_sizes:\n",
    "                continue\n",
    "            row = [fs, block_size]  # Add File System and Block Size to the row\n",
    "            for col in columns[2:]:  # Skip File System and Block Size\n",
    "                if len(col.split()) > 3:\n",
    "                    col = col.split()\n",
    "                    storage, metric, stat = col[0], col[1] + ' ' + col[2], col[3]\n",
    "                else:\n",
    "                    storage, metric, stat = col.split(\" \", 2)\n",
    "                metric_key = f\"{metric} (MiB/s)\" if \"Bandwidth\" in metric else metric\n",
    "                # Extract value\n",
    "                value = \"N/A\"\n",
    "                for device_type, workloads in devices.items():\n",
    "                    if device_type.lower() == storage.lower():\n",
    "                        for operation, metrics in workloads.items():\n",
    "                            if metric_key in metrics:\n",
    "                                value = metrics[metric_key].get(stat.lower(), \"N/A\")\n",
    "                                break\n",
    "                row.append(value)\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "# Example data\n",
    "# Generate and display tables for dd_resultsdict\n",
    "columns = generate_columns([\"Bandwidth READ\", \"Bandwidth WRITE\"], stats=[\"MIN\", \"AVG\", \"MAX\"], storage_types=[\"HDD\", \"SSD\", \"NVME\"])\n",
    "rows = extract_row_data(dd_resultsdict, columns, block_sizes=[\"4096\"])  # Specify block sizes\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "display(df.style.set_caption(\"Performance Metrics: DD Results\").format(precision=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot performance metrics for dd data\n",
    "plot_performance_metrics(dd_resultsdict, metrics=[\"Bandwidth READ (MiB/s)\", \"Bandwidth WRITE (MiB/s)\"], storage_types=storage_types, block_sizes=['4096']\n",
    ", include_min_max=True, hdparm_data=hdparm_resultsdict, hdparm_stat='max' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5410dd78",
   "metadata": {},
   "source": [
    "#TODO dodanie uniwersalnej funkcji fo tabelek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9276861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_arcstats_before_after(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        content = f.read()\n",
    "    before_match = re.search(r'===== ARC STATS BEFORE TEST =====\\n(.*?)\\n===== ARC STATS AFTER TEST =====', content, re.S)\n",
    "    after_match = re.search(r'===== ARC STATS AFTER TEST =====\\n(.*)', content, re.S)\n",
    "    if not before_match or not after_match:\n",
    "        return None\n",
    "\n",
    "    def parse_lines(lines):\n",
    "        stats = {}\n",
    "        for line in lines.splitlines():\n",
    "            match = re.match(r'(\\S+)\\s+\\d+\\s+(\\d+)', line)\n",
    "            if match:\n",
    "                key, value = match.groups()\n",
    "                stats[key] = int(value)\n",
    "        return stats\n",
    "\n",
    "    before_stats = parse_lines(before_match.group(1))\n",
    "    after_stats = parse_lines(after_match.group(1))\n",
    "\n",
    "    return {\n",
    "        'ARC': {\n",
    "            'hits': after_stats.get('hits', 0) - before_stats.get('hits', 0),\n",
    "            'misses': after_stats.get('misses', 0) - before_stats.get('misses', 0),\n",
    "            'size': after_stats.get('size', 0),\n",
    "            'limit': after_stats.get('c', 0)\n",
    "        },\n",
    "        'L2ARC': {\n",
    "            'hits': after_stats.get('l2_hits', 0) - before_stats.get('l2_hits', 0),\n",
    "            'misses': after_stats.get('l2_misses', 0) - before_stats.get('l2_misses', 0),\n",
    "            'size': after_stats.get('l2_size', 0) - before_stats.get('l2_size', 0),\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def collect_arcstats_raw(root_folder, program_type='fio_results'):\n",
    "    results = defaultdict(\n",
    "        lambda: defaultdict(\n",
    "            lambda: defaultdict(\n",
    "                lambda: defaultdict(\n",
    "                    lambda: defaultdict(list)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for dirpath, _, filenames in os.walk(root_folder):\n",
    "        for filename in filenames:\n",
    "            if not filename.startswith('arcstats_') or not filename.endswith('.log'):\n",
    "                continue\n",
    "\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            workload_match = re.match(r'arcstats_(.*?)_test\\.log', filename)\n",
    "            workload = workload_match.group(1) if workload_match else 'unknown'\n",
    "\n",
    "            parts = dirpath.split(os.sep)\n",
    "            if len(parts) < 4:\n",
    "                continue\n",
    "\n",
    "            filesystem_parts = parts[1].replace(f'{program_type}_', '').split('_')\n",
    "            if len(filesystem_parts) >= 2:\n",
    "                filesystem = '_'.join(filesystem_parts[:-1])\n",
    "                storage = filesystem_parts[-1]\n",
    "            else:\n",
    "                filesystem = filesystem_parts[0]\n",
    "                storage = 'unknown'\n",
    "\n",
    "            blocksize = re.search(r'block_size_(.*?)_compression', parts[2]).group(1)\n",
    "            compression = re.search(r'compression_(.*)', parts[2]).group(1)\n",
    "            lab = parts[3]\n",
    "\n",
    "            stats = parse_arcstats_before_after(filepath)\n",
    "            if stats:\n",
    "                results[filesystem][blocksize][compression][storage][workload].append({\n",
    "                    'lab': lab,\n",
    "                    'ARC': stats['ARC'],\n",
    "                    'L2ARC': stats['L2ARC']\n",
    "                })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Uruchomienie\n",
    "arcstats_raw_results = collect_arcstats_raw('../wyniki_zwykle_zfs_stats')\n",
    "arcstats_raw_results_limit = collect_arcstats_raw('../wyniki_zwykle_zfs_stats_limit')\n",
    "\n",
    "# Zamiana kluczy zfs -> zfs_limit oraz zfs_l2arc -> zfs_l2arc_limit w arcstats_raw_results_limit\n",
    "if 'zfs' in arcstats_raw_results_limit:\n",
    "    arcstats_raw_results_limit['zfs_limit'] = arcstats_raw_results_limit.pop('zfs')\n",
    "if 'zfs_l2arc' in arcstats_raw_results_limit:\n",
    "    arcstats_raw_results_limit['zfs_l2arc_limit'] = arcstats_raw_results_limit.pop('zfs_l2arc')\n",
    "\n",
    "deep_merge_results(arcstats_raw_results, arcstats_raw_results_limit)\n",
    "# Przykładowe wypisanie\n",
    "for fs, fs_data in arcstats_raw_results.items():\n",
    "    for block, block_data in fs_data.items():\n",
    "        for comp, comp_data in block_data.items():\n",
    "            for storage, storage_data in comp_data.items():\n",
    "                for workload, data in storage_data.items():\n",
    "                    print(f\"{fs}/{block}/{comp}/{storage}/{workload}: {data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1ba17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_arcstats_pie_from_results(arcstats_results, save_dir=None,\n",
    "                                   workloads=None, file_systems=None,\n",
    "                                   compressions=None, block_sizes=None,\n",
    "                                   storage_types=None,\n",
    "                                   lab=None,\n",
    "                                   size_unit='MB',\n",
    "                                   debug=True):\n",
    "    plt.style.use('ggplot')  # Nowoczesny styl\n",
    "\n",
    "    size_divisor = 1024**2 if size_unit.upper() == 'MB' else 1024**3\n",
    "    size_unit_label = 'MiB' if size_unit.upper() == 'MB' else 'GiB'\n",
    "\n",
    "    for fs, fs_data in arcstats_results.items():\n",
    "        if file_systems and fs not in file_systems:\n",
    "            continue\n",
    "        for blocksize, block_data in fs_data.items():\n",
    "            if block_sizes and blocksize not in block_sizes:\n",
    "                continue\n",
    "            for compression, comp_data in block_data.items():\n",
    "                if compressions and compression not in compressions:\n",
    "                    continue\n",
    "                for storage, storage_data in comp_data.items():\n",
    "                    if storage_types and storage not in storage_types:\n",
    "                        continue\n",
    "                    for workload, entries in storage_data.items():\n",
    "                        if workloads and workload not in workloads:\n",
    "                            continue\n",
    "\n",
    "                        # 🔎 Wybór laboratorium\n",
    "                        if lab in [None, 'median', 'best', 'worst']:\n",
    "                            ratios = []\n",
    "                            for e in entries:\n",
    "                                hits = e['ARC']['hits']\n",
    "                                misses = e['ARC']['misses']\n",
    "                                total = hits + misses\n",
    "                                ratio = hits / total if total > 0 else 0\n",
    "                                ratios.append((ratio, e))\n",
    "                            ratios.sort(key=lambda x: x[0])\n",
    "                            if lab == 'best':\n",
    "                                selected = ratios[-1][1]\n",
    "                            elif lab == 'worst':\n",
    "                                selected = ratios[0][1]\n",
    "                            else:  # median lub None\n",
    "                                selected = ratios[len(ratios) // 2][1]\n",
    "                        else:\n",
    "                            selected = next((e for e in entries if e['lab'] == lab), None)\n",
    "                            if not selected:\n",
    "                                print(f\"⚠️ Brak wyników dla lab {lab} w {workload}\")\n",
    "                                continue\n",
    "\n",
    "                        if debug:\n",
    "                            print(f\"🟩 Rysuję: {fs}/{blocksize}/{compression}/{storage}/{workload} | lab={selected['lab']}\")\n",
    "\n",
    "                        fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "                        for idx, cache_type in enumerate(['ARC', 'L2ARC']):\n",
    "                            hits = selected[cache_type]['hits']\n",
    "                            misses = selected[cache_type]['misses']\n",
    "                            size = selected[cache_type]['size']\n",
    "                            limit = selected[cache_type].get('limit', 0) if cache_type == 'ARC' else None\n",
    "\n",
    "                            if hits == 0 and misses == 0:\n",
    "                                axs[idx].axis('off')\n",
    "                                axs[idx].text(0.5, 0.5, f'Brak danych dla {cache_type}', ha='center', va='center', fontsize=14)\n",
    "                                continue\n",
    "\n",
    "                            wedges, texts, autotexts = axs[idx].pie(\n",
    "                                [hits, misses],\n",
    "                                labels=['Hits', 'Misses'],\n",
    "                                autopct='%1.1f%%',\n",
    "                                startangle=90,\n",
    "                                colors=['#4CAF50', '#F44336'],\n",
    "                                wedgeprops={'edgecolor': 'white', 'linewidth': 1.5}\n",
    "                            )\n",
    "                            for text in texts + autotexts:\n",
    "                                text.set_fontsize(12)\n",
    "\n",
    "                            size_val = round(size / size_divisor, 2)\n",
    "                            title_line = f\"{cache_type}\\nHits: {hits:,} | Misses: {misses:,}\\n\"\n",
    "                            if limit and limit > 0:\n",
    "                                limit_val = round(limit / size_divisor, 2)\n",
    "                                usage_pct = round(100 * size / limit, 1)\n",
    "                                title_line += f\"Size: {size_val}/{limit_val} {size_unit_label} ({usage_pct}%)\"\n",
    "                            else:\n",
    "                                title_line += f\"Size: {size_val} {size_unit_label}\"\n",
    "                            axs[idx].set_title(title_line, fontsize=13)\n",
    "\n",
    "                        fig.suptitle(\n",
    "                            f\"{workload.capitalize()} | {fs.upper()} | Blocksize: {blocksize} | Compression: {compression} | Storage: {storage} | Lab: {selected['lab']}\",\n",
    "                            fontsize=14,\n",
    "                            y=1.02\n",
    "                        )\n",
    "                        plt.subplots_adjust(top=0.85)\n",
    "                        plt.tight_layout()\n",
    "\n",
    "                        if save_dir:\n",
    "                            os.makedirs(save_dir, exist_ok=True)\n",
    "                            safe_fs = fs.replace(\"/\", \"_\")\n",
    "                            safe_file = f\"{safe_fs}_{blocksize}_{compression}_{storage}_{workload}_{selected['lab']}.png\"\n",
    "                            path = os.path.join(save_dir, safe_file)\n",
    "                            plt.savefig(path, bbox_inches='tight', dpi=150)\n",
    "                            print(f\"📁 Zapisano wykres: {path}\")\n",
    "                        else:\n",
    "                            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb516a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arcstats_pie_from_results(\n",
    "    arcstats_raw_results,\n",
    "    workloads=['database', 'webserver', 'multimedia', 'archive'],\n",
    "    file_systems=['zfs_limit'],\n",
    "    lab='worst',\n",
    "    compressions=['none'],\n",
    "    block_sizes=['default'],\n",
    "    storage_types=['hdd', 'nvme'],\n",
    "    size_unit='GB',  # Albo 'MB'\n",
    "    debug=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2453f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def plot_arcstats_bar_from_results_final(arcstats_results, save_dir=None, save=False,\n",
    "                                         workloads=None, file_systems=None,\n",
    "                                         compressions=None, block_sizes=None,\n",
    "                                         storage_types=None,\n",
    "                                         lab='best', same_lab_for_all=False,\n",
    "                                         show_lab=False,\n",
    "                                         size_unit='MB', debug=True,\n",
    "                                         fill_to_100=False):\n",
    "\n",
    "    size_divisor = 1024**2 if size_unit.upper() == 'MB' else 1024**3\n",
    "    size_unit_label = 'MiB' if size_unit.upper() == 'MB' else 'GiB'\n",
    "\n",
    "    for storage in storage_types or []:\n",
    "        for compression in compressions or []:\n",
    "            for blocksize in block_sizes or []:\n",
    "                for workload in workloads or []:\n",
    "                    bar_labels = []\n",
    "                    arc_ratios = []\n",
    "                    l2arc_ratios = []\n",
    "                    size_info = []\n",
    "                    arc_hits_list = []\n",
    "                    l2arc_hits_list = []\n",
    "                    arc_miss_list = []\n",
    "                    l2arc_miss_list = []\n",
    "\n",
    "                    selected_lab = None\n",
    "\n",
    "                    # Wybór wspólnego laba jeśli trzeba\n",
    "                    if same_lab_for_all:\n",
    "                        all_entries = []\n",
    "                        for fs in file_systems or []:\n",
    "                            entries = arcstats_results.get(fs, {}).get(blocksize, {}).get(compression, {}).get(storage, {}).get(workload, [])\n",
    "                            all_entries.extend(entries)\n",
    "                        if not all_entries:\n",
    "                            continue\n",
    "                        ratios = [(e['ARC']['hits'] / (e['ARC']['hits'] + e['ARC']['misses']), e)\n",
    "                                  for e in all_entries if (e['ARC']['hits'] + e['ARC']['misses']) > 0]\n",
    "                        ratios.sort(key=lambda x: x[0])\n",
    "                        if lab == 'best':\n",
    "                            selected_lab = ratios[-1][1]['lab']\n",
    "                        elif lab == 'worst':\n",
    "                            selected_lab = ratios[0][1]['lab']\n",
    "                        else:\n",
    "                            selected_lab = ratios[len(ratios)//2][1]['lab'] if lab == 'median' else lab\n",
    "\n",
    "                    # Zbierz dane dla każdego fs\n",
    "                    for fs in file_systems or []:\n",
    "                        entries = arcstats_results.get(fs, {}).get(blocksize, {}).get(compression, {}).get(storage, {}).get(workload, [])\n",
    "                        if not entries:\n",
    "                            continue\n",
    "                        if same_lab_for_all:\n",
    "                            selected = next((e for e in entries if e['lab'] == selected_lab), None)\n",
    "                        else:\n",
    "                            ratios = [(e['ARC']['hits'] / (e['ARC']['hits'] + e['ARC']['misses']), e)\n",
    "                                      for e in entries if (e['ARC']['hits'] + e['ARC']['misses']) > 0]\n",
    "                            ratios.sort(key=lambda x: x[0])\n",
    "                            if not ratios:\n",
    "                                continue\n",
    "                            if lab == 'best':\n",
    "                                selected = ratios[-1][1]\n",
    "                            elif lab == 'worst':\n",
    "                                selected = ratios[0][1]\n",
    "                            elif lab == 'median':\n",
    "                                selected = ratios[len(ratios)//2][1]\n",
    "                            else:\n",
    "                                selected = next((e for e in entries if e['lab'] == lab), None)\n",
    "                        if not selected:\n",
    "                            continue\n",
    "\n",
    "                        total = selected['ARC']['hits'] + selected['ARC']['misses']\n",
    "                        arc_ratio = 100 * selected['ARC']['hits'] / total if total > 0 else 0\n",
    "\n",
    "                        l2_total = selected['L2ARC']['hits'] + selected['L2ARC']['misses']\n",
    "                        l2_ratio = 100 * selected['L2ARC']['hits'] / l2_total if l2_total > 0 else None\n",
    "\n",
    "                        if show_lab:\n",
    "                            label_with_lab = f\"{fs_translations.get(fs,fs)} ({selected['lab']})\"\n",
    "                        else:\n",
    "                            label_with_lab = fs_translations.get(fs,fs)\n",
    "                        bar_labels.append(label_with_lab)\n",
    "                        arc_ratios.append(arc_ratio)\n",
    "                        l2arc_ratios.append(l2_ratio)\n",
    "                        size_val = round(selected['ARC']['size'] / size_divisor, 2)\n",
    "                        limit_val = round(selected['ARC'].get('limit', 0) / size_divisor, 2)\n",
    "                        size_info.append(f\"{size_val}/{limit_val} {size_unit_label}\")\n",
    "                        arc_hits_list.append(selected['ARC']['hits'])\n",
    "                        l2arc_hits_list.append(selected['L2ARC']['hits'] if 'L2ARC' in selected and selected['L2ARC']['hits'] else None)\n",
    "                        arc_miss_list.append(selected['ARC']['misses'])\n",
    "                        l2arc_miss_list.append(selected['L2ARC']['misses'] if 'L2ARC' in selected and selected['L2ARC']['misses'] else None)\n",
    "\n",
    "                    if not bar_labels:\n",
    "                        continue\n",
    "\n",
    "                    l2arc_exists = any(r is not None for r in l2arc_ratios)\n",
    "\n",
    "                    x = np.arange(len(bar_labels))\n",
    "                    width = 0.35 if l2arc_exists else 0.5\n",
    "\n",
    "                    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "                    # Kolory\n",
    "                    arc_color = '#6BA368'\n",
    "                    arc_bg_color = '#E6E6E6'\n",
    "                    l2arc_color = '#4C72B0'\n",
    "                    l2arc_bg_color = '#D0D0D0'\n",
    "                    fill_arc_color = '#FF9999'    # jaśniejszy czerwony\n",
    "                    fill_l2arc_color = '#FF4D4D'  # ciemniejszy czerwony\n",
    "\n",
    "                    # ARC słupki\n",
    "                    bars_arc = ax.bar(x, arc_ratios, width, color=arc_color, label='ARC - trafienia', edgecolor='black', zorder=3)\n",
    "                    ax.bar(x, [100]*len(bar_labels), width, color=arc_bg_color, alpha=0.3, zorder=0)\n",
    "                    if fill_to_100:\n",
    "                        fill_arc_heights = [100 if m and m > 0 else 0 for m in arc_miss_list]\n",
    "                        ax.bar(x, fill_arc_heights, width, color=fill_arc_color, alpha=0.3, zorder=1, label='ARC - chybienia')\n",
    "\n",
    "                    # L2ARC słupki\n",
    "                    if l2arc_exists:\n",
    "                        bars_l2arc = ax.bar(x + width, [r if r is not None else 0 for r in l2arc_ratios],\n",
    "                                            width, color=l2arc_color, label='L2ARC - trafienia', edgecolor='black', zorder=3)\n",
    "                        ax.bar(x + width, [100]*len(bar_labels), width, color=l2arc_bg_color, alpha=0.3, zorder=0)\n",
    "                        if fill_to_100:\n",
    "                            fill_l2arc_heights = [100 if m and m > 0 else 0 for m in l2arc_miss_list]\n",
    "                            ax.bar(x + width, fill_l2arc_heights, width, color=fill_l2arc_color, alpha=0.3, zorder=1, label='L2ARC - chybienia')\n",
    "\n",
    "\n",
    "                    # Procenty + ilość w słupkach\n",
    "                    bar_data = [(bars_arc, arc_ratios, arc_hits_list, arc_miss_list)]\n",
    "                    # Jeśli L2ARC istnieje, dodaj ją\n",
    "                    if l2arc_exists:\n",
    "                        bar_data.append((bars_l2arc, l2arc_ratios, l2arc_hits_list, l2arc_miss_list)) \n",
    "\n",
    "                    for bars, ratios, hits_list, miss_list in bar_data:\n",
    "                        for bar, ratio, hits, misses in zip(bars, ratios, hits_list, miss_list):\n",
    "                            if ratio is None:\n",
    "                                continue\n",
    "                            bar_x_center = bar.get_x() + bar.get_width() / 2\n",
    "\n",
    "                            if ratio > 5:\n",
    "                                text = f\"{ratio:.1f}%\\n({hits:,})\"\n",
    "                                font_size = 9 if len(bar_data)<2 else 7\n",
    "                                ax.text(bar_x_center, ratio / 2,\n",
    "                                        text,\n",
    "                                        ha='center', va='center',\n",
    "                                        color='white', fontsize=font_size, fontweight='bold')\n",
    "\n",
    "                            # Dodaj etykietę dla \"czerwonej\" części jeśli jest >5%\n",
    "                            miss_ratio = 100 - ratio\n",
    "                            if fill_to_100 and miss_ratio > 5:\n",
    "                                text = f\"{miss_ratio:.1f}%\\n({misses:,})\"\n",
    "                                font_size = 9 if len(bar_data)<2 else 7\n",
    "                                ax.text(bar_x_center, ratio + miss_ratio / 2,\n",
    "                                        text,\n",
    "                                        ha='center', va='center',\n",
    "                                        color='black', fontsize=font_size, fontweight='bold')\n",
    "\n",
    "\n",
    "\n",
    "                    # Oś X\n",
    "                    ax.set_xticks(x + (width/2 if l2arc_exists else 0))\n",
    "                    ax.set_xticklabels(bar_labels, rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "                    # Opisy\n",
    "                    ax.set_ylim(0, 110)\n",
    "                    ax.set_ylabel('Wskaźnik trafień (%)', fontsize=12)\n",
    "\n",
    "                    workload_label = workload_translations.get(workload, workload)\n",
    "                    compression_label = compression_translations.get(compression, compression)\n",
    "                    storage_label = storage_type_translations.get(storage, storage)\n",
    "                    blocksize_label = \"domyślny\" if blocksize == \"default\" else blocksize\n",
    "\n",
    "                    ax.set_title(\n",
    "                        f\"{workload_label.upper()} | kompresja: {compression_label} | blok: {blocksize_label} | nośnik: {storage_label}\",\n",
    "                        fontsize=13, pad=15\n",
    "                    )\n",
    "\n",
    "                    # Rozmiar ARC nad słupkami\n",
    "                    for i, txt in enumerate(size_info):\n",
    "                        ax.text(x[i] + (width/2 if l2arc_exists else 0), 105, f\"Rozmiar: {txt}\",\n",
    "                                ha='center', va='bottom', fontsize=9, color='black')\n",
    "\n",
    "                    ax.legend(loc='lower right', fontsize=9)\n",
    "                    ax.grid(axis='y', linestyle='--', alpha=0.5, zorder=0)\n",
    "\n",
    "                    plt.tight_layout()\n",
    "                    if save_dir and save:\n",
    "                        safe_file_systems = \"_\".join([fs.replace(\"_\", \"\") for fs in file_systems])\n",
    "                        filename = f\"arcstats_{safe_file_systems}_{storage}_{blocksize.lower()}_{compression}_{workload.lower()}.png\"\n",
    "                        print_latex_image(save_dir, filename)\n",
    "                        os.makedirs(\"wykresy/\"+save_dir, exist_ok=True)\n",
    "                        path = os.path.join(\"wykresy/\"+save_dir, filename)\n",
    "                        plt.savefig(path, bbox_inches='tight', dpi=150)\n",
    "                        if debug:\n",
    "                            print(f\"📁 Zapisano wykres: {path}\")\n",
    "                        plt.show()\n",
    "                    else:\n",
    "                        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4917748",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arcstats_bar_from_results_final(\n",
    "    arcstats_results=arcstats_raw_results,\n",
    "    workloads=['database', 'mulitmedia', 'archive', 'webserver'],  # Jeśli chcesz ograniczyć do wybranych\n",
    "    file_systems=['zfs', 'zfs_limit', 'zfs_l2arc', 'zfs_l2arc_limit'],  # Jeśli chcesz ograniczyć do wybranych\n",
    "    compressions=['none'],\n",
    "    block_sizes=['default'],  # Jeśli chcesz ograniczyć do wybranych\n",
    "    storage_types=['nvme', 'hdd'],  # Osobne wykresy dla tych storage\n",
    "    lab='median',    # Który komputer ma wybrać do pokazania\n",
    "    show_lab=False,  # Czy pokazywać lab w systemach do debugowania\n",
    "    size_unit='GB',\n",
    "    same_lab_for_all=False,  # \"same\" lub \"individual\"\n",
    "    debug=False,\n",
    "    fill_to_100=True,\n",
    "    save=False,\n",
    "    save_dir=\"arcstats\"  # Ścieżka do katalogu, gdzie zapisać wykresy\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
