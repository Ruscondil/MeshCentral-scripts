{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff78de34",
   "metadata": {},
   "source": [
    "Czytanie plik贸w fio i dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecadccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_rgb\n",
    "from matplotlib.patches import Patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0aa936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict_tree(d, indent=0, max_depth=None):\n",
    "    \"\"\"Rekurencyjnie wypisuje struktur kluczy zagnie偶d偶onego sownika do zadanej gbokoci.\"\"\"\n",
    "    if not isinstance(d, dict) or (max_depth is not None and indent >= max_depth):\n",
    "        return\n",
    "    for key in d:\n",
    "        print('  ' * indent + str(key))\n",
    "        print_dict_tree(d[key], indent + 1, max_depth)\n",
    "\n",
    "# Przykad u偶ycia:\n",
    "# print_dict_tree(fio_resultsdict, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac3bfb1-c53b-41c1-9991-c573de3694b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fio_results(file_path):\n",
    "    def parse_text_fio(file):\n",
    "        # Regular expressions\n",
    "        bandwidth_regex = re.compile(r'WRITE: bw=(\\d+(?:\\.\\d+)?)([MK]iB/s)')\n",
    "        bandwidth_read_regex = re.compile(r'READ: bw=(\\d+(?:\\.\\d+)?)([MK]iB/s)')\n",
    "        iops_regex = re.compile(r'write: IOPS=(\\d+)')\n",
    "        iops_read_regex = re.compile(r'read: IOPS=(\\d+)')\n",
    "        latency_regex = re.compile(r'lat (\\([mu]sec\\)): min=\\d+\\.?\\d*[km]?, max=\\d+\\.?\\d*[km]?, avg=(\\d+\\.\\d+[km]?), stdev=\\d+\\.?\\d*')\n",
    "\n",
    "        def convert_bandwidth(value, unit):\n",
    "            value = float(value)\n",
    "            if unit == \"KiB/s\":\n",
    "                return value / 1024\n",
    "            return value\n",
    "\n",
    "        results = {}\n",
    "        last = 'read'\n",
    "        for line in file:\n",
    "            if 'write' in line:\n",
    "                last = 'write'\n",
    "            elif 'read' in line:\n",
    "                last = 'read'\n",
    "\n",
    "            if (bw := bandwidth_regex.search(line)):\n",
    "                value, unit = bw.groups()\n",
    "                results['Bandwidth WRITE (MiB/s)'] = convert_bandwidth(value, unit)\n",
    "            if (bw := bandwidth_read_regex.search(line)):\n",
    "                value, unit = bw.groups()\n",
    "                results['Bandwidth READ (MiB/s)'] = convert_bandwidth(value, unit)\n",
    "            if (iops := iops_regex.search(line)):\n",
    "                results['IOPS WRITE'] = float(iops.group(1))\n",
    "            if (iops := iops_read_regex.search(line)):\n",
    "                results['IOPS READ'] = float(iops.group(1))\n",
    "            if (lat := latency_regex.search(line)):\n",
    "                lat_val = float(lat.group(2))\n",
    "                if lat.group(1) == '(usec)':\n",
    "                    lat_val /= 1000\n",
    "                results[f'Latency {last.upper()} (ms)'] = lat_val\n",
    "\n",
    "        return results\n",
    "\n",
    "    def parse_json_fio(data):\n",
    "        results = {}\n",
    "        for job in data.get('jobs', []):\n",
    "            for rw_type in ['read', 'write']:\n",
    "                if job.get(rw_type, {}).get('iops', 0) > 0:\n",
    "                    iops = job[rw_type]['iops']\n",
    "                    bw_kib = job[rw_type]['bw']\n",
    "                    latency_ns = job[rw_type].get('lat_ns', {}).get('mean', 0)\n",
    "\n",
    "                    results[f'IOPS {rw_type.upper()}'] = round(iops, 2)\n",
    "                    results[f'Bandwidth {rw_type.upper()} (MiB/s)'] = round(bw_kib / 1024, 2)\n",
    "                    results[f'Latency {rw_type.upper()} (ms)'] = round(latency_ns / 1_000_000, 3)\n",
    "\n",
    "        return results\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        first_char = f.read(1)\n",
    "        f.seek(0)\n",
    "        if first_char == '{':\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                return parse_json_fio(data)\n",
    "            except json.JSONDecodeError:\n",
    "                f.seek(0)\n",
    "                return parse_text_fio(f)\n",
    "        else:\n",
    "            return parse_text_fio(f)\n",
    "\n",
    "\n",
    "def parse_dd_results(file_path):\n",
    "    # Regular expressions\n",
    "    bandwidth_regex = re.compile(r'(\\d+(?:\\.\\d+)?) ([GMK]B/s)')\n",
    "    time_regex = re.compile(r'(\\d+(?:\\.\\d+)?) s')\n",
    "\n",
    "    # Function to convert bandwidth to MiB/s\n",
    "    def convert_bandwidth(value, unit):\n",
    "        value = float(value)\n",
    "        if unit == \"KB/s\":\n",
    "            return value / 1024  # Convert KB/s to MiB/s\n",
    "        elif unit == \"MB/s\":\n",
    "            return value  # Already in MiB/s\n",
    "        elif unit == \"GB/s\":\n",
    "            return value * 1024  # Convert GB/s to MiB/s\n",
    "        return value\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Match bandwidth\n",
    "            bw_match = bandwidth_regex.search(line)\n",
    "            if bw_match:\n",
    "                value, unit = bw_match.groups()\n",
    "                if 'write' in file_path:\n",
    "                    results['Bandwidth WRITE (MiB/s)'] = convert_bandwidth(value, unit)\n",
    "                else:\n",
    "                    results['Bandwidth READ (MiB/s)'] = convert_bandwidth(value, unit)\n",
    "\n",
    "            # Match time\n",
    "            time_match = time_regex.search(line)\n",
    "            if time_match:\n",
    "                results['Time (s)'] = float(time_match.group(1))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_values(resultsfolder, file_names, parser, program_type):\n",
    "    resultsdict = defaultdict(lambda: defaultdict(dict))\n",
    "    \n",
    "    # Funkcja pomocnicza do wyodrbnienia nazwy od drugiego sowa do sowa \"test\"\n",
    "    def extract_key(file_name):\n",
    "        match = re.match(r\"^[^_]+_(.*?)_test\", file_name)  # Pomija pierwsze sowo przed \"_\"\n",
    "        return match.group(1) if match else os.path.splitext(file_name)[0]\n",
    "\n",
    "    prepaths = [folder for folder in glob.glob(resultsfolder + '*/') if program_type in folder]  # Filtruj wedug typu programu\n",
    "    for prepath in prepaths:\n",
    "        # Wyodrbnij system plik贸w i typ pamici, uwzgldniajc podkrelenia w nazwach\n",
    "        folder_parts = prepath.split('\\\\')[-2].split('_')\n",
    "        filesystem = '_'.join(folder_parts[2:-1])  # Wszystkie czci midzy 2 a ostatni\n",
    "        storage = folder_parts[-1]  # Ostatnia cz to typ pamici\n",
    "        \n",
    "        # Sprawd藕, czy typ programu u偶ywa folder贸w z rozmiarem blok贸w\n",
    "        if program_type in ['fio_results', 'dd_results']:\n",
    "            block_size_folders = [folder for folder in glob.glob(prepath + '*/')]  # Uwzgldnij foldery z rozmiarem blok贸w\n",
    "            for block_size_folder in block_size_folders:\n",
    "                # Bezpiecznie wyodrbnij rozmiar bloku\n",
    "                folder_parts = block_size_folder.split('\\\\')[-2].split('_')\n",
    "                if len(folder_parts) > 2 and folder_parts[0] == \"block\" and folder_parts[1] == \"size\":\n",
    "                    block_size = folder_parts[2]\n",
    "                else:\n",
    "                    print(f\"Pomijanie folderu o nieoczekiwanej strukturze: {block_size_folder}\")\n",
    "                    continue\n",
    "\n",
    "                folders = [folder for folder in glob.glob(block_size_folder + '*/')]\n",
    "                cumulative_data = {}\n",
    "                for folder in folders:\n",
    "                    for file_name in file_names:\n",
    "                        file_path = os.path.join(folder, file_name)\n",
    "                        if os.path.exists(file_path):\n",
    "                            try:\n",
    "                                results = parser(file_path)\n",
    "                                if results:  # Dodaj tylko, jeli s jakie dane\n",
    "                                    test_key = extract_key(file_name)\n",
    "                                    if test_key not in cumulative_data:\n",
    "                                        cumulative_data[test_key] = defaultdict(list)\n",
    "                                    for key, value in results.items():\n",
    "                                        cumulative_data[test_key][key].append(value)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Bd podczas parsowania {file_path}: {e}\")\n",
    "                        else:\n",
    "                            print(f\"Plik nie znaleziony: {file_path}\")\n",
    "\n",
    "                ranges = {}\n",
    "                for test_key, metrics in cumulative_data.items():\n",
    "                    if metrics:  # Dodaj tylko, jeli s jakie dane\n",
    "                        ranges[test_key] = {\n",
    "                            key: {'min': round(min(values), 3), 'max': round(max(values), 3), 'avg': round(sum(values) / len(values), 2)} if values else '-'\n",
    "                            for key, values in metrics.items()\n",
    "                        }\n",
    "                if block_size not in resultsdict[filesystem]:\n",
    "                    resultsdict[filesystem][block_size] = {}\n",
    "                resultsdict[filesystem][block_size][storage] = ranges\n",
    "        else:\n",
    "            # Obsuga program贸w bez folder贸w z rozmiarem blok贸w (np. hdparm)\n",
    "            folders = [folder for folder in glob.glob(prepath + '*/')]\n",
    "            cumulative_data = {}\n",
    "            for folder in folders:\n",
    "                for file_name in file_names:\n",
    "                    file_path = os.path.join(folder, file_name)\n",
    "                    if os.path.exists(file_path):\n",
    "                        try:\n",
    "                            results = parser(file_path)\n",
    "                            if results:  # Dodaj tylko, jeli s jakie dane\n",
    "                                test_key = extract_key(file_name)\n",
    "                                if test_key not in cumulative_data:\n",
    "                                    cumulative_data[test_key] = defaultdict(list)\n",
    "                                for key, value in results.items():\n",
    "                                    cumulative_data[test_key][key].append(value)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Bd podczas parsowania {file_path}: {e}\")\n",
    "                    else:\n",
    "                        print(f\"Plik nie znaleziony: {file_path}\")\n",
    "\n",
    "            ranges = {}\n",
    "            for test_key, metrics in cumulative_data.items():\n",
    "                if metrics:  # Dodaj tylko, jeli s jakie dane\n",
    "                    ranges[test_key] = {\n",
    "                        key: {'min': round(min(values), 3), 'max': round(max(values), 3), 'avg': round(sum(values) / len(values), 2)} if values else '-'\n",
    "                        for key, values in metrics.items()\n",
    "                    }\n",
    "            if 'no_block_size' not in resultsdict[filesystem]:\n",
    "                resultsdict[filesystem]['no_block_size'] = {}\n",
    "            resultsdict[filesystem]['no_block_size'][storage] = ranges\n",
    "    return resultsdict\n",
    "\n",
    "def parse_hdparm_results(file_path):\n",
    "    # Regular expression to match the bandwidth\n",
    "    bandwidth_regex = re.compile(r'Timing O_DIRECT disk reads: (\\d+(?:\\.\\d+)?) MB in .* seconds = (\\d+(?:\\.\\d+)) MB/sec')\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Match bandwidth\n",
    "            bw_match = bandwidth_regex.search(line)\n",
    "            if bw_match:\n",
    "                total_mb, bandwidth = bw_match.groups()\n",
    "                results['Total Data Read (MB)'] = float(total_mb)\n",
    "                results['Bandwidth (MiB/s)'] = float(bandwidth)\n",
    "\n",
    "    return results\n",
    "    \n",
    "def extract_hdparm_values_by_device(resultsfolder, file_names, parser, group_by_computer=False):\n",
    "    resultsdict = defaultdict(lambda: defaultdict(lambda: defaultdict(list))) if group_by_computer else defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    # Iterate through all folders in the results folder\n",
    "    prepaths = glob.glob(resultsfolder + '*/')  # Get all subfolders\n",
    "    for prepath in prepaths:\n",
    "        # Extract device type from the folder name (e.g., \"hdparm_results_xfs_nvme\")\n",
    "        device_type = prepath.split('_')[-1].lower().strip('\\\\')  # Extract \"nvme\", \"ssd\", etc., and remove trailing slashes\n",
    "        \n",
    "        # Iterate through subfolders for each computer\n",
    "        folders = glob.glob(prepath + '*/')  # Get subfolders for each computer\n",
    "        for folder in folders:\n",
    "            computer_name = folder.split('\\\\')[-2]  # Extract computer name (e.g., \"lab-sec-13\")\n",
    "            for file_name in file_names:\n",
    "                file_path = os.path.join(folder, file_name)\n",
    "                if os.path.exists(file_path):\n",
    "                    try:\n",
    "                        # Parse the file and collect results\n",
    "                        results = parser(file_path)\n",
    "                        if group_by_computer:\n",
    "                            for key, value in results.items():\n",
    "                                resultsdict[device_type][computer_name][key].append(value)\n",
    "                        else:\n",
    "                            for key, value in results.items():\n",
    "                                resultsdict[device_type][key].append(value)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing {file_path}: {e}\")\n",
    "                else:\n",
    "                    print(f\"File not found: {file_path}\")\n",
    "\n",
    "    # Aggregate results by calculating min, max, and avg for each metric\n",
    "    aggregated_results = {}\n",
    "    for device_type, computers_or_metrics in resultsdict.items():\n",
    "        if group_by_computer:\n",
    "            aggregated_results[device_type] = {}\n",
    "            for computer, metrics in computers_or_metrics.items():\n",
    "                aggregated_results[device_type][computer] = {\n",
    "                    key: {\n",
    "                        'min': round(min(values), 3),\n",
    "                        'max': round(max(values), 3),\n",
    "                        'avg': round(sum(values) / len(values), 2)\n",
    "                    } if values else '-' for key, values in metrics.items()\n",
    "                }\n",
    "        else:\n",
    "            aggregated_results[device_type] = {\n",
    "                key: {\n",
    "                    'min': round(min(values), 3),\n",
    "                    'max': round(max(values), 3),\n",
    "                    'avg': round(sum(values) / len(values), 2)\n",
    "                } if values else '-' for key, values in computers_or_metrics.items()\n",
    "            }\n",
    "\n",
    "    return aggregated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9363a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage for fio\n",
    "fio_file_names = [\n",
    "    #'fio_database_test_output.txt',\n",
    "    #'fio_multimedia_test_output.txt',\n",
    "    #'fio_webserver_test_output.txt',\n",
    "    #'fio_archive_test_output.txt',\n",
    "    'fio_database_article_test_output.txt',\n",
    "    'fio_seq_read_article_test_output.txt',\n",
    "    'fio_seq_write_article_test_output.txt',\n",
    "]\n",
    "\n",
    "fio_resultsdict = extract_values('../wyniki_10G_article/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "print(fio_resultsdict)\n",
    "\n",
    "# Example usage for dd\n",
    "dd_file_names = [\n",
    "    'dd_read_test_output.txt',\n",
    "    'dd_write_test_output.txt',\n",
    "]\n",
    "\n",
    "dd_resultsdict = extract_values('../wyniki_10G_article/', dd_file_names, parse_dd_results, program_type='dd_results')\n",
    "print(dd_resultsdict)\n",
    "\n",
    "hdparm_file_names = [\n",
    "    'hdparm_test_output.txt',\n",
    "]\n",
    "\n",
    "hdparm_resultsdict = extract_hdparm_values_by_device('../wyniki_10G_article/', hdparm_file_names, parse_hdparm_results, group_by_computer=False)\n",
    "\n",
    "print(hdparm_resultsdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef3785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_merge_results(dict1, dict2):\n",
    "    \"\"\"Rekurencyjnie scala dict2 do dict1 na gboko 3 poziom贸w.\"\"\"\n",
    "    for fs, block_sizes in dict2.items():\n",
    "        if fs not in dict1:\n",
    "            dict1[fs] = block_sizes\n",
    "            continue\n",
    "        for block_size, storages in block_sizes.items():\n",
    "            if block_size not in dict1[fs]:\n",
    "                dict1[fs][block_size] = storages\n",
    "                continue\n",
    "            for storage, workloads in storages.items():\n",
    "                if storage not in dict1[fs][block_size]:\n",
    "                    dict1[fs][block_size][storage] = workloads\n",
    "                else:\n",
    "                    # Jeli istnieje, scal workloady (np. database, multimedia, itd.)\n",
    "                    for workload, metrics in workloads.items():\n",
    "                        if workload not in dict1[fs][block_size][storage]:\n",
    "                            dict1[fs][block_size][storage][workload] = metrics\n",
    "                        else:\n",
    "                            # Jeli istnieje, scal metryki (np. Bandwidth READ, IOPS, itd.)\n",
    "                            dict1[fs][block_size][storage][workload].update(metrics)\n",
    "\n",
    "\n",
    "fio_file_names = [\n",
    "    'fio_database_test_output.json',\n",
    "    'fio_multimedia_test_output.json',\n",
    "    'fio_webserver_test_output.json',\n",
    "    'fio_archive_test_output.json',\n",
    "    #'fio_database_article_test_output.txt',\n",
    "    #'fio_seq_read_article_test_output.txt',\n",
    "    #'fio_seq_write_article_test_output.txt',\n",
    "]\n",
    "\n",
    "#Scalanie r贸偶nych folder贸w wynik贸w\n",
    "\n",
    "# Usu dane dla zfs i zfs_nocache\n",
    "fio_resultsdict.pop('zfs', None)\n",
    "fio_resultsdict.pop('zfs_nocache', None)\n",
    "fio_resultsdict.pop('zfs_primary', None)\n",
    "\n",
    "# Wczytaj dane z drugiego folderu\n",
    "#fio_resultsdict_new = extract_values('../wyniki_zwykle_l2arc/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "fio_resultsdict_nowe_zfs = extract_values('../wyniki_zwykle_nowe_zfs/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "fio_resultsdict_raid = extract_values('../wyniki_raid/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "# Dodaj dane z drugiego folderu do g贸wnego zbioru\n",
    "#fio_resultsdict.update(fio_resultsdict_new)\n",
    "deep_merge_results(fio_resultsdict, fio_resultsdict_nowe_zfs)\n",
    "deep_merge_results(fio_resultsdict, fio_resultsdict_raid)\n",
    "print_dict_tree(fio_resultsdict, max_depth=3)\n",
    "# Wywietl zaktualizowany zbi贸r danych\n",
    "print(fio_resultsdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0a3d80",
   "metadata": {},
   "source": [
    "Funkcja do generowania wykres贸w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb5d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def darken_color(color, amount=0.7):\n",
    "    \"\"\"Zmniejsz jasno koloru RGB.\"\"\"\n",
    "    c = np.array(to_rgb(color))\n",
    "    return tuple(np.clip(c * amount, 0, 1))\n",
    "\n",
    "def plot_performance_metrics(data, metrics, storage_types, block_sizes,\n",
    "                              include_min_max=False, workload=None,\n",
    "                              file_systems=None, colors=None,\n",
    "                              hdparm_data=None, hdparm_stat='avg',\n",
    "                              preserve_ylim=False, debug=False,\n",
    "                              combine_storage_types=False, save_dir=None):\n",
    "\n",
    "    metric_translations = {\n",
    "        \"Bandwidth (MiB/s)\": \"Przepustowo (MiB/s)\",\n",
    "        \"Bandwidth READ (MiB/s)\": \"Odczyt (MiB/s)\",\n",
    "        \"Bandwidth WRITE (MiB/s)\": \"Zapis (MiB/s)\",\n",
    "        \"IOPS\": \"IOPS\",\n",
    "        \"IOPS READ\": \"IOPS - odczyt\",\n",
    "        \"IOPS WRITE\": \"IOPS - zapis\",\n",
    "        \"Latency (ms)\": \"Op贸藕nienie (ms)\",\n",
    "        \"Latency READ (ms)\": \"Op贸藕nienie - odczyt (ms)\",\n",
    "        \"Latency WRITE (ms)\": \"Op贸藕nienie - zapis (ms)\"\n",
    "    }\n",
    "\n",
    "    workload_translations = {\n",
    "        \"database\": \"baza danych\",\n",
    "        \"multimedia\": \"multimedia\",\n",
    "        \"webserver\": \"serwer WWW\",\n",
    "        \"archive\": \"archiwum\"\n",
    "    }\n",
    "\n",
    "    storage_type_translations = {\n",
    "        \"NVME\": \"NVMe\",\n",
    "        \"RAID02HDD\": \"RAID0 2 HDD\",\n",
    "        \"RAID04HDD\": \"RAID0 3 HDD\",\n",
    "        \"STRIPE4HDD\": \"Stripe 4 HDD\",\n",
    "        \"STRIPE2HDD\": \"Stripe 2 HDD\",\n",
    "        \"MIRROR\": \"Mirror\",\n",
    "    }\n",
    "\n",
    "    if colors is None:\n",
    "        colors = ['b', 'g', 'r', 'c', 'm', 'y', 'orange', '#FFD700', '#A0522D']\n",
    "    if file_systems is None:\n",
    "        file_systems = list(data.keys())\n",
    "\n",
    "    for block_size in block_sizes:\n",
    "        storage_list = storage_types if not combine_storage_types else [None]\n",
    "        print(storage_list)\n",
    "        for storage in storage_list:\n",
    "            prepared_data = {}\n",
    "\n",
    "            for fs in file_systems:\n",
    "                fs_block_data = data.get(fs, {}).get(block_size) or data.get(fs, {}).get('default')\n",
    "                if not fs_block_data:\n",
    "                    print(f\"锔 Pomijanie {fs}: brak danych dla block_size '{block_size}' i brak 'default'\")\n",
    "                    continue\n",
    "\n",
    "                if combine_storage_types:\n",
    "                    for st in storage_types:\n",
    "                        storage_data = fs_block_data.get(st.lower())\n",
    "                        if not storage_data:\n",
    "                            continue\n",
    "                        workload_data = storage_data.get(workload)\n",
    "                        if not isinstance(workload_data, dict):\n",
    "                            continue\n",
    "                        label = f\"{fs} {storage_type_translations.get(st.upper(), st.upper())}\"\n",
    "                        prepared_data[label] = workload_data\n",
    "                else:\n",
    "                    print(f\"Przygotowanie danych dla {fs} z block_size '{block_size}' i storage '{storage}'\")\n",
    "                    storage_data = fs_block_data.get(storage.lower())\n",
    "                    if not storage_data:\n",
    "                        print(f\"锔 Pomijanie {fs}: brak danych storage '{storage}' w block_size '{block_size}'\")\n",
    "                        continue\n",
    "                    workload_data = storage_data.get(workload)\n",
    "                    if not isinstance(workload_data, dict):\n",
    "                        print(f\"锔 Pomijanie {fs}: brak danych workload '{workload}' dla storage '{storage}' z block_size '{block_size}'\")\n",
    "                        continue\n",
    "                    prepared_data[fs] = workload_data\n",
    "\n",
    "            if not prepared_data:\n",
    "                continue\n",
    "            print(f\"Przygotowane dane: {prepared_data}\")\n",
    "            plots = []\n",
    "            for metric in metrics:\n",
    "                if isinstance(metric, dict):\n",
    "                    read_metric = metric.get(\"read\")\n",
    "                    write_metric = metric.get(\"write\")\n",
    "                    first_key = next(iter(prepared_data), None)\n",
    "                    if not first_key:\n",
    "                        continue\n",
    "                    sample = prepared_data[first_key]\n",
    "                    if read_metric and write_metric and read_metric in sample and write_metric in sample:\n",
    "                        plots.append((\"grouped\", metric[\"name\"], read_metric, write_metric))\n",
    "                    elif read_metric and read_metric in sample:\n",
    "                        plots.append((\"single\", read_metric))\n",
    "                    elif write_metric and write_metric in sample:\n",
    "                        plots.append((\"single\", write_metric))\n",
    "                    else:\n",
    "                        plots.append((\"single\", metric[\"name\"]))\n",
    "                else:\n",
    "                    first_key = next(iter(prepared_data), None)\n",
    "                    if first_key and metric in prepared_data[first_key]:\n",
    "                        plots.append((\"single\", metric))\n",
    "\n",
    "            fig, axs = plt.subplots(len(plots), 1, figsize=(10, 3 * len(plots)))\n",
    "            if len(plots) == 1:\n",
    "                axs = [axs]\n",
    "\n",
    "            # Zwiksz przestrze na tytu, ale zachowaj kontrol nad \"plot area\"\n",
    "            fig.subplots_adjust(top=0.9, bottom=0.15, left=0.15, right=0.95)\n",
    "\n",
    "            # Ustaw stay rozmiar plot area (w jednostkach figure-coordinates)\n",
    "            fixed_ax_height = 0.6 / len(plots)  # rozdziel proporcjonalnie\n",
    "            for i, ax in enumerate(axs):\n",
    "                ax.set_position([0.15, 0.9 - (i+1)*fixed_ax_height, 0.8, fixed_ax_height * 0.9])\n",
    "\n",
    "            \n",
    "            translated_workload = workload_translations.get(workload, workload)\n",
    "            translated_block_size = \"domylny\" if block_size == \"default\" else block_size\n",
    "\n",
    "            combined_label = \", \".join(\n",
    "                [storage_type_translations.get(s.upper(), s.upper()) for s in storage_types]\n",
    "            ) if combine_storage_types else storage_type_translations.get(storage.upper(), storage.upper())\n",
    "\n",
    "            fig.suptitle(f'Typ dysku: {combined_label}\\n Rozmiar bloku: {translated_block_size}'\n",
    "                        f'{f\" - {translated_workload.capitalize()}\" if workload else \"\"}')\n",
    "\n",
    "            for i, plot in enumerate(plots):\n",
    "                ax = axs[i]\n",
    "\n",
    "                if plot[0] == \"grouped\":\n",
    "                    _, base_name, read_metric, write_metric = plot\n",
    "                    fs_labels = []\n",
    "                    read_vals, write_vals = [], []\n",
    "                    read_err, write_err = ([], []), ([], [])\n",
    "                    color_map = {}\n",
    "\n",
    "                    for idx, label in enumerate(prepared_data):\n",
    "                        workload_data = prepared_data[label]\n",
    "                        read_data = workload_data.get(read_metric, {})\n",
    "                        write_data = workload_data.get(write_metric, {})\n",
    "\n",
    "                        read_avg = read_data.get('avg')\n",
    "                        write_avg = write_data.get('avg')\n",
    "                        if read_avg is not None and write_avg is not None:\n",
    "                            fs_labels.append(label)\n",
    "                            read_vals.append(read_avg)\n",
    "                            write_vals.append(write_avg)\n",
    "\n",
    "                            if include_min_max:\n",
    "                                read_min = read_data.get('min', read_avg)\n",
    "                                read_max = read_data.get('max', read_avg)\n",
    "                                read_err[0].append(read_avg - read_min)\n",
    "                                read_err[1].append(read_max - read_avg)\n",
    "\n",
    "                                write_min = write_data.get('min', write_avg)\n",
    "                                write_max = write_data.get('max', write_avg)\n",
    "                                write_err[0].append(write_avg - write_min)\n",
    "                                write_err[1].append(write_max - write_avg)\n",
    "\n",
    "                            color_map[label] = colors[idx % len(colors)]\n",
    "\n",
    "                    x = np.arange(len(fs_labels))\n",
    "                    bar_width = 0.35\n",
    "                    read_colors = [color_map[fs] for fs in fs_labels]\n",
    "                    write_colors = [darken_color(color_map[fs]) for fs in fs_labels]\n",
    "\n",
    "                    read_bars = ax.bar(x - bar_width/2, read_vals, bar_width,\n",
    "                                    label='Odczyt', color=read_colors,\n",
    "                                    yerr=read_err if include_min_max else None, capsize=5)\n",
    "                    write_bars = ax.bar(x + bar_width/2, write_vals, bar_width,\n",
    "                                        label='Zapis', color=write_colors,\n",
    "                                        yerr=write_err if include_min_max else None, capsize=5)\n",
    "\n",
    "                    for j, bar in enumerate(read_bars):\n",
    "                        y = bar.get_height()\n",
    "                        err = read_err[1][j] if include_min_max else 0\n",
    "                        ax.text(bar.get_x() + bar.get_width() / 2, y + err + 0.02 * y,\n",
    "                                f'{y:.2f}\\nOdczyt', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "                    for j, bar in enumerate(write_bars):\n",
    "                        y = bar.get_height()\n",
    "                        err = write_err[1][j] if include_min_max else 0\n",
    "                        ax.text(bar.get_x() + bar.get_width() / 2, y + err + 0.02 * y,\n",
    "                                f'{y:.2f}\\nZapis', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "                    max_height = max(\n",
    "                        [v + e for v, e in zip(read_vals, read_err[1])] +\n",
    "                        [v + e for v, e in zip(write_vals, write_err[1])]\n",
    "                    ) if include_min_max else max(max(read_vals, default=0), max(write_vals, default=0))\n",
    "                    ax.set_ylim(0, max_height * 1.4)\n",
    "\n",
    "                    translated_label = metric_translations.get(base_name, base_name)\n",
    "                    ax.set_xticks(x)\n",
    "                    if len(fs_labels) > 6:\n",
    "                        print(\"Ustawiam etykiety osi X w poziomie\")\n",
    "                        ax.set_xticklabels(fs_labels, rotation=45, ha='center', fontsize=8)\n",
    "                    else:\n",
    "                        ax.set_xticks(range(len(fs_labels)))\n",
    "\n",
    "                    ax.set_ylabel(translated_label)\n",
    "                    ax.set_title(translated_label)\n",
    "                    # ax.legend()\n",
    "\n",
    "                elif plot[0] == \"single\":\n",
    "                    _, metric = plot\n",
    "                    fs_labels = []\n",
    "                    avg_values = []\n",
    "                    y_errs = ([], [])\n",
    "                    color_map = {}\n",
    "\n",
    "                    for idx, label in enumerate(prepared_data):\n",
    "                        workload_data = prepared_data[label]\n",
    "                        metric_data = workload_data.get(metric, {})\n",
    "                        avg = metric_data.get(\"avg\")\n",
    "                        if avg is not None:\n",
    "                            fs_labels.append(label)\n",
    "                            avg_values.append(avg)\n",
    "\n",
    "                            if include_min_max:\n",
    "                                min_val = metric_data.get(\"min\", avg)\n",
    "                                max_val = metric_data.get(\"max\", avg)\n",
    "                                y_errs[0].append(avg - min_val)\n",
    "                                y_errs[1].append(max_val - avg)\n",
    "\n",
    "                            color_map[label] = colors[idx % len(colors)]\n",
    "\n",
    "                    bars = ax.bar(fs_labels, avg_values, color=[color_map[fs] for fs in fs_labels],\n",
    "                                yerr=y_errs if include_min_max else None, capsize=5)\n",
    "\n",
    "                    if len(fs_labels) > 6:\n",
    "                        ax.set_xticks(range(len(fs_labels)))\n",
    "                        ax.set_xticklabels(fs_labels, rotation=45, ha='center', fontsize=8)\n",
    "                    else:\n",
    "                        ax.set_xticks(range(len(fs_labels)))\n",
    "                        ax.set_xticklabels(fs_labels, fontsize=10)\n",
    "\n",
    "                    for j, bar in enumerate(bars):\n",
    "                        y = bar.get_height()\n",
    "                        err = y_errs[1][j] if include_min_max else 0\n",
    "                        ax.text(bar.get_x() + bar.get_width() / 2, y + err + 0.02 * y,\n",
    "                                f'{y:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "                    if avg_values:\n",
    "                        max_height = max([v + e for v, e in zip(avg_values, y_errs[1])] if include_min_max else avg_values)\n",
    "                        ax.set_ylim(0, max_height * 1.4)\n",
    "\n",
    "                    translated_label = metric_translations.get(metric, metric)\n",
    "                    ax.set_ylabel(translated_label)\n",
    "                    ax.set_title(translated_label)\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "            if save_dir:\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                safe_combined_label = combined_label.replace(\" \", \"\").replace(\"/\", \"\")\n",
    "                filename = f\"syntetyk_{safe_combined_label.lower()}_{block_size.lower()}_{workload.lower()}.png\"\n",
    "                filepath = os.path.join(save_dir, filename)\n",
    "                fig.savefig(filepath, bbox_inches='tight')\n",
    "                if debug:\n",
    "                    print(f\" Zapisano wykres do pliku: {filepath}\")\n",
    "            else:\n",
    "                plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1b1198",
   "metadata": {},
   "source": [
    "Funkcja do generowania tabelki fio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ad6153-be83-4703-bb31-52f4d1bc2602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_columns(metrics, stats=[\"MIN\", \"AVG\", \"MAX\"], storage_types=[\"HDD\", \"SSD\", \"NVME\"]):\n",
    "    columns = [\"File System\", \"Block Size\"]\n",
    "    for storage in storage_types:\n",
    "        for metric in metrics:\n",
    "            for stat in stats:\n",
    "                columns.append(f\"{storage} {metric} {stat}\")\n",
    "    return columns\n",
    "\n",
    "def extract_row_data(data, workload, columns, file_systems=None, block_sizes=None):\n",
    "    rows = []\n",
    "    for fs, block_data in data.items():\n",
    "        if file_systems and fs not in file_systems:\n",
    "            continue\n",
    "        # Sprawd藕 wszystkie block_sizes, kt贸re maj by pokazane\n",
    "        for block_size in block_sizes if block_sizes else block_data.keys():\n",
    "            original_block_size = block_size  # Zapisz oryginalny\n",
    "            devices = block_data.get(block_size)\n",
    "            if not devices:\n",
    "                # Spr贸buj u偶y \"default\", jeli nie ma block_size\n",
    "                devices = block_data.get(\"default\")\n",
    "                if not devices:\n",
    "                    continue  # Pomijaj, jeli nie ma tak偶e \"default\"\n",
    "                block_size = f\"default\"\n",
    "            row = [fs, block_size]\n",
    "            for col in columns[2:]:  # Skip File System and Block Size\n",
    "                if len(col.split()) > 3:\n",
    "                    col = col.split()\n",
    "                    storage, metric, stat = col[0], col[1] + ' ' + col[2], col[3]\n",
    "                else:\n",
    "                    storage, metric, stat = col.split(\" \", 2)\n",
    "                if \"Bandwidth\" in metric:\n",
    "                    metric_key = f\"{metric} (MiB/s)\"\n",
    "                elif \"Latency\" in metric:\n",
    "                    metric_key = f\"{metric} (ms)\"\n",
    "                else:\n",
    "                    metric_key = metric\n",
    "                # Extract value\n",
    "                value = \"N/A\"\n",
    "                for device_type, workloads in devices.items():\n",
    "                    if device_type.lower() == storage.lower() and workload in workloads:\n",
    "                        value = workloads[workload].get(metric_key, {}).get(stat.lower(), \"N/A\")\n",
    "                        break\n",
    "                row.append(value)\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "\n",
    "def display_performance_metrics(data, workloads, metrics, stats=[\"MIN\", \"AVG\", \"MAX\"], storage_types=[\"HDD\", \"SSD\", \"NVME\"], file_systems=None, block_sizes=None):\n",
    "    for workload in workloads:\n",
    "        columns = generate_columns(metrics, stats, storage_types)\n",
    "        rows = extract_row_data(data, workload, columns, file_systems, block_sizes)\n",
    "        df = pd.DataFrame(rows, columns=columns)\n",
    "        display(df.style.set_caption(f\"Performance Metrics: {workload.capitalize()}\").format(precision=3))\n",
    "\n",
    "# Example usage\n",
    "workloads = [\"database\", \"multimedia\", \"webserver\", \"archive\"]\n",
    "#workloads = [\"database_article\", \"seq_read_article\", \"seq_write_article\"] \n",
    "metrics = {\n",
    "    'database_article': [\"Bandwidth READ\", \"Bandwidth WRITE\", \"IOPS READ\", \"IOPS WRITE\", \"Latency READ\", \"Latency WRITE\"],\n",
    "    'seq_write_article': [\"Bandwidth WRITE\", \"IOPS WRITE\", \"Latency WRITE\"],\n",
    "    'seq_read_article': [\"Bandwidth READ\", \"IOPS READ\", \"Latency READ\"],\n",
    "    'database': [\"Bandwidth READ\", \"Bandwidth WRITE\", \"IOPS READ\", \"IOPS WRITE\", \"Latency READ\", \"Latency WRITE\"],\n",
    "    'archive': [\"Bandwidth WRITE\", \"IOPS WRITE\", \"Latency WRITE\"],\n",
    "    'multimedia': [\"Bandwidth READ\", \"IOPS READ\", \"Latency READ\"],\n",
    "    'webserver': [\"Bandwidth READ\", \"IOPS READ\", \"Latency READ\"],\n",
    "    \"default\": [\"Bandwidth READ\", \"Bandwidth WRITE\", \"IOPS READ\", \"IOPS WRITE\", \"Latency READ\", \"Latency WRITE\"],\n",
    "}\n",
    "block_sizes = [\"4096\"]  # Specify block sizes to display\n",
    "\n",
    "# Generate and display tables for each workload\n",
    "for workload in workloads:\n",
    "  \n",
    "    workload_metrics = metrics.get(workload, metrics[\"default\"])\n",
    "    display_performance_metrics(fio_resultsdict, [workload], workload_metrics, block_sizes=block_sizes, storage_types=[\"HDD\", \"SSD\",\"NVME\"], stats = [\"AVG\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b62525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "metrics = [\n",
    "    {\"name\": \"Bandwidth (MiB/s)\", \"read\": \"Bandwidth READ (MiB/s)\", \"write\": \"Bandwidth WRITE (MiB/s)\"},\n",
    "    #{\"name\": \"IOPS\", \"read\": \"IOPS READ\", \"write\": \"IOPS WRITE\"}, \n",
    "    #{\"name\": \"Latency (ms)\", \"read\": \"Latency READ (ms)\", \"write\": \"Latency WRITE (ms)\"},\n",
    "]\n",
    "\n",
    "storage_types = ['HDD','RAID02HDD', 'RAID04HDD', 'RAID1', 'RAID5', 'RAID6', 'RAID10']\n",
    "#storage_types = ['NVME']  \n",
    "block_sizes = ['default']  \n",
    "file_systems = ['btrfs']   #['exfat','ext4', 'xfs', 'btrfs', \"f2fs\", \"zfs\", \"zfs_nocache\"]  \n",
    "#file_systems = ['btrfs', 'exfat', 'ext4', \"f2fs\", \"xfs\", \"zfs\", \"zfs_nocache\", \"zfs_l2arc\"]  #['exfat','ext4', 'xfs', 'btrfs', \"f2fs\", \"zfs\", \"zfs_nocache\"]\n",
    "# Plot performance metrics for fio data\n",
    "#workloads = [\"database\"]\n",
    "workloads = [\"database\", \"multimedia\", \"webserver\", \"archive\"]\n",
    "#workloads = [\"database_article\", \"seq_read_article\", \"seq_write_article\"]\n",
    "print(fio_resultsdict)\n",
    "for workload in workloads:\n",
    "    #plot_performance_metrics(fio_resultsdict, metrics, storage_types, block_sizes, include_min_max=True, workload=workload, hdparm_data=hdparm_resultsdict, hdparm_stat='avg' )\n",
    "    plot_performance_metrics(fio_resultsdict, metrics, storage_types, block_sizes, include_min_max=True, workload=workload, combine_storage_types=True, file_systems=file_systems, hdparm_data=hdparm_resultsdict, hdparm_stat='avg', preserve_ylim=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb57889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate all possible columns\n",
    "def generate_columns(metrics, stats=[\"MIN\", \"MAX\", \"AVG\"], storage_types=[\"HDD\", \"SSD\", \"NVME\"], file_systems=None):\n",
    "    columns = [\"File System\", \"Block Size\"]  # Include Block Size in columns\n",
    "    for storage in storage_types:\n",
    "        for metric in metrics:\n",
    "            for stat in stats:\n",
    "                columns.append(f\"{storage} {metric} {stat}\")\n",
    "    return columns\n",
    "\n",
    "def extract_row_data(data, columns, file_systems=None, block_sizes=None):\n",
    "    rows = []\n",
    "    for fs, block_data in data.items():\n",
    "        if file_systems and fs not in file_systems:\n",
    "            continue\n",
    "        for block_size, devices in block_data.items():  # Iterate over block sizes\n",
    "            if block_sizes and block_size not in block_sizes:\n",
    "                continue\n",
    "            row = [fs, block_size]  # Add File System and Block Size to the row\n",
    "            for col in columns[2:]:  # Skip File System and Block Size\n",
    "                if len(col.split()) > 3:\n",
    "                    col = col.split()\n",
    "                    storage, metric, stat = col[0], col[1] + ' ' + col[2], col[3]\n",
    "                else:\n",
    "                    storage, metric, stat = col.split(\" \", 2)\n",
    "                metric_key = f\"{metric} (MiB/s)\" if \"Bandwidth\" in metric else metric\n",
    "                # Extract value\n",
    "                value = \"N/A\"\n",
    "                for device_type, workloads in devices.items():\n",
    "                    if device_type.lower() == storage.lower():\n",
    "                        for operation, metrics in workloads.items():\n",
    "                            if metric_key in metrics:\n",
    "                                value = metrics[metric_key].get(stat.lower(), \"N/A\")\n",
    "                                break\n",
    "                row.append(value)\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "# Example data\n",
    "# Generate and display tables for dd_resultsdict\n",
    "columns = generate_columns([\"Bandwidth READ\", \"Bandwidth WRITE\"], stats=[\"MIN\", \"AVG\", \"MAX\"], storage_types=[\"HDD\", \"SSD\", \"NVME\"])\n",
    "rows = extract_row_data(dd_resultsdict, columns, block_sizes=[\"4096\"])  # Specify block sizes\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "display(df.style.set_caption(\"Performance Metrics: DD Results\").format(precision=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot performance metrics for dd data\n",
    "plot_performance_metrics(dd_resultsdict, metrics=[\"Bandwidth READ (MiB/s)\", \"Bandwidth WRITE (MiB/s)\"], storage_types=storage_types, block_sizes=['4096']\n",
    ", include_min_max=True, hdparm_data=hdparm_resultsdict, hdparm_stat='max' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5410dd78",
   "metadata": {},
   "source": [
    "#TODO dodanie uniwersalnej funkcji fo tabelek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9276861f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
