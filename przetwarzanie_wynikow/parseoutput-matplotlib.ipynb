{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff78de34",
   "metadata": {},
   "source": [
    "Czytanie plików fio i dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecadccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_rgb\n",
    "from matplotlib.patches import Patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac3bfb1-c53b-41c1-9991-c573de3694b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fio_results(file_path):\n",
    "    # Regular expressions\n",
    "    bandwidth_regex = re.compile(r'WRITE: bw=(\\d+(?:\\.\\d+)?)([MK]iB/s)')\n",
    "    bandwidth_read_regex = re.compile(r'READ: bw=(\\d+(?:\\.\\d+)?)([MK]iB/s)')\n",
    "    iops_regex = re.compile(r'write: IOPS=(\\d+)')\n",
    "    iops_read_regex = re.compile(r'read: IOPS=(\\d+)')\n",
    "    latency_regex = re.compile(r'lat (\\([mu]sec\\)): min=\\d+\\.?\\d*[km]?, max=\\d+\\.?\\d*[km]?, avg=(\\d+\\.\\d+[km]?), stdev=\\d+\\.?\\d*')\n",
    "\n",
    "    # Function to convert bandwidth to MiB/s\n",
    "    def convert_bandwidth(value, unit):\n",
    "        value = float(value)\n",
    "        if unit == \"KiB/s\":\n",
    "            return value / 1024  # Convert KiB/s to MiB/s\n",
    "        return value  # Already in MiB/s\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        last = 'read'\n",
    "        for line in file:\n",
    "            # Match write bandwidth\n",
    "            if 'write' in line:\n",
    "                last = 'write'\n",
    "            elif 'read' in line:\n",
    "                last = 'read'\n",
    "            bw_match = bandwidth_regex.search(line)\n",
    "            if bw_match:\n",
    "                value, unit = bw_match.groups()\n",
    "                results['Bandwidth WRITE (MiB/s)'] = convert_bandwidth(value, unit)\n",
    "\n",
    "            # Match read bandwidth\n",
    "            bw_read_match = bandwidth_read_regex.search(line)\n",
    "            if bw_read_match:\n",
    "                value, unit = bw_read_match.groups()\n",
    "                results['Bandwidth READ (MiB/s)'] = convert_bandwidth(value, unit)\n",
    "\n",
    "            # Match write IOPS\n",
    "            iops_match = iops_regex.search(line)\n",
    "            if iops_match:\n",
    "                results['IOPS WRITE'] = float(iops_match.group(1))\n",
    "\n",
    "            # Match read IOPS\n",
    "            iops_read_match = iops_read_regex.search(line)\n",
    "            if iops_read_match:\n",
    "                results['IOPS READ'] = float(iops_read_match.group(1))\n",
    "\n",
    "            # Match latency\n",
    "            lat_match = latency_regex.search(line)\n",
    "            if lat_match:\n",
    "                lat_val = float(lat_match.group(2))\n",
    "                if lat_match.group(1) == '(usec)':\n",
    "                    lat_val /= 1000\n",
    "                if last == 'read':\n",
    "                    results['Latency READ (ms)'] = lat_val\n",
    "                else:\n",
    "                    results['Latency WRITE (ms)'] = lat_val\n",
    "\n",
    "    return results\n",
    "\n",
    "def parse_dd_results(file_path):\n",
    "    # Regular expressions\n",
    "    bandwidth_regex = re.compile(r'(\\d+(?:\\.\\d+)?) ([GMK]B/s)')\n",
    "    time_regex = re.compile(r'(\\d+(?:\\.\\d+)?) s')\n",
    "\n",
    "    # Function to convert bandwidth to MiB/s\n",
    "    def convert_bandwidth(value, unit):\n",
    "        value = float(value)\n",
    "        if unit == \"KB/s\":\n",
    "            return value / 1024  # Convert KB/s to MiB/s\n",
    "        elif unit == \"MB/s\":\n",
    "            return value  # Already in MiB/s\n",
    "        elif unit == \"GB/s\":\n",
    "            return value * 1024  # Convert GB/s to MiB/s\n",
    "        return value\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Match bandwidth\n",
    "            bw_match = bandwidth_regex.search(line)\n",
    "            if bw_match:\n",
    "                value, unit = bw_match.groups()\n",
    "                if 'write' in file_path:\n",
    "                    results['Bandwidth WRITE (MiB/s)'] = convert_bandwidth(value, unit)\n",
    "                else:\n",
    "                    results['Bandwidth READ (MiB/s)'] = convert_bandwidth(value, unit)\n",
    "\n",
    "            # Match time\n",
    "            time_match = time_regex.search(line)\n",
    "            if time_match:\n",
    "                results['Time (s)'] = float(time_match.group(1))\n",
    "\n",
    "    return results\n",
    "\n",
    "import re\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_values(resultsfolder, file_names, parser, program_type):\n",
    "    resultsdict = defaultdict(lambda: defaultdict(dict))\n",
    "    \n",
    "    # Funkcja pomocnicza do wyodrębnienia nazwy od drugiego słowa do słowa \"test\"\n",
    "    def extract_key(file_name):\n",
    "        match = re.match(r\"^[^_]+_(.*?)_test\", file_name)  # Pomija pierwsze słowo przed \"_\"\n",
    "        return match.group(1) if match else os.path.splitext(file_name)[0]\n",
    "\n",
    "    prepaths = [folder for folder in glob.glob(resultsfolder + '*/') if program_type in folder]  # Filtruj według typu programu\n",
    "    for prepath in prepaths:\n",
    "        # Wyodrębnij system plików i typ pamięci, uwzględniając podkreślenia w nazwach\n",
    "        folder_parts = prepath.split('\\\\')[-2].split('_')\n",
    "        filesystem = '_'.join(folder_parts[2:-1])  # Wszystkie części między 2 a ostatnią\n",
    "        storage = folder_parts[-1]  # Ostatnia część to typ pamięci\n",
    "        \n",
    "        # Sprawdź, czy typ programu używa folderów z rozmiarem bloków\n",
    "        if program_type in ['fio_results', 'dd_results']:\n",
    "            block_size_folders = [folder for folder in glob.glob(prepath + '*/')]  # Uwzględnij foldery z rozmiarem bloków\n",
    "            for block_size_folder in block_size_folders:\n",
    "                # Bezpiecznie wyodrębnij rozmiar bloku\n",
    "                folder_parts = block_size_folder.split('\\\\')[-2].split('_')\n",
    "                if len(folder_parts) > 2 and folder_parts[0] == \"block\" and folder_parts[1] == \"size\":\n",
    "                    block_size = folder_parts[2]\n",
    "                else:\n",
    "                    print(f\"Pomijanie folderu o nieoczekiwanej strukturze: {block_size_folder}\")\n",
    "                    continue\n",
    "\n",
    "                folders = [folder for folder in glob.glob(block_size_folder + '*/')]\n",
    "                cumulative_data = {}\n",
    "                for folder in folders:\n",
    "                    for file_name in file_names:\n",
    "                        file_path = os.path.join(folder, file_name)\n",
    "                        if os.path.exists(file_path):\n",
    "                            try:\n",
    "                                results = parser(file_path)\n",
    "                                if results:  # Dodaj tylko, jeśli są jakieś dane\n",
    "                                    test_key = extract_key(file_name)\n",
    "                                    if test_key not in cumulative_data:\n",
    "                                        cumulative_data[test_key] = defaultdict(list)\n",
    "                                    for key, value in results.items():\n",
    "                                        cumulative_data[test_key][key].append(value)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Błąd podczas parsowania {file_path}: {e}\")\n",
    "                        else:\n",
    "                            print(f\"Plik nie znaleziony: {file_path}\")\n",
    "\n",
    "                ranges = {}\n",
    "                for test_key, metrics in cumulative_data.items():\n",
    "                    if metrics:  # Dodaj tylko, jeśli są jakieś dane\n",
    "                        ranges[test_key] = {\n",
    "                            key: {'min': round(min(values), 3), 'max': round(max(values), 3), 'avg': round(sum(values) / len(values), 2)} if values else '-'\n",
    "                            for key, values in metrics.items()\n",
    "                        }\n",
    "                if block_size not in resultsdict[filesystem]:\n",
    "                    resultsdict[filesystem][block_size] = {}\n",
    "                resultsdict[filesystem][block_size][storage] = ranges\n",
    "        else:\n",
    "            # Obsługa programów bez folderów z rozmiarem bloków (np. hdparm)\n",
    "            folders = [folder for folder in glob.glob(prepath + '*/')]\n",
    "            cumulative_data = {}\n",
    "            for folder in folders:\n",
    "                for file_name in file_names:\n",
    "                    file_path = os.path.join(folder, file_name)\n",
    "                    if os.path.exists(file_path):\n",
    "                        try:\n",
    "                            results = parser(file_path)\n",
    "                            if results:  # Dodaj tylko, jeśli są jakieś dane\n",
    "                                test_key = extract_key(file_name)\n",
    "                                if test_key not in cumulative_data:\n",
    "                                    cumulative_data[test_key] = defaultdict(list)\n",
    "                                for key, value in results.items():\n",
    "                                    cumulative_data[test_key][key].append(value)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Błąd podczas parsowania {file_path}: {e}\")\n",
    "                    else:\n",
    "                        print(f\"Plik nie znaleziony: {file_path}\")\n",
    "\n",
    "            ranges = {}\n",
    "            for test_key, metrics in cumulative_data.items():\n",
    "                if metrics:  # Dodaj tylko, jeśli są jakieś dane\n",
    "                    ranges[test_key] = {\n",
    "                        key: {'min': round(min(values), 3), 'max': round(max(values), 3), 'avg': round(sum(values) / len(values), 2)} if values else '-'\n",
    "                        for key, values in metrics.items()\n",
    "                    }\n",
    "            if 'no_block_size' not in resultsdict[filesystem]:\n",
    "                resultsdict[filesystem]['no_block_size'] = {}\n",
    "            resultsdict[filesystem]['no_block_size'][storage] = ranges\n",
    "    return resultsdict\n",
    "\n",
    "def parse_hdparm_results(file_path):\n",
    "    # Regular expression to match the bandwidth\n",
    "    bandwidth_regex = re.compile(r'Timing O_DIRECT disk reads: (\\d+(?:\\.\\d+)?) MB in .* seconds = (\\d+(?:\\.\\d+)) MB/sec')\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Match bandwidth\n",
    "            bw_match = bandwidth_regex.search(line)\n",
    "            if bw_match:\n",
    "                total_mb, bandwidth = bw_match.groups()\n",
    "                results['Total Data Read (MB)'] = float(total_mb)\n",
    "                results['Bandwidth (MiB/s)'] = float(bandwidth)\n",
    "\n",
    "    return results\n",
    "    \n",
    "def extract_hdparm_values_by_device(resultsfolder, file_names, parser, group_by_computer=False):\n",
    "    resultsdict = defaultdict(lambda: defaultdict(lambda: defaultdict(list))) if group_by_computer else defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    # Iterate through all folders in the results folder\n",
    "    prepaths = glob.glob(resultsfolder + '*/')  # Get all subfolders\n",
    "    for prepath in prepaths:\n",
    "        # Extract device type from the folder name (e.g., \"hdparm_results_xfs_nvme\")\n",
    "        device_type = prepath.split('_')[-1].lower().strip('\\\\')  # Extract \"nvme\", \"ssd\", etc., and remove trailing slashes\n",
    "        \n",
    "        # Iterate through subfolders for each computer\n",
    "        folders = glob.glob(prepath + '*/')  # Get subfolders for each computer\n",
    "        for folder in folders:\n",
    "            computer_name = folder.split('\\\\')[-2]  # Extract computer name (e.g., \"lab-sec-13\")\n",
    "            for file_name in file_names:\n",
    "                file_path = os.path.join(folder, file_name)\n",
    "                if os.path.exists(file_path):\n",
    "                    try:\n",
    "                        # Parse the file and collect results\n",
    "                        results = parser(file_path)\n",
    "                        if group_by_computer:\n",
    "                            for key, value in results.items():\n",
    "                                resultsdict[device_type][computer_name][key].append(value)\n",
    "                        else:\n",
    "                            for key, value in results.items():\n",
    "                                resultsdict[device_type][key].append(value)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing {file_path}: {e}\")\n",
    "                else:\n",
    "                    print(f\"File not found: {file_path}\")\n",
    "\n",
    "    # Aggregate results by calculating min, max, and avg for each metric\n",
    "    aggregated_results = {}\n",
    "    for device_type, computers_or_metrics in resultsdict.items():\n",
    "        if group_by_computer:\n",
    "            aggregated_results[device_type] = {}\n",
    "            for computer, metrics in computers_or_metrics.items():\n",
    "                aggregated_results[device_type][computer] = {\n",
    "                    key: {\n",
    "                        'min': round(min(values), 3),\n",
    "                        'max': round(max(values), 3),\n",
    "                        'avg': round(sum(values) / len(values), 2)\n",
    "                    } if values else '-' for key, values in metrics.items()\n",
    "                }\n",
    "        else:\n",
    "            aggregated_results[device_type] = {\n",
    "                key: {\n",
    "                    'min': round(min(values), 3),\n",
    "                    'max': round(max(values), 3),\n",
    "                    'avg': round(sum(values) / len(values), 2)\n",
    "                } if values else '-' for key, values in computers_or_metrics.items()\n",
    "            }\n",
    "\n",
    "    return aggregated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9363a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage for fio\n",
    "fio_file_names = [\n",
    "    #'fio_database_test_output.txt',\n",
    "    #'fio_multimedia_test_output.txt',\n",
    "    #'fio_webserver_test_output.txt',\n",
    "    #'fio_archive_test_output.txt',\n",
    "    'fio_database_article_test_output.txt',\n",
    "    'fio_seq_read_article_test_output.txt',\n",
    "    'fio_seq_write_article_test_output.txt',\n",
    "]\n",
    "\n",
    "fio_resultsdict = extract_values('../wyniki_10G_article/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "print(fio_resultsdict)\n",
    "\n",
    "# Example usage for dd\n",
    "dd_file_names = [\n",
    "    'dd_read_test_output.txt',\n",
    "    'dd_write_test_output.txt',\n",
    "]\n",
    "\n",
    "dd_resultsdict = extract_values('../wyniki_10G_article/', dd_file_names, parse_dd_results, program_type='dd_results')\n",
    "print(dd_resultsdict)\n",
    "\n",
    "hdparm_file_names = [\n",
    "    'hdparm_test_output.txt',\n",
    "]\n",
    "\n",
    "hdparm_resultsdict = extract_hdparm_values_by_device('../wyniki_10G_article/', hdparm_file_names, parse_hdparm_results, group_by_computer=False)\n",
    "\n",
    "print(hdparm_resultsdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0a3d80",
   "metadata": {},
   "source": [
    "Funkcja do generowania wykresów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb5d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def darken_color(color, amount=0.7):\n",
    "    \"\"\"Zmniejsz jasność koloru RGB.\"\"\"\n",
    "    c = np.array(to_rgb(color))\n",
    "    return tuple(np.clip(c * amount, 0, 1))\n",
    "\n",
    "def plot_performance_metrics(data, metrics, storage_types, block_sizes, include_min_max=False, workload=None, file_systems=None, colors=None, hdparm_data=None, hdparm_stat='avg', preserve_ylim=False, debug=False):\n",
    "    if colors is None:\n",
    "        colors = ['b', 'g', 'r', 'c', 'm', 'y', 'orange']\n",
    "    if file_systems is None:\n",
    "        file_systems = list(data.keys())\n",
    "\n",
    "    for block_size in block_sizes:\n",
    "        for storage in storage_types:\n",
    "            plots = []\n",
    "\n",
    "            # Rozpoznaj typy metryk\n",
    "            for metric in metrics:\n",
    "                if isinstance(metric, dict):\n",
    "                    read_metric = metric.get(\"read\")\n",
    "                    write_metric = metric.get(\"write\")\n",
    "                    fs_data_check = data[file_systems[0]].get(block_size) or data[file_systems[0]].get('default')\n",
    "                    if not fs_data_check:\n",
    "                        continue\n",
    "                    storage_data_check = fs_data_check.get(storage.lower())\n",
    "                    if not storage_data_check:\n",
    "                        continue\n",
    "                    workload_data_check = storage_data_check.get(workload)\n",
    "                    if not workload_data_check:\n",
    "                        continue\n",
    "                    if read_metric and write_metric and read_metric in workload_data_check and write_metric in workload_data_check:\n",
    "                        plots.append((\"grouped\", metric[\"name\"], read_metric, write_metric))\n",
    "                    elif read_metric and read_metric in workload_data_check:\n",
    "                        plots.append((\"single\", read_metric))\n",
    "                    elif write_metric and write_metric in workload_data_check:\n",
    "                        plots.append((\"single\", write_metric))\n",
    "                    else:\n",
    "                        plots.append((\"single\", metric[\"name\"]))\n",
    "                else:\n",
    "                    fs_data_check = data[file_systems[0]].get(block_size) or data[file_systems[0]].get('default')\n",
    "                    if not fs_data_check:\n",
    "                        continue\n",
    "                    storage_data_check = fs_data_check.get(storage.lower())\n",
    "                    if not storage_data_check:\n",
    "                        continue\n",
    "                    workload_data_check = storage_data_check.get(workload)\n",
    "                    if metric in workload_data_check:\n",
    "                        plots.append((\"single\", metric))\n",
    "\n",
    "            fig, axs = plt.subplots(len(plots), 1, figsize=(10, 3 * len(plots)))\n",
    "            if len(plots) == 1:\n",
    "                axs = [axs]\n",
    "\n",
    "            fig.suptitle(f'Performance Metrics for {storage} (Block Size: {block_size}){\" - \" + workload.capitalize() if workload else \"\"}')\n",
    "\n",
    "            for i, plot in enumerate(plots):\n",
    "                ax = axs[i]\n",
    "                if plot[0] == \"grouped\":\n",
    "                    _, base_name, read_metric, write_metric = plot\n",
    "                    fs_labels = []\n",
    "                    read_vals, write_vals = [], []\n",
    "                    read_mins, read_maxs = [], []\n",
    "                    write_mins, write_maxs = [], []\n",
    "                    color_map = {}\n",
    "\n",
    "                    for idx, fs in enumerate(file_systems):\n",
    "                        fs_storage_data = data.get(fs, {})\n",
    "                        fs_block_data = fs_storage_data.get(block_size)\n",
    "                        if not fs_block_data and 'default' in fs_storage_data:\n",
    "                            if debug:\n",
    "                                print(f\"ℹ️ Użyto 'default' dla systemu plików: {fs}, block_size: {block_size}\")\n",
    "                            fs_block_data = fs_storage_data.get('default')\n",
    "\n",
    "                        if not fs_block_data:\n",
    "                            print(f\"⚠️ Pomijanie {fs}: brak danych dla block_size '{block_size}' i brak 'default'\")\n",
    "                            continue\n",
    "\n",
    "                        op_data = fs_block_data.get(storage.lower(), {}).get(workload)\n",
    "                        if not isinstance(op_data, dict):\n",
    "                            print(f\"⚠️ Pomijanie {fs}: brak danych workload '{workload}' dla storage '{storage}' z block_size '{block_size}'\")\n",
    "                            continue\n",
    "\n",
    "                        read_data = op_data.get(read_metric, {})\n",
    "                        write_data = op_data.get(write_metric, {})\n",
    "                        read_avg = read_data.get('avg')\n",
    "                        write_avg = write_data.get('avg')\n",
    "                        if read_avg is not None and write_avg is not None:\n",
    "                            fs_labels.append(fs)\n",
    "                            read_vals.append(read_avg)\n",
    "                            write_vals.append(write_avg)\n",
    "                            read_mins.append(read_data.get('min', read_avg))\n",
    "                            read_maxs.append(read_data.get('max', read_avg))\n",
    "                            write_mins.append(write_data.get('min', write_avg))\n",
    "                            write_maxs.append(write_data.get('max', write_avg))\n",
    "                            color_map[fs] = colors[idx % len(colors)]\n",
    "\n",
    "                    x = np.arange(len(fs_labels))\n",
    "                    bar_width = 0.35\n",
    "                    read_colors = [color_map[fs] for fs in fs_labels]\n",
    "                    write_colors = [darken_color(color_map[fs]) for fs in fs_labels]\n",
    "\n",
    "                    ax.bar(x - bar_width/2, read_vals, bar_width, label='READ', color=read_colors)\n",
    "                    ax.bar(x + bar_width/2, write_vals, bar_width, label='WRITE', color=write_colors)\n",
    "\n",
    "                    if include_min_max:\n",
    "                        read_yerr = [np.array(read_vals) - np.array(read_mins), np.array(read_maxs) - np.array(read_vals)]\n",
    "                        write_yerr = [np.array(write_vals) - np.array(write_mins), np.array(write_maxs) - np.array(write_vals)]\n",
    "\n",
    "                        ax.errorbar(x - bar_width/2, read_vals, yerr=np.abs(read_yerr), fmt='none', ecolor='black', capsize=5)\n",
    "                        ax.errorbar(x + bar_width/2, write_vals, yerr=np.abs(write_yerr), fmt='none', ecolor='black', capsize=5)\n",
    "\n",
    "                    ax.set_xticks(x)\n",
    "                    ax.set_xticklabels(fs_labels)\n",
    "                    ax.set_ylabel(base_name)\n",
    "                    ax.set_title(base_name)\n",
    "                    ax.legend()\n",
    "                    if preserve_ylim:\n",
    "                        ax.set_ylim(ax.get_ylim())\n",
    "\n",
    "                elif plot[0] == \"single\":\n",
    "                    _, metric = plot\n",
    "                    fs_labels = []\n",
    "                    avg_values, min_values, max_values = [], [], []\n",
    "                    color_map = {}\n",
    "\n",
    "                    for idx, fs in enumerate(file_systems):\n",
    "                        fs_storage_data = data.get(fs, {})\n",
    "                        fs_block_data = fs_storage_data.get(block_size)\n",
    "                        if not fs_block_data and 'default' in fs_storage_data:\n",
    "                            if debug:                            \n",
    "                                print(f\"ℹ️ Użyto 'default' dla systemu plików: {fs}, block_size: {block_size}\")\n",
    "                            fs_block_data = fs_storage_data.get('default')\n",
    "\n",
    "                        if not fs_block_data:\n",
    "                            print(f\"⚠️ Pomijanie {fs}: brak danych dla block_size '{block_size}' i brak 'default'\")\n",
    "                            continue\n",
    "\n",
    "                        op_data = fs_block_data.get(storage.lower(), {}).get(workload)\n",
    "                        if not isinstance(op_data, dict):\n",
    "                            print(f\"⚠️ Pomijanie {fs}: brak danych workload '{workload}' dla storage '{storage}' z block_size '{block_size}'\")\n",
    "                            continue\n",
    "\n",
    "                        metric_data = op_data.get(metric, {})\n",
    "                        avg = metric_data.get(\"avg\")\n",
    "                        if avg is not None:\n",
    "                            fs_labels.append(fs)\n",
    "                            avg_values.append(avg)\n",
    "                            min_values.append(metric_data.get(\"min\", avg))\n",
    "                            max_values.append(metric_data.get(\"max\", avg))\n",
    "                            color_map[fs] = colors[idx % len(colors)]\n",
    "\n",
    "                    ax.bar(fs_labels, avg_values, color=[color_map[fs] for fs in fs_labels])\n",
    "                    if include_min_max:\n",
    "                        yerr = [np.array(avg_values) - np.array(min_values), np.array(max_values) - np.array(avg_values)]\n",
    "                        ax.errorbar(fs_labels, avg_values, yerr=np.abs(yerr), fmt='none', color='black', capsize=5)\n",
    "\n",
    "                    if hdparm_data and storage.lower() in hdparm_data and \"Bandwidth\" in metric:\n",
    "                        hdparm_value = hdparm_data[storage.lower()].get('Bandwidth (MiB/s)', {}).get(hdparm_stat)\n",
    "                        if hdparm_value is not None:\n",
    "                            ax.axhline(y=hdparm_value, color='red', linestyle='--', label=f'hdparm {hdparm_stat.capitalize()}')\n",
    "                            ax.legend()\n",
    "\n",
    "                    if preserve_ylim:\n",
    "                        ax.set_ylim(ax.get_ylim())\n",
    "\n",
    "                    ax.set_ylabel(metric)\n",
    "                    ax.set_title(metric)\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1b1198",
   "metadata": {},
   "source": [
    "Funkcja do generowania tabelki fio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ad6153-be83-4703-bb31-52f4d1bc2602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_columns(metrics, stats=[\"MIN\", \"AVG\", \"MAX\"], storage_types=[\"HDD\", \"SSD\", \"NVME\"]):\n",
    "    columns = [\"File System\", \"Block Size\"]\n",
    "    for storage in storage_types:\n",
    "        for metric in metrics:\n",
    "            for stat in stats:\n",
    "                columns.append(f\"{storage} {metric} {stat}\")\n",
    "    return columns\n",
    "\n",
    "def extract_row_data(data, workload, columns, file_systems=None, block_sizes=None):\n",
    "    rows = []\n",
    "    for fs, block_data in data.items():\n",
    "        if file_systems and fs not in file_systems:\n",
    "            continue\n",
    "        for block_size, devices in block_data.items():\n",
    "            if block_sizes and block_size not in block_sizes:\n",
    "                continue\n",
    "            row = [fs, block_size]\n",
    "            for col in columns[2:]:  # Skip File System and Block Size\n",
    "                if len(col.split()) > 3:\n",
    "                    col = col.split()\n",
    "                    storage, metric, stat = col[0], col[1] + ' ' + col[2], col[3]\n",
    "                else:\n",
    "                    storage, metric, stat = col.split(\" \", 2)\n",
    "                if \"Bandwidth\" in metric:\n",
    "                    metric_key = f\"{metric} (MiB/s)\"\n",
    "                elif \"Latency\" in metric:\n",
    "                    metric_key = f\"{metric} (ms)\"\n",
    "                else:\n",
    "                    metric_key = metric\n",
    "                # Extract value\n",
    "                value = \"N/A\"\n",
    "                for device_type, workloads in devices.items():\n",
    "                    if device_type.lower() == storage.lower() and workload in workloads:\n",
    "                        value = workloads[workload].get(metric_key, {}).get(stat.lower(), \"N/A\")\n",
    "                        break\n",
    "                row.append(value)\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def display_performance_metrics(data, workloads, metrics, stats=[\"MIN\", \"AVG\", \"MAX\"], storage_types=[\"HDD\", \"SSD\", \"NVME\"], file_systems=None, block_sizes=None):\n",
    "    for workload in workloads:\n",
    "        columns = generate_columns(metrics, stats, storage_types)\n",
    "        rows = extract_row_data(data, workload, columns, file_systems, block_sizes)\n",
    "        df = pd.DataFrame(rows, columns=columns)\n",
    "        display(df.style.set_caption(f\"Performance Metrics: {workload.capitalize()}\").format(precision=3))\n",
    "\n",
    "# Example usage\n",
    "#workloads = [\"database\", \"multimedia\", \"webserver\", \"archive\"]\n",
    "workloads = [\"database_article\", \"seq_read_article\", \"seq_write_article\"] \n",
    "metrics = {\n",
    "    'database_article': [\"Bandwidth READ\", \"Bandwidth WRITE\", \"IOPS READ\", \"IOPS WRITE\", \"Latency READ\", \"Latency WRITE\"],\n",
    "    'seq_write_article': [\"Bandwidth WRITE\", \"IOPS WRITE\", \"Latency WRITE\"],\n",
    "    'seq_read_article': [\"Bandwidth READ\", \"IOPS READ\", \"Latency READ\"],\n",
    "    \"default\": [\"Bandwidth READ\", \"Bandwidth WRITE\", \"IOPS READ\", \"IOPS WRITE\", \"Latency READ\", \"Latency WRITE\"],\n",
    "}\n",
    "block_sizes = [\"4096\"]  # Specify block sizes to display\n",
    "\n",
    "# Generate and display tables for each workload\n",
    "for workload in workloads:\n",
    "  \n",
    "    workload_metrics = metrics.get(workload, metrics[\"default\"])\n",
    "    display_performance_metrics(fio_resultsdict, [workload], workload_metrics, block_sizes=block_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b62525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "metrics = [\n",
    "    {\"name\": \"Bandwidth (MiB/s)\", \"read\": \"Bandwidth READ (MiB/s)\", \"write\": \"Bandwidth WRITE (MiB/s)\"},\n",
    "    {\"name\": \"IOPS\", \"read\": \"IOPS READ\", \"write\": \"IOPS WRITE\"}, \n",
    "    \"Latency READ (ms)\", \"Latency WRITE (ms)\",\n",
    "]\n",
    "\n",
    "storage_types = ['HDD', 'SSD', 'NVME']\n",
    "block_sizes = ['4096']\n",
    "\n",
    "# Plot performance metrics for fio data\n",
    "#workloads = [\"database\", \"multimedia\", \"webserver\", \"archive\"]\n",
    "workloads = [\"database_article\", \"seq_read_article\", \"seq_write_article\"]\n",
    "for workload in workloads:\n",
    "    #plot_performance_metrics(fio_resultsdict, metrics, storage_types, block_sizes, include_min_max=True, workload=workload, hdparm_data=hdparm_resultsdict, hdparm_stat='avg' )\n",
    "    plot_performance_metrics(fio_resultsdict, metrics, storage_types, block_sizes, include_min_max=True, workload=workload )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb57889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate all possible columns\n",
    "def generate_columns(metrics, stats=[\"MIN\", \"MAX\", \"AVG\"], storage_types=[\"HDD\", \"SSD\", \"NVME\"], file_systems=None):\n",
    "    columns = [\"File System\", \"Block Size\"]  # Include Block Size in columns\n",
    "    for storage in storage_types:\n",
    "        for metric in metrics:\n",
    "            for stat in stats:\n",
    "                columns.append(f\"{storage} {metric} {stat}\")\n",
    "    return columns\n",
    "\n",
    "def extract_row_data(data, columns, file_systems=None, block_sizes=None):\n",
    "    rows = []\n",
    "    for fs, block_data in data.items():\n",
    "        if file_systems and fs not in file_systems:\n",
    "            continue\n",
    "        for block_size, devices in block_data.items():  # Iterate over block sizes\n",
    "            if block_sizes and block_size not in block_sizes:\n",
    "                continue\n",
    "            row = [fs, block_size]  # Add File System and Block Size to the row\n",
    "            for col in columns[2:]:  # Skip File System and Block Size\n",
    "                if len(col.split()) > 3:\n",
    "                    col = col.split()\n",
    "                    storage, metric, stat = col[0], col[1] + ' ' + col[2], col[3]\n",
    "                else:\n",
    "                    storage, metric, stat = col.split(\" \", 2)\n",
    "                metric_key = f\"{metric} (MiB/s)\" if \"Bandwidth\" in metric else metric\n",
    "                # Extract value\n",
    "                value = \"N/A\"\n",
    "                for device_type, workloads in devices.items():\n",
    "                    if device_type.lower() == storage.lower():\n",
    "                        for operation, metrics in workloads.items():\n",
    "                            if metric_key in metrics:\n",
    "                                value = metrics[metric_key].get(stat.lower(), \"N/A\")\n",
    "                                break\n",
    "                row.append(value)\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "# Example data\n",
    "# Generate and display tables for dd_resultsdict\n",
    "columns = generate_columns([\"Bandwidth READ\", \"Bandwidth WRITE\"], stats=[\"MIN\", \"AVG\", \"MAX\"], storage_types=[\"HDD\", \"SSD\", \"NVME\"])\n",
    "rows = extract_row_data(dd_resultsdict, columns, block_sizes=[\"4096\"])  # Specify block sizes\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "display(df.style.set_caption(\"Performance Metrics: DD Results\").format(precision=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot performance metrics for dd data\n",
    "plot_performance_metrics(dd_resultsdict, metrics=[\"Bandwidth READ (MiB/s)\", \"Bandwidth WRITE (MiB/s)\"], storage_types=storage_types, block_sizes=['4096']\n",
    ", include_min_max=True, hdparm_data=hdparm_resultsdict, hdparm_stat='max' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5410dd78",
   "metadata": {},
   "source": [
    "#TODO dodanie uniwersalnej funkcji fo tabelek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9276861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_columns(metrics, stats=[\"MIN\", \"AVG\", \"MAX\"], storage_types=[\"HDD\", \"SSD\", \"NVME\"]):\n",
    "    columns = [\"File System\", \"Block Size\"]\n",
    "    for storage in storage_types:\n",
    "        for metric in metrics:\n",
    "            for stat in stats:\n",
    "                columns.append(f\"{storage} {metric} {stat}\")\n",
    "    return columns\n",
    "\n",
    "def extract_row_data(data, workload, columns, file_systems=None, block_sizes=None):\n",
    "    rows = []\n",
    "    for fs, block_data in data.items():\n",
    "        if file_systems and fs not in file_systems:\n",
    "            continue\n",
    "        for block_size, devices in block_data.items():\n",
    "            if block_sizes and block_size not in block_sizes:\n",
    "                continue\n",
    "            row = [fs, block_size]\n",
    "            for col in columns[2:]:  # Skip File System and Block Size\n",
    "                if len(col.split()) > 3:\n",
    "                    col = col.split()\n",
    "                    storage, metric, stat = col[0], col[1] + ' ' + col[2], col[3]\n",
    "                else:\n",
    "                    storage, metric, stat = col.split(\" \", 2)\n",
    "                if workload == 'read':\n",
    "                    metric_key = f\"{metric} READ (MiB/s)\" if \"Bandwidth\" in metric else f\"{metric} READ\"\n",
    "                elif workload == 'write':\n",
    "                    metric_key = f\"{metric} WRITE (MiB/s)\" if \"Bandwidth\" in metric else f\"{metric} WRITE\"\n",
    "                else:\n",
    "                    metric_key = f\"{metric} (MiB/s)\" if \"Bandwidth\" in metric else metric\n",
    "                # Extract value\n",
    "                value = \"N/A\"\n",
    "                for device_type, workloads in devices.items():\n",
    "                    if device_type.lower() == storage.lower() and workload in workloads:\n",
    "                        value = workloads[workload].get(metric_key, {}).get(stat.lower(), \"N/A\")\n",
    "                        break\n",
    "                row.append(value)\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def display_performance_metrics(data, workloads, metrics, stats=[\"MIN\", \"AVG\", \"MAX\"], storage_types=[\"HDD\", \"SSD\", \"NVME\"], file_systems=None, block_sizes=None):\n",
    "    for workload in workloads:\n",
    "        columns = generate_columns(metrics, stats, storage_types)\n",
    "        rows = extract_row_data(data, workload, columns, file_systems, block_sizes)\n",
    "        df = pd.DataFrame(rows, columns=columns)\n",
    "        display(df.style.set_caption(f\"Performance Metrics: {workload.capitalize()}\").format(precision=3))\n",
    "\n",
    "# Example usage for dd\n",
    "dd_workloads = [\"read\", \"write\"]\n",
    "dd_metrics = [\"Bandwidth\", \"Time\"]\n",
    "block_sizes = [\"4096\"]  # Specify block sizes to display\n",
    "\n",
    "# Generate and display tables for dd data\n",
    "for workload in dd_workloads:\n",
    "    display_performance_metrics(dd_resultsdict, [workload], dd_metrics, block_sizes=block_sizes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
