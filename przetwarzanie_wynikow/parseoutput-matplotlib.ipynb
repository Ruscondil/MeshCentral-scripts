{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff78de34",
   "metadata": {},
   "source": [
    "Czytanie plików fio i dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecadccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_rgb\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0aa936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict_tree(d, indent=0, max_depth=None):\n",
    "    \"\"\"Rekurencyjnie wypisuje strukturę kluczy zagnieżdżonego słownika do zadanej głębokości.\"\"\"\n",
    "    if not isinstance(d, dict) or (max_depth is not None and indent >= max_depth):\n",
    "        return\n",
    "    for key in d:\n",
    "        print('  ' * indent + str(key))\n",
    "        print_dict_tree(d[key], indent + 1, max_depth)\n",
    "\n",
    "# Przykład użycia:\n",
    "# print_dict_tree(fio_resultsdict, max_depth=3)\n",
    "\n",
    "\n",
    "def print_latex_image(image_path, image_name, caption=None, label=None, width=None):\n",
    "    if image_name.endswith('.png'):\n",
    "        image_name = image_name[:-4]\n",
    "    width_str = f\"[width={width}]\" if width else \"[width=\\\\textwidth]\"\n",
    "    caption_str = f\"\\\\caption{{{caption}}}\" if caption else \"\\\\caption{TODO caption}\" \n",
    "    label_str = f\"\\\\label{{{label}}}\" if label else f\"\\\\label{{fig:{image_name}}}\"\n",
    "    latex_code = (\n",
    "        \"\\\\begin{figure}[H]\\n\"\n",
    "        \"    \\\\centering\\n\"\n",
    "        f\"    \\\\includegraphics{width_str}{{images/{image_path}/{image_name}}}\\n\"\n",
    "        f\"    {caption_str}\\n\"\n",
    "        f\"    {label_str}\\n\"\n",
    "        \"\\\\end{figure}\\n\"\n",
    "    )\n",
    "    print(latex_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac3bfb1-c53b-41c1-9991-c573de3694b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fio_results(file_path):\n",
    "    def parse_text_fio(file):\n",
    "        # Regular expressions\n",
    "        bandwidth_regex = re.compile(r'WRITE: bw=(\\d+(?:\\.\\d+)?)([MK]iB/s)')\n",
    "        bandwidth_read_regex = re.compile(r'READ: bw=(\\d+(?:\\.\\d+)?)([MK]iB/s)')\n",
    "        iops_regex = re.compile(r'write: IOPS=(\\d+)')\n",
    "        iops_read_regex = re.compile(r'read: IOPS=(\\d+)')\n",
    "        latency_regex = re.compile(r'lat (\\([mu]sec\\)): min=\\d+\\.?\\d*[km]?, max=\\d+\\.?\\d*[km]?, avg=(\\d+\\.\\d+[km]?), stdev=\\d+\\.?\\d*')\n",
    "\n",
    "        def convert_bandwidth(value, unit):\n",
    "            value = float(value)\n",
    "            if unit == \"KiB/s\":\n",
    "                return value / 1024\n",
    "            return value\n",
    "\n",
    "        results = {}\n",
    "        last = 'read'\n",
    "        for line in file:\n",
    "            if 'write' in line:\n",
    "                last = 'write'\n",
    "            elif 'read' in line:\n",
    "                last = 'read'\n",
    "\n",
    "            if (bw := bandwidth_regex.search(line)):\n",
    "                value, unit = bw.groups()\n",
    "                results['Bandwidth WRITE (MiB/s)'] = convert_bandwidth(value, unit)\n",
    "            if (bw := bandwidth_read_regex.search(line)):\n",
    "                value, unit = bw.groups()\n",
    "                results['Bandwidth READ (MiB/s)'] = convert_bandwidth(value, unit)\n",
    "            if (iops := iops_regex.search(line)):\n",
    "                results['IOPS WRITE'] = float(iops.group(1))\n",
    "            if (iops := iops_read_regex.search(line)):\n",
    "                results['IOPS READ'] = float(iops.group(1))\n",
    "            if (lat := latency_regex.search(line)):\n",
    "                lat_val = float(lat.group(2))\n",
    "                if lat.group(1) == '(usec)':\n",
    "                    lat_val /= 1000\n",
    "                results[f'Latency {last.upper()} (ms)'] = lat_val\n",
    "\n",
    "        return results\n",
    "\n",
    "    def parse_json_fio(data):\n",
    "        results = {}\n",
    "        for job in data.get('jobs', []):\n",
    "            for rw_type in ['read', 'write']:\n",
    "                if job.get(rw_type, {}).get('iops', 0) > 0:\n",
    "                    iops = job[rw_type]['iops']\n",
    "                    bw_kib = job[rw_type]['bw']\n",
    "                    latency_ns = job[rw_type].get('lat_ns', {}).get('mean', 0)\n",
    "\n",
    "                    results[f'IOPS {rw_type.upper()}'] = round(iops, 2)\n",
    "                    results[f'Bandwidth {rw_type.upper()} (MiB/s)'] = round(bw_kib / 1024, 2)\n",
    "                    results[f'Latency {rw_type.upper()} (ms)'] = round(latency_ns / 1_000_000, 3)\n",
    "\n",
    "        return results\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        first_char = f.read(1)\n",
    "        f.seek(0)\n",
    "        if first_char == '{':\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                return parse_json_fio(data)\n",
    "            except json.JSONDecodeError:\n",
    "                f.seek(0)\n",
    "                return parse_text_fio(f)\n",
    "        else:\n",
    "            return parse_text_fio(f)\n",
    "\n",
    "\n",
    "def parse_dd_results(file_path):\n",
    "    # Regular expressions\n",
    "    bandwidth_regex = re.compile(r'(\\d+(?:\\.\\d+)?) ([GMK]B/s)')\n",
    "    time_regex = re.compile(r'(\\d+(?:\\.\\d+)?) s')\n",
    "\n",
    "    # Function to convert bandwidth to MiB/s\n",
    "    def convert_bandwidth(value, unit):\n",
    "        value = float(value)\n",
    "        if unit == \"KB/s\":\n",
    "            return value / 1024  # Convert KB/s to MiB/s\n",
    "        elif unit == \"MB/s\":\n",
    "            return value  # Already in MiB/s\n",
    "        elif unit == \"GB/s\":\n",
    "            return value * 1024  # Convert GB/s to MiB/s\n",
    "        return value\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Match bandwidth\n",
    "            bw_match = bandwidth_regex.search(line)\n",
    "            if bw_match:\n",
    "                value, unit = bw_match.groups()\n",
    "                if 'write' in file_path:\n",
    "                    results['Bandwidth WRITE (MiB/s)'] = convert_bandwidth(value, unit)\n",
    "                else:\n",
    "                    results['Bandwidth READ (MiB/s)'] = convert_bandwidth(value, unit)\n",
    "\n",
    "            # Match time\n",
    "            time_match = time_regex.search(line)\n",
    "            if time_match:\n",
    "                results['Time (s)'] = float(time_match.group(1))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "def extract_values(resultsfolder, file_names, parser, program_type):\n",
    "    resultsdict = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(dict))))\n",
    "\n",
    "    def extract_key(file_name):\n",
    "        match = re.match(r\"^[^_]+_(.*?)_test\", file_name)\n",
    "        return match.group(1) if match else os.path.splitext(file_name)[0]\n",
    "\n",
    "    prepaths = [folder for folder in glob.glob(resultsfolder + '*/') if program_type in folder]\n",
    "    for prepath in prepaths:\n",
    "        folder_parts = prepath.split('\\\\')[-2].split('_')\n",
    "        filesystem = '_'.join(folder_parts[2:-1])\n",
    "        storage = folder_parts[-1]\n",
    "\n",
    "        if program_type in ['fio_results', 'dd_results']:\n",
    "            block_size_folders = [folder for folder in glob.glob(prepath + '*/')]\n",
    "            for block_size_folder in block_size_folders:\n",
    "                folder_name_parts = block_size_folder.split('\\\\')[-2].split('_')\n",
    "\n",
    "                compression = 'none'  # domyślnie brak kompresji\n",
    "                if len(folder_name_parts) >= 3 and folder_name_parts[0] == \"block\" and folder_name_parts[1] == \"size\":\n",
    "                    block_size = folder_name_parts[2]\n",
    "                    try:\n",
    "                        compression_idx = folder_name_parts.index(\"compression\")\n",
    "                        compression = '_'.join(folder_name_parts[compression_idx + 1:])\n",
    "                        if not compression:\n",
    "                            compression = 'none'\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                else:\n",
    "                    print(f\"Pomijanie folderu o nieoczekiwanej strukturze: {block_size_folder}\")\n",
    "                    continue\n",
    "\n",
    "                folders = [folder for folder in glob.glob(block_size_folder + '*/')]\n",
    "                cumulative_data = {}\n",
    "                for folder in folders:\n",
    "                    for file_name in file_names:\n",
    "                        file_path = os.path.join(folder, file_name)\n",
    "                        if os.path.exists(file_path):\n",
    "                            try:\n",
    "                                results = parser(file_path)\n",
    "                                if results:\n",
    "                                    test_key = extract_key(file_name)\n",
    "                                    if test_key not in cumulative_data:\n",
    "                                        cumulative_data[test_key] = defaultdict(list)\n",
    "                                    for key, value in results.items():\n",
    "                                        cumulative_data[test_key][key].append(value)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Błąd podczas parsowania {file_path}: {e}\")\n",
    "                        else:\n",
    "                            print(f\"Plik nie znaleziony: {file_path}\")\n",
    "\n",
    "                ranges = {}\n",
    "                for test_key, metrics in cumulative_data.items():\n",
    "                    if metrics:\n",
    "                        ranges[test_key] = {\n",
    "                            key: {\n",
    "                                'median': round(np.median(values), 3),\n",
    "                                'std': round(np.std(values), 3),\n",
    "                                'min': round(min(values), 3),\n",
    "                                'max': round(max(values), 3),\n",
    "                                'avg': round(sum(values) / len(values), 2)\n",
    "                            } if values else '-' for key, values in metrics.items()\n",
    "                        }\n",
    "\n",
    "                resultsdict[filesystem][block_size][compression][storage] = ranges\n",
    "        else:\n",
    "            # Programy bez rozmiaru bloku i kompresji (np. hdparm)\n",
    "            folders = [folder for folder in glob.glob(prepath + '*/')]\n",
    "            cumulative_data = {}\n",
    "            for folder in folders:\n",
    "                for file_name in file_names:\n",
    "                    file_path = os.path.join(folder, file_name)\n",
    "                    if os.path.exists(file_path):\n",
    "                        try:\n",
    "                            results = parser(file_path)\n",
    "                            if results:\n",
    "                                test_key = extract_key(file_name)\n",
    "                                if test_key not in cumulative_data:\n",
    "                                    cumulative_data[test_key] = defaultdict(list)\n",
    "                                for key, value in results.items():\n",
    "                                    cumulative_data[test_key][key].append(value)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Błąd podczas parsowania {file_path}: {e}\")\n",
    "                    else:\n",
    "                        print(f\"Plik nie znaleziony: {file_path}\")\n",
    "\n",
    "            ranges = {}\n",
    "            for test_key, metrics in cumulative_data.items():\n",
    "                if metrics:\n",
    "                    ranges[test_key] = {\n",
    "                        key: {\n",
    "                            'median': round(np.median(values), 3),\n",
    "                            'std': round(np.std(values), 3),\n",
    "                            'min': round(min(values), 3),\n",
    "                            'max': round(max(values), 3),\n",
    "                            'avg': round(sum(values) / len(values), 2)\n",
    "                        } if values else '-' for key, values in metrics.items()\n",
    "                    }\n",
    "\n",
    "            # Brak rozmiaru bloku — zapisujemy pod nazwą 'no_block_size' i compression='none'\n",
    "            resultsdict[filesystem]['no_block_size']['none'][storage] = ranges\n",
    "\n",
    "    return resultsdict\n",
    "\n",
    "\n",
    "\n",
    "def parse_hdparm_results(file_path):\n",
    "    # Regular expression to match the bandwidth\n",
    "    bandwidth_regex = re.compile(r'Timing O_DIRECT disk reads: (\\d+(?:\\.\\d+)?) MB in .* seconds = (\\d+(?:\\.\\d+)) MB/sec')\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Match bandwidth\n",
    "            bw_match = bandwidth_regex.search(line)\n",
    "            if bw_match:\n",
    "                total_mb, bandwidth = bw_match.groups()\n",
    "                results['Total Data Read (MB)'] = float(total_mb)\n",
    "                results['Bandwidth (MiB/s)'] = float(bandwidth)\n",
    "\n",
    "    return results\n",
    "    \n",
    "def extract_hdparm_values_by_device(resultsfolder, file_names, parser, group_by_computer=False):\n",
    "    # Adjust the default dictionary structure based on whether grouping by computer\n",
    "    resultsdict = (\n",
    "        defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "        if group_by_computer\n",
    "        else defaultdict(lambda: defaultdict(list))\n",
    "    )\n",
    "\n",
    "    # Iterate through all device type folders (e.g., hdparm_results_btrfs_hdd)\n",
    "    prepaths = glob.glob(os.path.join(resultsfolder, '*/'))\n",
    "    for prepath in prepaths:\n",
    "        # Check if folder starts with 'hdparm'\n",
    "        folder_name = os.path.basename(os.path.normpath(prepath))\n",
    "        if not folder_name.startswith('hdparm'):\n",
    "            continue  # skip non-hdparm folders\n",
    "\n",
    "        device_type = prepath.split('_')[-1].lower().strip('\\\\').strip('/')\n",
    "        \n",
    "        # Iterate through configuration folders\n",
    "        config_folders = glob.glob(os.path.join(prepath, '*/'))  # e.g., block_size_4096_compression_none/\n",
    "        for config_folder in config_folders:\n",
    "            # Iterate through computer folders (lab-sec-*)\n",
    "            computer_folders = glob.glob(os.path.join(config_folder, '*/'))\n",
    "            for computer_folder in computer_folders:\n",
    "                computer_name = os.path.basename(os.path.normpath(computer_folder))\n",
    "                \n",
    "                for file_name in file_names:\n",
    "                    file_path = os.path.join(computer_folder, file_name)\n",
    "                    if os.path.exists(file_path):\n",
    "                        try:\n",
    "                            # Parse the file and collect results\n",
    "                            results = parser(file_path)\n",
    "                            if group_by_computer:\n",
    "                                for key, value in results.items():\n",
    "                                    resultsdict[device_type][computer_name][key].append(value)\n",
    "                            else:\n",
    "                                for key, value in results.items():\n",
    "                                    resultsdict[device_type][key].append(value)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error parsing {file_path}: {e}\")\n",
    "                    else:\n",
    "                        print(f\"File not found: {file_path}\")\n",
    "\n",
    "    # Aggregate results by calculating min, max, and avg for each metric\n",
    "    aggregated_results = {}\n",
    "    for device_type, computers_or_metrics in resultsdict.items():\n",
    "        if group_by_computer:\n",
    "            aggregated_results[device_type] = {}\n",
    "            for computer, metrics in computers_or_metrics.items():\n",
    "                aggregated_results[device_type][computer] = {\n",
    "                    key: {\n",
    "                        'median': round(np.median(values), 3),\n",
    "                        'std': round(np.std(values), 3),\n",
    "                        'min': round(min(values), 3),\n",
    "                        'max': round(max(values), 3),\n",
    "                        'avg': round(sum(values) / len(values), 2)\n",
    "                    } if values else '-' for key, values in metrics.items()\n",
    "                }\n",
    "        else:\n",
    "            aggregated_results[device_type] = {\n",
    "                key: {\n",
    "                    'median': round(np.median(values), 3),\n",
    "                    'std': round(np.std(values), 3),\n",
    "                    'min': round(min(values), 3),\n",
    "                    'max': round(max(values), 3),\n",
    "                    'avg': round(sum(values) / len(values), 2)\n",
    "                } if values else '-' for key, values in computers_or_metrics.items()\n",
    "            }\n",
    "\n",
    "    return aggregated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9363a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = \"zwykle\"\n",
    "resultsfolder = ''\n",
    "fio_file_names = []\n",
    "\n",
    "if tests == \"article\":\n",
    "    resultsfolder = '../wyniki_50G_article/'\n",
    "    fio_file_names = [\n",
    "    'fio_database_article_test_output.txt',\n",
    "    'fio_seq_read_article_test_output.txt',\n",
    "    'fio_seq_write_article_test_output.txt',\n",
    "    ]\n",
    "elif tests == \"zwykle\":\n",
    "    resultsfolder = '../wyniki_zwykle/'\n",
    "    fio_file_names = [\n",
    "        'fio_database_test_output.txt',\n",
    "        'fio_multimedia_test_output.txt',\n",
    "        'fio_webserver_test_output.txt',\n",
    "        'fio_archive_test_output.txt',\n",
    "    ]\n",
    "\n",
    "\n",
    "fio_resultsdict = extract_values(resultsfolder, fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "print_dict_tree(fio_resultsdict, max_depth=4)\n",
    "\n",
    "# Example usage for dd\n",
    "dd_file_names = [\n",
    "    'dd_read_test_output.txt',\n",
    "    'dd_write_test_output.txt',\n",
    "]\n",
    "\n",
    "dd_resultsdict = extract_values(resultsfolder, dd_file_names, parse_dd_results, program_type='dd_results')\n",
    "print(dd_resultsdict)\n",
    "\n",
    "hdparm_file_names = [\n",
    "    'hdparm_test_output.txt',\n",
    "]\n",
    "\n",
    "hdparm_resultsdict = extract_hdparm_values_by_device(resultsfolder, hdparm_file_names, parse_hdparm_results, group_by_computer=False)\n",
    "\n",
    "print(hdparm_resultsdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef3785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_merge_results(dict1, dict2):\n",
    "    \"\"\"Rekurencyjnie scala dict2 do dict1 na głębokość 3 poziomów.\"\"\"\n",
    "    for fs, block_sizes in dict2.items():\n",
    "        if fs not in dict1:\n",
    "            dict1[fs] = block_sizes\n",
    "            continue\n",
    "        for block_size, storages in block_sizes.items():\n",
    "            if block_size not in dict1[fs]:\n",
    "                dict1[fs][block_size] = storages\n",
    "                continue\n",
    "            for storage, workloads in storages.items():\n",
    "                if storage not in dict1[fs][block_size]:\n",
    "                    dict1[fs][block_size][storage] = workloads\n",
    "                else:\n",
    "                    # Jeśli istnieje, scal workloady (np. database, multimedia, itd.)\n",
    "                    for workload, metrics in workloads.items():\n",
    "                        if workload not in dict1[fs][block_size][storage]:\n",
    "                            dict1[fs][block_size][storage][workload] = metrics\n",
    "                        else:\n",
    "                            # Jeśli istnieje, scal metryki (np. Bandwidth READ, IOPS, itd.)\n",
    "                            dict1[fs][block_size][storage][workload].update(metrics)\n",
    "\n",
    "\n",
    "if tests == \"article\":\n",
    "    fio_file_names = [\n",
    "    'fio_database_article_test_output.json',\n",
    "    'fio_seq_read_article_test_output.json',\n",
    "    'fio_seq_write_article_test_output.json',\n",
    "    ]\n",
    "\n",
    "    fio_resultsdict_raid= extract_values('../wyniki_raid_article_50G/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "    fio_resultsdict_raid_compression= extract_values('../wyniki_raid_article_50G_kompresja/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "    deep_merge_results(fio_resultsdict, fio_resultsdict_raid)\n",
    "    deep_merge_results(fio_resultsdict, fio_resultsdict_raid_compression)\n",
    "    print_dict_tree(fio_resultsdict, max_depth=3)\n",
    "\n",
    "elif tests == \"zwykle\":\n",
    "    fio_file_names = [\n",
    "        'fio_database_test_output.json',\n",
    "        'fio_multimedia_test_output.json',\n",
    "        'fio_webserver_test_output.json',\n",
    "        'fio_archive_test_output.json',\n",
    "    ]\n",
    "    #Scalanie różnych folderów wyników\n",
    "\n",
    "    # Usuń dane dla zfs i zfs_nocache\n",
    "    fio_resultsdict.pop('zfs', None)\n",
    "    fio_resultsdict.pop('zfs_nocache', None)\n",
    "    fio_resultsdict.pop('zfs_primary', None)\n",
    "\n",
    "    # Wczytaj dane z drugiego folderu\n",
    "    #fio_resultsdict_new = extract_values('../wyniki_zwykle_l2arc/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "    fio_resultsdict_zfs_stats = extract_values('../wyniki_zwykle_zfs_stats/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "    fio_resultsdict_kompresja = extract_values('../wyniki_zwykle_kompresja/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "    fio_resultsdict_raid = extract_values('../wyniki_raid/', fio_file_names, parse_fio_results, program_type='fio_results')\n",
    "    # Dodaj dane z drugiego folderu do głównego zbioru\n",
    "    #fio_resultsdict.update(fio_resultsdict_new)\n",
    "    deep_merge_results(fio_resultsdict, fio_resultsdict_zfs_stats)\n",
    "    deep_merge_results(fio_resultsdict, fio_resultsdict_kompresja)\n",
    "    deep_merge_results(fio_resultsdict, fio_resultsdict_raid)\n",
    "    print_dict_tree(fio_resultsdict, max_depth=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0a3d80",
   "metadata": {},
   "source": [
    "Funkcja do generowania wykresów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb5d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def darken_color(color, amount=0.7):\n",
    "    \"\"\"Zmniejsz jasność koloru RGB.\"\"\"\n",
    "    c = np.array(to_rgb(color))\n",
    "    return tuple(np.clip(c * amount, 0, 1))\n",
    "\n",
    "def plot_performance_metrics(data, metrics, storage_types, block_sizes,\n",
    "                              include_min_max=False, include_std=False,\n",
    "                              workload=None,\n",
    "                              file_systems=None, colors=None,\n",
    "                              hdparm_data=None, hdparm_stat='avg',\n",
    "                              preserve_ylim=False, debug=False,\n",
    "                              combine_storage_types=False, combine_compression_types=False, save_dir=None, compression_types=None):\n",
    "\n",
    "    metric_translations = {\n",
    "        \"Bandwidth (MiB/s)\": \"Przepustowość (MiB/s)\",\n",
    "        \"Bandwidth READ (MiB/s)\": \"Odczyt (MiB/s)\",\n",
    "        \"Bandwidth WRITE (MiB/s)\": \"Zapis (MiB/s)\",\n",
    "        \"IOPS\": \"IOPS\",\n",
    "        \"IOPS READ\": \"IOPS - odczyt\",\n",
    "        \"IOPS WRITE\": \"IOPS - zapis\",\n",
    "        \"Latency (ms)\": \"Opóźnienie (ms)\",\n",
    "        \"Latency READ (ms)\": \"Opóźnienie - odczyt (ms)\",\n",
    "        \"Latency WRITE (ms)\": \"Opóźnienie - zapis (ms)\"\n",
    "    }\n",
    "\n",
    "    workload_translations = {\n",
    "        \"database\": \"baza danych\",\n",
    "        \"multimedia\": \"multimedia\",\n",
    "        \"webserver\": \"serwer WWW\",\n",
    "        \"archive\": \"archiwum\"\n",
    "    }\n",
    "\n",
    "    storage_type_translations = {\n",
    "        \"NVME\": \"NVMe\",\n",
    "        \"RAID02HDD\": \"RAID0 2 HDD\",\n",
    "        \"RAID04HDD\": \"RAID0 3 HDD\",\n",
    "        \"STRIPE4HDD\": \"Stripe 4 HDD\",\n",
    "        \"STRIPE2HDD\": \"Stripe 2 HDD\",\n",
    "        \"MIRROR\": \"Mirror\",\n",
    "    }\n",
    "\n",
    "    compression_translations = {\n",
    "    \"none\": \"brak\",\n",
    "    \"lz4\": \"lz4\",\n",
    "    \"zstd\": \"zstd\",\n",
    "    }\n",
    "\n",
    "    if colors is None:\n",
    "        colors = ['b', 'g', 'r', 'c', 'm', 'y', 'orange', '#FFD700', '#A0522D']\n",
    "    if file_systems is None:\n",
    "        file_systems = list(data.keys())\n",
    "\n",
    "    for block_size in block_sizes:\n",
    "        compression_list = compression_types if compression_types else [\"none\"]\n",
    "        if combine_compression_types:\n",
    "            compression_list = [None]\n",
    "        for compression in compression_list:\n",
    "            storage_list = storage_types if not combine_storage_types else [None]\n",
    "\n",
    "            for storage in storage_list:\n",
    "                prepared_data = {}\n",
    "\n",
    "                for fs in file_systems:\n",
    "                    if combine_storage_types:\n",
    "                        fs_block_data = data.get(fs, {})\n",
    "                        for st in storage_types:\n",
    "                            combined_label = f\"{fs} {storage_type_translations.get(st.upper(), st.upper())}\"\n",
    "                            for comp in (compression_types if combine_compression_types else [compression]):\n",
    "                                storage_data = (\n",
    "                                    fs_block_data.get(block_size, {}).get(comp, {}).get(st.lower()) or\n",
    "                                    fs_block_data.get('default', {}).get(comp, {}).get(st.lower())\n",
    "                                )\n",
    "                                if not storage_data:\n",
    "                                    print(f\"⚠️ Pomijanie {fs} - brak danych dla storage '{st}' w block_size '{block_size}' lub 'default'\")\n",
    "                                    continue\n",
    "\n",
    "                                workload_data = storage_data.get(workload)\n",
    "                                if not isinstance(workload_data, dict):\n",
    "                                    print(f\"⚠️ Pomijanie {fs} - storage '{st}': brak danych dla workload '{workload}'\")\n",
    "                                    continue\n",
    "                                label = f\"{combined_label} ({compression_translations.get(comp, comp)})\" if combine_compression_types else combined_label\n",
    "                                prepared_data[label] = workload_data\n",
    "                    else:\n",
    "                        fs_block_data = data.get(fs, {}).get(block_size, {}) or data.get(fs, {}).get('default', {})\n",
    "                        for comp in (compression_types if combine_compression_types else [compression]):\n",
    "                            storage_data = fs_block_data.get(comp, {}).get(storage.lower())\n",
    "                            if not storage_data:\n",
    "                                continue\n",
    "                            workload_data = storage_data.get(workload)\n",
    "                            if not isinstance(workload_data, dict):\n",
    "                                continue\n",
    "                            label = f\"{fs} ({compression_translations.get(comp, comp)})\" if combine_compression_types else fs\n",
    "                            prepared_data[label] = workload_data\n",
    "\n",
    "\n",
    "                if not prepared_data:\n",
    "                    print(f\"⚠️ Brak danych po przetworzeniu dla compression={compression}, block_size={block_size}, workload={workload}\")\n",
    "                    continue\n",
    "\n",
    "                plots = []\n",
    "                for metric in metrics:\n",
    "                    if isinstance(metric, dict):\n",
    "                        read_metric = metric.get(\"read\")\n",
    "                        write_metric = metric.get(\"write\")\n",
    "                        first_key = next(iter(prepared_data), None)\n",
    "                        if not first_key:\n",
    "                            continue\n",
    "                        sample = prepared_data[first_key]\n",
    "                        if read_metric and write_metric and read_metric in sample and write_metric in sample:\n",
    "                            plots.append((\"grouped\", metric[\"name\"], read_metric, write_metric))\n",
    "                        elif read_metric and read_metric in sample:\n",
    "                            plots.append((\"single\", read_metric))\n",
    "                        elif write_metric and write_metric in sample:\n",
    "                            plots.append((\"single\", write_metric))\n",
    "                        else:\n",
    "                            plots.append((\"single\", metric[\"name\"]))\n",
    "                    else:\n",
    "                        first_key = next(iter(prepared_data), None)\n",
    "                        if first_key and metric in prepared_data[first_key]:\n",
    "                            plots.append((\"single\", metric))\n",
    "\n",
    "                fig, axs = plt.subplots(len(plots), 1, figsize=(10, 3 * len(plots)))\n",
    "                if len(plots) == 1:\n",
    "                    axs = [axs]\n",
    "\n",
    "                # Zwiększ przestrzeń na tytuł, ale zachowaj kontrolę nad \"plot area\"\n",
    "                fig.subplots_adjust(top=0.9, bottom=0.15, left=0.15, right=0.95)\n",
    "\n",
    "                # Ustaw stały rozmiar plot area (w jednostkach figure-coordinates)\n",
    "                fixed_ax_height = 0.6 / len(plots)  # rozdziel proporcjonalnie\n",
    "                for i, ax in enumerate(axs):\n",
    "                    ax.set_position([0.15, 0.9 - (i+1)*fixed_ax_height, 0.8, fixed_ax_height * 0.9])\n",
    "\n",
    "                \n",
    "                translated_workload = workload_translations.get(workload, workload)\n",
    "                translated_block_size = \"domyślny\" if block_size == \"default\" else block_size\n",
    "\n",
    "                combined_label = \", \".join(\n",
    "                    [storage_type_translations.get(s.upper(), s.upper()) for s in storage_types]\n",
    "                ) if combine_storage_types else storage_type_translations.get(storage.upper(), storage.upper())\n",
    "\n",
    "                if combine_compression_types:\n",
    "                    compression_label = \", \".join([compression_translations.get(c, c) for c in compression_types])\n",
    "                else:\n",
    "                    compression_label = compression_translations.get(compression, compression)\n",
    "\n",
    "                fig.suptitle(\n",
    "                    f'{translated_workload.capitalize() if workload else \"\"}'\n",
    "                    f'\\nTyp dysku: {combined_label}'\n",
    "                    f'\\nRozmiar bloku: {translated_block_size}'\n",
    "                    f'\\nKompresja: {compression_label}'\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "                for i, plot in enumerate(plots):\n",
    "                    ax = axs[i]\n",
    "\n",
    "                    if plot[0] == \"grouped\":\n",
    "                        _, base_name, read_metric, write_metric = plot\n",
    "                        fs_labels = []\n",
    "                        read_vals, write_vals = [], []\n",
    "                        read_err, write_err = ([], []), ([], [])\n",
    "                        color_map = {}\n",
    "\n",
    "                        for idx, label in enumerate(prepared_data):\n",
    "                            workload_data = prepared_data[label]\n",
    "                            read_data = workload_data.get(read_metric, {})\n",
    "                            write_data = workload_data.get(write_metric, {})\n",
    "\n",
    "                            read_avg = read_data.get('avg')\n",
    "                            write_avg = write_data.get('avg')\n",
    "                            if read_avg is not None and write_avg is not None:\n",
    "                                fs_labels.append(label)\n",
    "                                read_vals.append(read_avg)\n",
    "                                write_vals.append(write_avg)\n",
    "\n",
    "                                if include_min_max:\n",
    "                                    read_min = read_data.get('min', read_avg)\n",
    "                                    read_max = read_data.get('max', read_avg)\n",
    "                                    read_err[0].append(read_avg - read_min)\n",
    "                                    read_err[1].append(read_max - read_avg)\n",
    "\n",
    "                                    write_min = write_data.get('min', write_avg)\n",
    "                                    write_max = write_data.get('max', write_avg)\n",
    "                                    write_err[0].append(write_avg - write_min)\n",
    "                                    write_err[1].append(write_max - write_avg)\n",
    "\n",
    "                                elif include_std:\n",
    "                                    read_std = read_data.get('std', 0)\n",
    "                                    write_std = write_data.get('std', 0)\n",
    "                                    read_err[0].append(read_std)\n",
    "                                    read_err[1].append(read_std)\n",
    "                                    write_err[0].append(write_std)\n",
    "                                    write_err[1].append(write_std)\n",
    "\n",
    "\n",
    "                                color_map[label] = colors[idx % len(colors)]\n",
    "\n",
    "                        x = np.arange(len(fs_labels))\n",
    "                        bar_width = 0.35\n",
    "                        read_colors = [color_map[fs] for fs in fs_labels]\n",
    "                        write_colors = [darken_color(color_map[fs]) for fs in fs_labels]\n",
    "\n",
    "                        read_bars = ax.bar(x - bar_width/2, read_vals, bar_width,\n",
    "                                        label='_Odczyt', color=read_colors,\n",
    "                                        yerr=read_err if include_min_max or include_std else None, capsize=5)\n",
    "                        write_bars = ax.bar(x + bar_width/2, write_vals, bar_width,\n",
    "                                            label='_Zapis', color=write_colors,\n",
    "                                            yerr=write_err if include_min_max or include_std else None, capsize=5)\n",
    "\n",
    "                        for j, bar in enumerate(read_bars):\n",
    "                            y = bar.get_height()\n",
    "                            err = read_err[1][j] if include_min_max or include_std else 0\n",
    "                            ax.text(bar.get_x() + bar.get_width() / 2, y + err + 0.02 * y,\n",
    "                                    f'{y:.2f}\\nOdczyt', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "                        for j, bar in enumerate(write_bars):\n",
    "                            y = bar.get_height()\n",
    "                            err = write_err[1][j] if include_min_max or include_std else 0\n",
    "                            ax.text(bar.get_x() + bar.get_width() / 2, y + err + 0.02 * y,\n",
    "                                    f'{y:.2f}\\nZapis', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "                        max_height = max(\n",
    "                            [v + e for v, e in zip(read_vals, read_err[1])] +\n",
    "                            [v + e for v, e in zip(write_vals, write_err[1])]\n",
    "                        ) if include_min_max or include_std else max(max(read_vals, default=0), max(write_vals, default=0))\n",
    "                        ax.set_ylim(0, max_height * 1.4)\n",
    "\n",
    "                        translated_label = metric_translations.get(base_name, base_name)\n",
    "                        ax.set_xticks(x)\n",
    "                        if len(fs_labels) > 6:\n",
    "                            ax.set_xticklabels(fs_labels, rotation=45, ha='center', fontsize=8)\n",
    "                        else:\n",
    "                            ax.set_xticks(x)\n",
    "                            ax.set_xticklabels(fs_labels, fontsize=10)  \n",
    "\n",
    "                        ax.set_ylabel(translated_label)\n",
    "                        ax.set_title(translated_label)\n",
    "                        # ax.legend()\n",
    "                        if hdparm_data and hdparm_stat and metric[\"name\"] == \"Bandwidth (MiB/s)\" and not combine_storage_types:\n",
    "                            if storage and storage.lower() in hdparm_data:\n",
    "                                device_data = hdparm_data[storage.lower()]\n",
    "                                hdparm_val = device_data.get(\"Bandwidth (MiB/s)\", {}).get(hdparm_stat)\n",
    "                                if hdparm_val:\n",
    "                                    ylim = ax.get_ylim()\n",
    "                                    label_text = f'hdparm {storage} ({hdparm_val:.1f} MiB/s)'\n",
    "                                    if hdparm_val <= ylim[1]:\n",
    "                                        ax.axhline(hdparm_val, color='#8B0000', linestyle='--', linewidth=1.5, alpha=0.5 , label=label_text)\n",
    "                                        ax.legend(fontsize=8, loc='upper left', bbox_to_anchor=(0.78, 1.25))\n",
    "                                    else:\n",
    "                                        # Dodaj \"fake\" obiekt do legendy\n",
    "                                        legend_line = Line2D([0], [0], color='#8B0000', linestyle='--', linewidth=1.5, alpha=0.5)\n",
    "                                        ax.legend([legend_line], [label_text], fontsize=8, loc='upper left', bbox_to_anchor=(0.78, 1.25))\n",
    "                            elif debug:\n",
    "                                print(f\"⚠️ Brak danych hdparm dla {storage} w {metric['name']}\")\n",
    "\n",
    "                    elif plot[0] == \"single\":\n",
    "                        _, metric = plot\n",
    "                        fs_labels = []\n",
    "                        avg_values = []\n",
    "                        y_errs = ([], [])\n",
    "                        color_map = {}\n",
    "\n",
    "                        for idx, label in enumerate(prepared_data):\n",
    "                            workload_data = prepared_data[label]\n",
    "                            metric_data = workload_data.get(metric, {})\n",
    "                            avg = metric_data.get(\"avg\")\n",
    "                            if avg is not None:\n",
    "                                fs_labels.append(label)\n",
    "                                avg_values.append(avg)\n",
    "\n",
    "                                if include_min_max:\n",
    "                                    min_val = metric_data.get(\"min\", avg)\n",
    "                                    max_val = metric_data.get(\"max\", avg)\n",
    "                                    y_errs[0].append(avg - min_val)\n",
    "                                    y_errs[1].append(max_val - avg)\n",
    "                                elif include_std:\n",
    "                                    std_val = metric_data.get(\"std\", 0)\n",
    "                                    y_errs[0].append(std_val)\n",
    "                                    y_errs[1].append(std_val)\n",
    "\n",
    "\n",
    "                                color_map[label] = colors[idx % len(colors)]\n",
    "\n",
    "                        bars = ax.bar(fs_labels, avg_values, color=[color_map[fs] for fs in fs_labels],\n",
    "                                    yerr=y_errs if include_min_max or include_std else None, capsize=5)\n",
    "\n",
    "                        if len(fs_labels) > 6:\n",
    "                            ax.set_xticks(range(len(fs_labels)))\n",
    "                            ax.set_xticklabels(fs_labels, rotation=45, ha='center', fontsize=8)\n",
    "                        else:\n",
    "                            ax.set_xticks(range(len(fs_labels)))\n",
    "                            ax.set_xticklabels(fs_labels, fontsize=10)\n",
    "\n",
    "                        for j, bar in enumerate(bars):\n",
    "                            y = bar.get_height()\n",
    "                            err = y_errs[1][j] if include_min_max or include_std else 0\n",
    "                            ax.text(bar.get_x() + bar.get_width() / 2, y + err + 0.02 * y,\n",
    "                                    f'{y:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "                        if avg_values:\n",
    "                            max_height = max([v + e for v, e in zip(avg_values, y_errs[1])] if include_min_max or include_std else avg_values)\n",
    "                            ax.set_ylim(0, max_height * 1.4)\n",
    "\n",
    "                        translated_label = metric_translations.get(metric, metric)\n",
    "                        ax.set_ylabel(translated_label)\n",
    "                        ax.set_title(translated_label)\n",
    "                        if hdparm_data and hdparm_stat and \"bandwidth\" in metric.lower() and not combine_storage_types:\n",
    "                            if storage and storage.lower() in hdparm_data:\n",
    "                                device_data = hdparm_data[storage.lower()]\n",
    "                                hdparm_val = device_data.get(\"Bandwidth (MiB/s)\", {}).get(hdparm_stat)\n",
    "                                if hdparm_val:\n",
    "                                    ylim = ax.get_ylim()\n",
    "                                    label_text = f'hdparm {storage} ({hdparm_val:.1f} MiB/s)'\n",
    "                                    if hdparm_val <= ylim[1]:\n",
    "                                        ax.axhline(hdparm_val, color='#8B0000', linestyle='--', linewidth=1.5, alpha=0.5, label=label_text)\n",
    "                                        ax.legend(fontsize=8, loc='upper left', bbox_to_anchor=(0.78, 1.25))\n",
    "\n",
    "                                    else:\n",
    "                                        # Dodaj \"fake\" obiekt do legendy\n",
    "                                        legend_line = Line2D([0], [0], color='#8B0000', linestyle='--', alpha=0.5, linewidth=1.5)\n",
    "                                        ax.legend([legend_line], [label_text], fontsize=8, loc='upper left', bbox_to_anchor=(0.78, 1.25))\n",
    "                            elif debug:\n",
    "                                print(f\"⚠️ Brak danych hdparm dla {storage} w {metric}\")\n",
    "\n",
    "                plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "                if save_dir:\n",
    "                    os.makedirs(\"wykresy/\"+save_dir, exist_ok=True)\n",
    "                    safe_combined_label = combined_label.replace(\" \", \"\").replace(\"/\", \"\")\n",
    "                    safe_compression_label = \"combined\" if combine_compression_types else compression.lower()\n",
    "                    safe_file_systems = \"_\".join([fs.replace(\"_\", \"\") for fs in file_systems])\n",
    "                    filename = f\"syntetyk_{safe_file_systems}_{safe_combined_label.lower()}_{block_size.lower()}_{safe_compression_label}_{workload.lower()}.png\"\n",
    "                    filepath = os.path.join(\"wykresy/\"+save_dir, filename)\n",
    "                    fig.savefig(filepath, bbox_inches='tight')\n",
    "                    if debug:\n",
    "                        print(f\"📁 Zapisano wykres do pliku: {filepath}\")\n",
    "                    print_latex_image(save_dir, filename)\n",
    "                else:\n",
    "                    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1b1198",
   "metadata": {},
   "source": [
    "Funkcja do generowania tabelki fio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ad6153-be83-4703-bb31-52f4d1bc2602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_row_data(data, workload, columns, file_systems=None, block_sizes=None, compressions=None):\n",
    "    rows = []\n",
    "    for fs, block_data in data.items():\n",
    "        if file_systems and fs not in file_systems:\n",
    "            continue\n",
    "        for block_size in block_sizes if block_sizes else block_data.keys():\n",
    "            block_entry = block_data.get(block_size) or block_data.get(\"default\")\n",
    "            if not block_entry:\n",
    "                continue\n",
    "            for compression, devices in block_entry.items():\n",
    "                if compressions and compression not in compressions:\n",
    "                    continue\n",
    "                row = [fs, block_size, compression]\n",
    "                for col in columns[3:]:  # Skip File System, Block Size, Compression\n",
    "                    if len(col.split()) > 3:\n",
    "                        col = col.split()\n",
    "                        storage, metric, stat = col[0], col[1] + ' ' + col[2], col[3]\n",
    "                    else:\n",
    "                        storage, metric, stat = col.split(\" \", 2)\n",
    "                    if \"Bandwidth\" in metric:\n",
    "                        metric_key = f\"{metric} (MiB/s)\"\n",
    "                    elif \"Latency\" in metric:\n",
    "                        metric_key = f\"{metric} (ms)\"\n",
    "                    else:\n",
    "                        metric_key = metric\n",
    "                    value = \"N/A\"\n",
    "                    for device_type, workloads in devices.items():\n",
    "                        if device_type.lower() == storage.lower() and workload in workloads:\n",
    "                            value = workloads[workload].get(metric_key, {}).get(stat.lower(), \"N/A\")\n",
    "                            break\n",
    "                    row.append(value)\n",
    "                rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def generate_columns(metrics, stats=[\"MIN\", \"AVG\", \"MAX\"], storage_types=[\"HDD\", \"SSD\", \"NVME\"]):\n",
    "    columns = [\"File System\", \"Block Size\", \"Compression\"]\n",
    "    for storage in storage_types:\n",
    "        for metric in metrics:\n",
    "            for stat in stats:\n",
    "                columns.append(f\"{storage} {metric} {stat}\")\n",
    "    return columns\n",
    "\n",
    "\n",
    "def display_performance_metrics(data, workloads, metrics, stats=[\"MIN\", \"AVG\", \"MAX\"],\n",
    "                                storage_types=[\"HDD\", \"SSD\", \"NVME\"], file_systems=None,\n",
    "                                block_sizes=None, compressions=None):\n",
    "    for workload in workloads:\n",
    "        columns = generate_columns(metrics, stats, storage_types)\n",
    "        rows = extract_row_data(data, workload, columns, file_systems, block_sizes, compressions)\n",
    "        df = pd.DataFrame(rows, columns=columns)\n",
    "        display(df.style.set_caption(f\"Performance Metrics: {workload.capitalize()}\").format(precision=3))\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if tests == \"article\":\n",
    "    workloads = [\"database_article\", \"seq_read_article\", \"seq_write_article\"] \n",
    "elif tests == \"zwykle\":\n",
    "    workloads = [\"database\", \"multimedia\", \"webserver\", \"archive\"]\n",
    "\n",
    "metrics = {\n",
    "    'database_article': [\"Bandwidth READ\", \"Bandwidth WRITE\", \"IOPS READ\", \"IOPS WRITE\", \"Latency READ\", \"Latency WRITE\"],\n",
    "    'seq_write_article': [\"Bandwidth WRITE\", \"IOPS WRITE\", \"Latency WRITE\"],\n",
    "    'seq_read_article': [\"Bandwidth READ\", \"IOPS READ\", \"Latency READ\"],\n",
    "    'database': [\"Bandwidth READ\", \"Bandwidth WRITE\", \"IOPS READ\", \"IOPS WRITE\", \"Latency READ\", \"Latency WRITE\"],\n",
    "    'archive': [\"Bandwidth WRITE\", \"IOPS WRITE\", \"Latency WRITE\"],\n",
    "    'multimedia': [\"Bandwidth READ\", \"IOPS READ\", \"Latency READ\"],\n",
    "    'webserver': [\"Bandwidth READ\", \"IOPS READ\", \"Latency READ\"],\n",
    "    \"default\": [\"Bandwidth READ\", \"Bandwidth WRITE\", \"IOPS READ\", \"IOPS WRITE\", \"Latency READ\", \"Latency WRITE\"],\n",
    "}\n",
    "block_sizes = [\"default\"]  # Specify block sizes to display\n",
    "#storage_types=[\"hdd\", \"stripe2hdd\", \"stripe4hdd\", \"mirror\", \"raidz1\", \"raidz2\", \"raid10\"]\n",
    "storage_types=[\"hdd\"]\n",
    "# Generate and display tables for each workload\n",
    "for workload in workloads:\n",
    "  \n",
    "    workload_metrics = metrics.get(workload, metrics[\"default\"])\n",
    "    display_performance_metrics(fio_resultsdict, [workload], workload_metrics, block_sizes=block_sizes, storage_types=storage_types, stats = [\"AVG\"], file_systems = [\"zfs_nocache\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b62525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "metrics = [\n",
    "    {\"name\": \"Bandwidth (MiB/s)\", \"read\": \"Bandwidth READ (MiB/s)\", \"write\": \"Bandwidth WRITE (MiB/s)\"},\n",
    "    #{\"name\": \"IOPS\", \"read\": \"IOPS READ\", \"write\": \"IOPS WRITE\"}, \n",
    "    #{\"name\": \"Latency (ms)\", \"read\": \"Latency READ (ms)\", \"write\": \"Latency WRITE (ms)\"},\n",
    "]\n",
    "\n",
    "if tests == \"article\":\n",
    "    workloads = [\"database_article\", \"seq_read_article\", \"seq_write_article\"]\n",
    "elif tests == \"zwykle\":\n",
    "    workloads = [\"database\", \"multimedia\", \"webserver\", \"archive\"]\n",
    "\n",
    "#storage_types = ['HDD','RAID02HDD', 'RAID04HDD', 'RAID1', 'RAID5', 'RAID6', 'RAID10']\n",
    "#storage_types = [\"HDD\"]\n",
    "\n",
    "block_sizes = ['default']  \n",
    "\n",
    "file_systems = ['zfs']  #['exfat','ext4', 'xfs', 'btrfs', \"f2fs\", \"zfs\", \"zfs_nocache\"]\n",
    "\n",
    "#workloads = [\"database\"]\n",
    "\n",
    "#compression_types = [\"none\",\"zlib_1\", \"zlib_3\", \"zlib-9\", \"zstd_1\", \"zstd_1\", \"zstd_3\", \"zstd_9\", \"zstd_15\"]  # Kompresja brfs\n",
    "compression_types = [\"none\", \"gzip-1\", \"gzip-3\", \"gzip-9\", \"lz4\"]  # Kompresja zfs\n",
    "\n",
    "for workload in workloads:\n",
    "    #plot_performance_metrics(fio_resultsdict, metrics, storage_types, block_sizes, include_min_max=True, workload=workload, hdparm_data=hdparm_resultsdict, hdparm_stat='avg' )\n",
    "    plot_performance_metrics(fio_resultsdict, metrics, storage_types, block_sizes, compression_types=compression_types, include_min_max=True, workload=workload, combine_storage_types=True, combine_compression_types=True, file_systems=file_systems, hdparm_data=hdparm_resultsdict, hdparm_stat='avg', preserve_ylim=True, debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb57889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate all possible columns\n",
    "def generate_columns(metrics, stats=[\"MIN\", \"MAX\", \"AVG\"], storage_types=[\"HDD\", \"SSD\", \"NVME\"], file_systems=None):\n",
    "    columns = [\"File System\", \"Block Size\"]  # Include Block Size in columns\n",
    "    for storage in storage_types:\n",
    "        for metric in metrics:\n",
    "            for stat in stats:\n",
    "                columns.append(f\"{storage} {metric} {stat}\")\n",
    "    return columns\n",
    "\n",
    "def extract_row_data(data, columns, file_systems=None, block_sizes=None):\n",
    "    rows = []\n",
    "    for fs, block_data in data.items():\n",
    "        if file_systems and fs not in file_systems:\n",
    "            continue\n",
    "        for block_size, devices in block_data.items():  # Iterate over block sizes\n",
    "            if block_sizes and block_size not in block_sizes:\n",
    "                continue\n",
    "            row = [fs, block_size]  # Add File System and Block Size to the row\n",
    "            for col in columns[2:]:  # Skip File System and Block Size\n",
    "                if len(col.split()) > 3:\n",
    "                    col = col.split()\n",
    "                    storage, metric, stat = col[0], col[1] + ' ' + col[2], col[3]\n",
    "                else:\n",
    "                    storage, metric, stat = col.split(\" \", 2)\n",
    "                metric_key = f\"{metric} (MiB/s)\" if \"Bandwidth\" in metric else metric\n",
    "                # Extract value\n",
    "                value = \"N/A\"\n",
    "                for device_type, workloads in devices.items():\n",
    "                    if device_type.lower() == storage.lower():\n",
    "                        for operation, metrics in workloads.items():\n",
    "                            if metric_key in metrics:\n",
    "                                value = metrics[metric_key].get(stat.lower(), \"N/A\")\n",
    "                                break\n",
    "                row.append(value)\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "# Example data\n",
    "# Generate and display tables for dd_resultsdict\n",
    "columns = generate_columns([\"Bandwidth READ\", \"Bandwidth WRITE\"], stats=[\"MIN\", \"AVG\", \"MAX\"], storage_types=[\"HDD\", \"SSD\", \"NVME\"])\n",
    "rows = extract_row_data(dd_resultsdict, columns, block_sizes=[\"4096\"])  # Specify block sizes\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "display(df.style.set_caption(\"Performance Metrics: DD Results\").format(precision=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot performance metrics for dd data\n",
    "plot_performance_metrics(dd_resultsdict, metrics=[\"Bandwidth READ (MiB/s)\", \"Bandwidth WRITE (MiB/s)\"], storage_types=storage_types, block_sizes=['4096']\n",
    ", include_min_max=True, hdparm_data=hdparm_resultsdict, hdparm_stat='max' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5410dd78",
   "metadata": {},
   "source": [
    "#TODO dodanie uniwersalnej funkcji fo tabelek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9276861f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
