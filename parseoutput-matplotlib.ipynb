{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac3bfb1-c53b-41c1-9991-c573de3694b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 112\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resultsdict\n\u001b[0;32m    105\u001b[0m file_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfio_database_test_output.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfio_multimedia_test_output.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfio_webserver_test_output.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfio_archive_test_output.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    110\u001b[0m ]\n\u001b[1;32m--> 112\u001b[0m resultsdict \u001b[38;5;241m=\u001b[39m \u001b[43mextract_values\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./wyniki/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Function to generate all possible columns\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_columns\u001b[39m(metrics):\n",
      "Cell \u001b[1;32mIn[4], line 76\u001b[0m, in \u001b[0;36mextract_values\u001b[1;34m(resultsfolder, file_names)\u001b[0m\n\u001b[0;32m     74\u001b[0m prepaths \u001b[38;5;241m=\u001b[39m [folder \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(resultsfolder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*/\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prepath \u001b[38;5;129;01min\u001b[39;00m prepaths:\n\u001b[1;32m---> 76\u001b[0m     filesystem \u001b[38;5;241m=\u001b[39m \u001b[43mprepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     77\u001b[0m     storage \u001b[38;5;241m=\u001b[39m prepath\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m     78\u001b[0m     folders \u001b[38;5;241m=\u001b[39m [folder \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(prepath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*/\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "def parse_fio_results(file_path):\n",
    "    # Regular expressions\n",
    "    bandwidth_regex = re.compile(r'WRITE: bw=(\\d+(?:\\.\\d+)?)([MK]iB/s)')\n",
    "    bandwidth_read_regex = re.compile(r'READ: bw=(\\d+(?:\\.\\d+)?)([MK]iB/s)')\n",
    "    iops_regex = re.compile(r'write: IOPS=(\\d+)')\n",
    "    iops_read_regex = re.compile(r'read: IOPS=(\\d+)')\n",
    "    latency_regex = re.compile(r'lat (\\([mu]sec\\)): min=\\d+\\.?\\d*[km]?, max=\\d+\\.?\\d*[km]?, avg=(\\d+\\.\\d+[km]?), stdev=\\d+\\.?\\d*')\n",
    "\n",
    "\n",
    "\n",
    "    # Function to convert bandwidth to MiB/s\n",
    "    def convert_bandwidth(value, unit):\n",
    "        value = float(value)\n",
    "        if unit == \"KiB/s\":\n",
    "            return value / 1024  # Convert KiB/s to MiB/s\n",
    "        return value  # Already in MiB/s\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        last = 'read'\n",
    "        for line in file:\n",
    "            # Match write bandwidth\n",
    "            if 'write' in line:\n",
    "                last = 'write'\n",
    "            elif 'read' in line:\n",
    "                last = 'read'\n",
    "            bw_match = bandwidth_regex.search(line)\n",
    "            if bw_match:\n",
    "                value, unit = bw_match.groups()\n",
    "                results['Bandwidth WRITE (MiB/s)'] = convert_bandwidth(value, unit)\n",
    "\n",
    "            # Match read bandwidth\n",
    "            bw_read_match = bandwidth_read_regex.search(line)\n",
    "            if bw_read_match:\n",
    "                value, unit = bw_read_match.groups()\n",
    "                results['Bandwidth READ (MiB/s)'] = convert_bandwidth(value, unit)\n",
    "\n",
    "            # Match write IOPS\n",
    "            iops_match = iops_regex.search(line)\n",
    "            if iops_match:\n",
    "                results['IOPS WRITE'] = float(iops_match.group(1))\n",
    "\n",
    "            # Match read IOPS\n",
    "            iops_read_match = iops_read_regex.search(line)\n",
    "            if iops_read_match:\n",
    "                results['IOPS READ'] = float(iops_read_match.group(1))\n",
    "\n",
    "            # Match latency\n",
    "            lat_match = latency_regex.search(line)\n",
    "            if lat_match:\n",
    "                lat_val = float(lat_match.group(2))\n",
    "                if lat_match.group(1) == '(usec)':\n",
    "                    lat_val/=1000\n",
    "                if last == 'read':\n",
    "                    results['Latency READ (ms)'] = lat_val\n",
    "                else:\n",
    "                    results['Latency WRITE (ms)'] = lat_val\n",
    "\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_values(resultsfolder, file_names):\n",
    "    resultsdict = {'ext4': {},  'xfs': {}, 'btrfs': {}, 'zfs': {}, 'f2fs': {}}\n",
    "    cumulative_data = {file_name.split('_')[1]: defaultdict(list) for file_name in file_names}\n",
    "    prepaths = [folder for folder in glob.glob(resultsfolder + '*/')]\n",
    "    for prepath in prepaths:\n",
    "        filesystem = prepath.split('/')[-2].split('_')[2]\n",
    "        storage = prepath.split('/')[-2].split('_')[3]\n",
    "        folders = [folder for folder in glob.glob(prepath + '*/')]\n",
    "        cumulative_data = {file_name.split('_')[1]: defaultdict(list) for file_name in file_names}\n",
    "        for folder in folders:\n",
    "            for file_name in file_names:\n",
    "                file_path = os.path.join(folder, file_name)\n",
    "                if os.path.exists(file_path):\n",
    "                    try:\n",
    "                        results = parse_fio_results(file_path)\n",
    "                        for key, value in results.items():\n",
    "                            cumulative_data[file_name.split('_')[1]][key].append(value)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing {file_path}: {e}\")\n",
    "                else:\n",
    "                    print(f\"File not found: {file_path}\")\n",
    "\n",
    "        ranges = {}\n",
    "        for file_name, metrics in cumulative_data.items():\n",
    "            ranges[file_name] = {\n",
    "                key: {'min': round(min(values),3), 'max':round(max(values),3), 'avg':round(sum(values) / len(values), 2)} if values else '-'\n",
    "                for key, values in metrics.items()\n",
    "            }\n",
    "        resultsdict[filesystem][storage] = ranges\n",
    "    return resultsdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file_names = [\n",
    "    'fio_database_test_output.txt',\n",
    "    'fio_multimedia_test_output.txt',\n",
    "    'fio_webserver_test_output.txt',\n",
    "    'fio_archive_test_output.txt',\n",
    "]\n",
    "\n",
    "resultsdict = extract_values('./wyniki/', file_names)\n",
    "\n",
    "\n",
    "# Function to generate all possible columns\n",
    "def generate_columns(metrics):\n",
    "    storage_types = [\"HDD\", \"SSD\", \"NVME\"]\n",
    "    stats = [\"MIN\", \"MAX\", \"AVG\"]\n",
    "    columns = [\"File System\"]\n",
    "    for storage in storage_types:\n",
    "        for metric in metrics:\n",
    "            for stat in stats:\n",
    "                columns.append(f\"{storage} {metric} {stat}\")\n",
    "    return columns\n",
    "\n",
    "\n",
    "def extract_row_data(data, workload, columns):\n",
    "    rows = []\n",
    "    for fs, devices in data.items():\n",
    "        row = [fs]\n",
    "        for col in columns[1:]:  # Skip File System \n",
    "            if len(col.split()) > 3:\n",
    "                col = col.split()\n",
    "                storage, metric, stat = col[0],col[1]+' '+col[2],col[3]\n",
    "            else:\n",
    "                storage, metric, stat = col.split(\" \", 2)\n",
    "            if workload == 'database':\n",
    "                if \"Latency\" in metric:\n",
    "                    metric_key = f\"{metric} (ms)\"\n",
    "                else:\n",
    "                    metric_key = f\"{metric} (MiB/s)\" if \"Bandwidth\" in metric else metric\n",
    "            elif workload == 'archive':\n",
    "                if metric == \"Latency\":\n",
    "                    metric_key = \"Latency WRITE (ms)\"\n",
    "                else:\n",
    "                    metric_key = f\"{metric} WRITE (MiB/s)\" if metric == \"Bandwidth\" else f\"{metric} WRITE\"\n",
    "            else:\n",
    "                if metric == \"Latency\":\n",
    "                    metric_key = \"Latency READ (ms)\"\n",
    "                else:\n",
    "                    metric_key = f\"{metric} READ (MiB/s)\" if metric == \"Bandwidth\" else f\"{metric} READ\"\n",
    "            # Extract value\n",
    "            value = \"N/A\"\n",
    "            for device_type, workloads in devices.items():\n",
    "                if device_type.lower() == storage.lower() and workload in workloads:\n",
    "                    value = workloads[workload].get(metric_key, {}).get(stat.lower(), \"N/A\")\n",
    "                    break\n",
    "            row.append(value)\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "\n",
    "# Workloads to process\n",
    "workloads = [\"database\", \"multimedia\", \"webserver\", \"archive\"]\n",
    "\n",
    "# Generate and display tables for each workload\n",
    "for workload in workloads:\n",
    "    if workload == \"database\":\n",
    "        columns = generate_columns([\"Bandwidth READ\",\"Bandwidth WRITE\", \"IOPS READ\",\"IOPS WRITE\",\"Latency READ\", \"Latency WRITE\"])\n",
    "    else:\n",
    "        columns = generate_columns([\"Bandwidth\", \"IOPS\", \"Latency\"])\n",
    "    rows = extract_row_data(resultsdict, workload, columns)\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    display(df.style.set_caption(f\"Performance Metrics: {workload.capitalize()}\").format(precision=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ad6153-be83-4703-bb31-52f4d1bc2602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
